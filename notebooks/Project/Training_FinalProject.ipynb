{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (1470, 50, 50, 3), (1470, 1))\n",
      "('Test set', (630, 50, 50, 3), (630, 1))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file = '-images.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['X_train']\n",
    "    y_train = save['y_train']\n",
    "    X_test = save['X_test']\n",
    "    y_test = save['y_test']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', X_train.shape, y_train.shape)\n",
    "    print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## implement your CNN starting here.\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1470, 50, 50, 3)\n",
      "(1470, 1)\n",
      "(50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "num_classes = 2 #reducing number of classes \n",
    "\n",
    "# image dimensions\n",
    "img_rows, img_cols = X_train.shape[1],  X_train.shape[2]\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3) #changing from 1 to 3 for third dimension \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "\n",
    "print input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 128 # *increasing the number of training examples in one forward/backward pass--> making it too large can\n",
    "# cause the kernel to keep dying \n",
    "nb_epoch = 30\n",
    "\n",
    "# network architecture\n",
    "patch_size_1 = 4 #reducing the patch size for deeper neural network/more extensive connections\n",
    "patch_size_2 = 4\n",
    "patch_size_3 = 4 \n",
    "\n",
    "depth_1 = 40\n",
    "depth_2 = 50 #increasing depth size, but not too much to the hundreds \n",
    "depth_3 = 50\n",
    "\n",
    "pool_size = 2\n",
    "\n",
    "num_hidden_1 = 100\n",
    "num_hidden_2 = 100\n",
    "\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new Keras Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add first convolutional layer to model and specify it's depth and filter size\n",
    "# for the first layer we also have to specify the size of each input image\n",
    "# which we calculated above\n",
    "model.add(Convolution2D(depth_1, patch_size_1, patch_size_1,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "# apply 'relu' activation function for first layer\n",
    "model.add(Activation('relu'))\n",
    "# apply max pooling to reduce the size of the image by a factor of 2\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# repeat these operations for the second convolutional layer\n",
    "# this time Keras can figure out the input size \n",
    "# from the previous layer on it's own\n",
    "model.add(Convolution2D(depth_2, patch_size_2, patch_size_2,\n",
    "                        border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Convolution2D(depth_3, patch_size_3, patch_size_3,\n",
    "                        border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "\n",
    "# flatten the three-dimensional convolutional layer to a single layer of neurons\n",
    "model.add(Flatten())\n",
    "\n",
    "# add the first fully connected layer, applying 'relu' activation and dropout\n",
    "model.add(Dense(num_hidden_1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the second fully connected layer\n",
    "model.add(Dense(num_hidden_2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the final classification layer with the number of neurons \n",
    "# matching the number of classes we are trying to learn\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "# apply the 'softmax' activation to the final layer to convert the output to \n",
    "# a probability distribution\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1470 samples, validate on 630 samples\n",
      "Epoch 1/30\n",
      "1470/1470 [==============================] - 12s - loss: 0.6619 - acc: 0.9197 - val_loss: 0.4794 - val_acc: 0.9286\n",
      "Epoch 2/30\n",
      "1470/1470 [==============================] - 13s - loss: 0.3026 - acc: 0.9279 - val_loss: 0.2595 - val_acc: 0.9286\n",
      "Epoch 3/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.2632 - acc: 0.9279 - val_loss: 0.2358 - val_acc: 0.9286\n",
      "Epoch 4/30\n",
      "1470/1470 [==============================] - 12s - loss: 0.2458 - acc: 0.9279 - val_loss: 0.2052 - val_acc: 0.9286\n",
      "Epoch 5/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.2281 - acc: 0.9279 - val_loss: 0.1355 - val_acc: 0.9286\n",
      "Epoch 6/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.1778 - acc: 0.9279 - val_loss: 0.0935 - val_acc: 0.9286\n",
      "Epoch 7/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.1716 - acc: 0.9279 - val_loss: 0.0970 - val_acc: 0.9286\n",
      "Epoch 8/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.1103 - acc: 0.9279 - val_loss: 0.5653 - val_acc: 0.9286\n",
      "Epoch 9/30\n",
      "1470/1470 [==============================] - 12s - loss: 0.1387 - acc: 0.9279 - val_loss: 0.0876 - val_acc: 0.9286\n",
      "Epoch 10/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0859 - acc: 0.9313 - val_loss: 0.0672 - val_acc: 0.9651\n",
      "Epoch 11/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0798 - acc: 0.9626 - val_loss: 0.0652 - val_acc: 0.9873\n",
      "Epoch 12/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0759 - acc: 0.9707 - val_loss: 0.0563 - val_acc: 0.9905\n",
      "Epoch 13/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0964 - acc: 0.9626 - val_loss: 0.0534 - val_acc: 0.9810\n",
      "Epoch 14/30\n",
      "1470/1470 [==============================] - 12s - loss: 0.0727 - acc: 0.9735 - val_loss: 0.0426 - val_acc: 0.9841\n",
      "Epoch 15/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0668 - acc: 0.9694 - val_loss: 0.0701 - val_acc: 0.9730\n",
      "Epoch 16/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0647 - acc: 0.9755 - val_loss: 0.0282 - val_acc: 0.9937\n",
      "Epoch 17/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.1035 - acc: 0.9537 - val_loss: 0.0397 - val_acc: 0.9905\n",
      "Epoch 18/30\n",
      "1470/1470 [==============================] - 11s - loss: 0.0409 - acc: 0.9857 - val_loss: 0.0325 - val_acc: 0.9889\n",
      "Epoch 19/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0589 - acc: 0.9735 - val_loss: 0.0253 - val_acc: 0.9937\n",
      "Epoch 20/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0956 - acc: 0.9653 - val_loss: 0.0306 - val_acc: 0.9889\n",
      "Epoch 21/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0431 - acc: 0.9810 - val_loss: 0.0303 - val_acc: 0.9873\n",
      "Epoch 22/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0353 - acc: 0.9864 - val_loss: 0.0339 - val_acc: 0.9825\n",
      "Epoch 23/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0363 - acc: 0.9857 - val_loss: 0.0432 - val_acc: 0.9794\n",
      "Epoch 24/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0325 - acc: 0.9898 - val_loss: 0.0239 - val_acc: 0.9905\n",
      "Epoch 25/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0562 - acc: 0.9803 - val_loss: 0.0231 - val_acc: 0.9889\n",
      "Epoch 26/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0404 - acc: 0.9830 - val_loss: 0.0213 - val_acc: 0.9952\n",
      "Epoch 27/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0427 - acc: 0.9803 - val_loss: 0.1424 - val_acc: 0.9651\n",
      "Epoch 28/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0543 - acc: 0.9782 - val_loss: 0.0464 - val_acc: 0.9794\n",
      "Epoch 29/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0444 - acc: 0.9803 - val_loss: 0.0278 - val_acc: 0.9889\n",
      "Epoch 30/30\n",
      "1470/1470 [==============================] - 10s - loss: 0.0366 - acc: 0.9830 - val_loss: 0.0477 - val_acc: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a44639d50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualization though? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0476633516857\n",
      "Test accuracy: 97.94%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print 'Test score:', score[0]\n",
    "print 'Test accuracy: {:.2%}'.format(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"-modelCairo.hdf5\"\n",
    "checkpointer = ModelCheckpoint(checkpoint_name, verbose=0, save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
