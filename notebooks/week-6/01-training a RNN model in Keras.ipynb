{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18309\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "- - maps to -> 3\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "# print chars\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print '-', \"- maps to ->\", char_to_int[\"-\"] # dash is recognized but deleted in the above text...??\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18209\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences\n",
    "# print inputs[0]\n",
    "# print outputs[0]\n",
    "# print inputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equires recognising that americas economy is an enormously complicated mechanism. as appealing as so --> m\n"
     ]
    }
   ],
   "source": [
    "print inputs[1], \"-->\", outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18209, 100, 44)\n",
      "y dims --> (18209, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 3.2229 - val_loss: 2.9976\n",
      "----- generating with seed: quality, ensuring that everyone who wants a job can get one and building a resilient economy thats p\n",
      "----- diversity: 0.5\n",
      "quality, ensuring that everyone who wants a job can get one and building a resilient economy thats powt\n",
      " enitintpt  g ouata  frttht ar   f aaio ti  nass   ra  orca ao     e igpeira dta tilt ettngt    \n",
      "----- diversity: 1.2\n",
      "quality, ensuring that everyone who wants a job can get one and building a resilient economy thats p m;ucruo-, tfy ivnefhm n muoav4osmts,oer e0b e.dticfeeslao,y4mbg.p1trck iwe .eacc v miim3t2s rr cb-m\n",
      "epoch: 2 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 3.0186 - val_loss: 2.9522\n",
      "----- generating with seed: toric paris climate agreement, which presents the best opportunity to save the planet for future gen\n",
      "----- diversity: 0.5\n",
      "toric paris climate agreement, which presents the best opportunity to save the planet for future genniaso rsmaho s s  rsoa      su  i r ftaem a det   st teai r il   toinhile g ane n d  baet o  f n   t\n",
      "----- diversity: 1.2\n",
      "toric paris climate agreement, which presents the best opportunity to save the planet for future genri odtu7o1 mi 8 1lnsoern taunelb6a\n",
      "r1oi7wnpdoles hatdunatrb ht :fepr8 eoyaim-i?ecub pee soten.cdeili\n",
      "epoch: 3 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 49s - loss: 2.9615 - val_loss: 2.8968\n",
      "----- generating with seed: a larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw t\n",
      "----- diversity: 0.5\n",
      "a larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw t s ote  oo etiin ae lte  soa s  itee  ttc enoas a ncae  mtteen e u  nio  et aoa  n astnitet a stuitp\n",
      "----- diversity: 1.2\n",
      "a larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw tacansahteaspl tcperdbrg ter scbnpnyn-r et.onrcliadh othsxiyder rd gtomd tlae. .gpant at ra.sheiijvli\n",
      "epoch: 4 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.8708 - val_loss: 2.7783\n",
      "----- generating with seed: ds now pay more of their fair share, tax changes enacted during my administration have increased the\n",
      "----- diversity: 0.5\n",
      "ds now pay more of their fair share, tax changes enacted during my administration have increased thest on ioesi en hso sporadi esct oy  ioait is e ins eni d nee te th a oranas pahe t er eatehise  ooe \n",
      "----- diversity: 1.2\n",
      "ds now pay more of their fair share, tax changes enacted during my administration have increased thefth hlmoif d fiabou sne u2f-ihtloau- nphe.oise  thmbustotb yieo vwmotis \n",
      "udxgdfoegterunr lsngsiuw-en\n",
      "epoch: 5 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 48s - loss: 2.7539 - val_loss: 2.6614\n",
      "----- generating with seed:  echoes nativist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mi\n",
      "----- diversity: 0.5\n",
      " echoes nativist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mia on ore anse or een aee  as o pir  eo ete aol ihet ie  ee opot e oori al hte soe  ain  in ame  aner\n",
      "----- diversity: 1.2\n",
      " echoes nativist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mievevtotivgant., rasaithrucssoxn9eswoficoprsmehnto nz8gvoe efntte ltmanlsueem s nlaulayeenst ateroh w\n",
      "epoch: 6 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 48s - loss: 2.6477 - val_loss: 2.5691\n",
      "----- generating with seed: il. this can happen through the tendency towards monopoly and rent-seeking that this newspaper has d\n",
      "----- diversity: 0.5\n",
      "il. this can happen through the tendency towards monopoly and rent-seeking that this newspaper has d gonathe thl the toithta tin tot toe ing ond ans pord rore at are anl on inn one tove tan the ito th\n",
      "----- diversity: 1.2\n",
      "il. this can happen through the tendency towards monopoly and rent-seeking that this newspaper has dycero,e skntowwp bog rc mhpugmetal  ao hogea wog chumiignnmanarti h ng -anle arectaber carclranltrrb\n",
      "epoch: 7 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 49s - loss: 2.5650 - val_loss: 2.5058\n",
      "----- generating with seed:  distribution by 18 by 2017, while raising the average tax rates on households projected to earn ove\n",
      "----- diversity: 0.5\n",
      " distribution by 18 by 2017, while raising the average tax rates on households projected to earn overerin sun thas se and aned the rerole fore taitin the thre an and ant ere the an  he the ser the tha\n",
      "----- diversity: 1.2\n",
      " distribution by 18 by 2017, while raising the average tax rates on households projected to earn ove phonr agpd i,weaa sed tobmend ind nmpmunso tfp atedovresadc thencengos senatlesbo.uy tt4reln uug fe\n",
      "epoch: 8 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 49s - loss: 2.5145 - val_loss: 2.4528\n",
      "----- generating with seed:  would add flexibility for employees and employers. reforms to our criminal-justice system and impro\n",
      "----- diversity: 0.5\n",
      " would add flexibility for employees and employers. reforms to our criminal-justice system and improtc he the sue the in ie the thar tow ind fos res ye fhat il ans an pale the sther aninat are to the \n",
      "----- diversity: 1.2\n",
      " would add flexibility for employees and employers. reforms to our criminal-justice system and improul nrivnnsr an ursdyithll pstos. at. auli?eg c8woicst i0nieng le4lserld ma.sac1eg mofvy4 oro urmedef\n",
      "epoch: 9 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 2.4647 - val_loss: 2.4122\n",
      "----- generating with seed: the consumer financial protection bureauto hold financial institutions accountable, so their custome\n",
      "----- diversity: 0.5\n",
      "the consumer financial protection bureauto hold financial institutions accountable, so their custome the the fas the the ate th uon th ion d tho tor th an  the the  oos ant rang the th antor than thas\n",
      "----- diversity: 1.2\n",
      "the consumer financial protection bureauto hold financial institutions accountable, so their customexondns adrayntnilhy aly m?na.  eln aogd bc al 2nc emnttonplangn nyo2tyre i9ytomgr ctesol tkeni0 s qc\n",
      "epoch: 10 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.4351 - val_loss: 2.3854\n",
      "----- generating with seed: dvanced economies see chart 1. without a faster-growing economy, we will not be able to generate the\n",
      "----- diversity: 0.5\n",
      "dvanced economies see chart 1. without a faster-growing economy, we will not be able to generate the ereg the ard ale in the pacse the cas and ale an pind ing the ores pame the  or an the anite an the\n",
      "----- diversity: 1.2\n",
      "dvanced economies see chart 1. without a faster-growing economy, we will not be able to generate the sedoleme,s arit onkeesra:s  uluo5utie. s be th1we lu t0 ulotof\n",
      "ld cy srpismeedee u7 ,opserslelnn r9\n",
      "epoch: 11 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 49s - loss: 2.3992 - val_loss: 2.3616\n",
      "----- generating with seed: f the recent productivity slowdown has been a shortfall of public and private investment caused, in \n",
      "----- diversity: 0.5\n",
      "f the recent productivity slowdown has been a shortfall of public and private investment caused, in the tore tho sort fo to ion en tore the the the or he an the the the dor the en of ins the the mon p\n",
      "----- diversity: 1.2\n",
      "f the recent productivity slowdown has been a shortfall of public and private investment caused, in voracis \n",
      "se-gngvekiyu8camgnd gwe,h srmesad ,aon  h wose pmottiawg srsehantt d wfionce mokt turae ncs\n",
      "epoch: 12 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 2.3705 - val_loss: 2.3319\n",
      "----- generating with seed: rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "it\n",
      "----- diversity: 0.5\n",
      "rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "itan inan the ry onity and the bealy an oa croftet anta\n",
      "as forican the poreras an the the ten and whe \n",
      "----- diversity: 1.2\n",
      "rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "itiob eus einsbelt s5ey ge of ropd nhinicibe pocasp wksl-y\n",
      "uustehu teno\n",
      "ubelzanl; fol, the at. ior ne \n",
      "epoch: 13 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 2.3447 - val_loss: 2.3160\n",
      "----- generating with seed: d the fastest productivity growth in the g7, but it has slowed across nearly all advanced economies \n",
      "----- diversity: 0.5\n",
      "d the fastest productivity growth in the g7, but it has slowed across nearly all advanced economies an we phe the pare the for that orit an ire tho  fonte tho the font conor t0a the the to the or the \n",
      "----- diversity: 1.2\n",
      "d the fastest productivity growth in the g7, but it has slowed across nearly all advanced economies sl fot  mofuge.o colsing de.es.iden whimon gore sha.mhro-n fho ohe. the lay th o0m fhaede\n",
      "nmuns is c\n",
      "epoch: 14 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 2.3291 - val_loss: 2.2924\n",
      "----- generating with seed: that export pay their workers up to 18 more on average than companies that do not, according to a re\n",
      "----- diversity: 0.5\n",
      "that export pay their workers up to 18 more on average than companies that do not, according to a recontee on the sese the pasy th al or fas en and athe bore thand ant patint on sorte and and geore la\n",
      "----- diversity: 1.2\n",
      "that export pay their workers up to 18 more on average than companies that do not, according to a re balilg o0uy cofabticeng incy naseseun tare 3ap ant alerykth tos amirt g yrawlyniabramyd tian6 l pov\n",
      "epoch: 15 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 2.2891 - val_loss: 2.2766\n",
      "----- generating with seed: ucing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are clear: a more dur\n",
      "----- diversity: 0.5\n",
      "ucing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are clear: a more durle to le tha es ar the alle man cale bitut on mithe that and that foruning and palling the wins and \n",
      "----- diversity: 1.2\n",
      "ucing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are clear: a more duringot tfhtwurelta. pafsoreen sangemcan ip coy 19olly won thensantiy wle tianloct anern th. wde.riine\n",
      "epoch: 16 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 2.2690 - val_loss: 2.2560\n",
      "----- generating with seed: ld, often manifested in skepticism towards international institutions, trade agreements and immigrat\n",
      "----- diversity: 0.5\n",
      "ld, often manifested in skepticism towards international institutions, trade agreements and immigrate and palicing and and on the the wore satich oull anins this far sy onc alition more the the bine t\n",
      "----- diversity: 1.2\n",
      "ld, often manifested in skepticism towards international institutions, trade agreements and immigrat end fo7pvice on msdt he iconf-the ereco, wophce wyngury sioutlay as woo?l an tre sar cwen- seh ronp\n",
      "epoch: 17 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 2.2539 - val_loss: 2.2376\n",
      "----- generating with seed: nturies.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of wha\n",
      "----- diversity: 0.5\n",
      "nturies.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of whald an the ar the prove the con aming roriss the eron the mant palling the and the con the ano the re\n",
      "----- diversity: 1.2\n",
      "nturies.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of whaxddnat th poution 6nott t ruthe wore oam mcate-latton rowarhin ressprpeebe -oscuteltus\n",
      "sweur chavils\n",
      "epoch: 18 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 2.2316 - val_loss: 2.2315\n",
      "----- generating with seed:  our union would take far longer. the presidency is a relay race, requiring each of us to do our par\n",
      "----- diversity: 0.5\n",
      " our union would take far longer. the presidency is a relay race, requiring each of us to do our part and ingosle the adly bes ane cally sedes fors whe the and wer the preconting of ane ane than in fo\n",
      "----- diversity: 1.2\n",
      " our union would take far longer. the presidency is a relay race, requiring each of us to do our partngeat ragear the de-calithicsthon y;xicorl, salm can tuil inquavle 1eling ofrecisigy,-\n",
      "urecwals tha\n",
      "epoch: 19 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 48s - loss: 2.2045 - val_loss: 2.2109\n",
      "----- generating with seed: kes a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a deva\n",
      "----- diversity: 0.5\n",
      "kes a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a deva the tore an wore mons mestine more the cor the ar in for and in ant on als pore the recan bitiln an\n",
      "----- diversity: 1.2\n",
      "kes a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a devad co m0 eed, bowlunks for the insgevistians the btapty. pthaunte\n",
      "axso lx ase ty tre-rcumamceme paqd \n",
      "epoch: 20 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.1849 - val_loss: 2.1948\n",
      "----- generating with seed:  automation have weakened the position of workers and their ability to secure a decent wage. too man\n",
      "----- diversity: 0.5\n",
      " automation have weakened the position of workers and their ability to secure a decent wage. too mane contiming ins ast the seres in sete the les biet that and thine uat recons the and, and pre sere a\n",
      "----- diversity: 1.2\n",
      " automation have weakened the position of workers and their ability to secure a decent wage. too man frhive peatth theg tuved mixc, amin6, jut th e oqthered ie shavere. ebhriwa, fuccrtacl. so,ed in ou\n",
      "epoch: 21 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 2.1729 - val_loss: 2.1876\n",
      "----- generating with seed: nities have suffered from foreign competition, trade has helped our economy much more than it has hu\n",
      "----- diversity: 0.5\n",
      "nities have suffered from foreign competition, trade has helped our economy much more than it has hure the prolle the rale perofof the averisis the and toe the ing the pofpreste cons the wone the sund\n",
      "----- diversity: 1.2\n",
      "nities have suffered from foreign competition, trade has helped our economy much more than it has huved orcmicllameed cofiles alr ccanitnameyinlows0s hinwe thals ar cavy mert cove sisc us rarecovenss.\n",
      "epoch: 22 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 55s - loss: 2.1543 - val_loss: 2.1795\n",
      "----- generating with seed: rdless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent productivity slowdown has been a s\n",
      "----- diversity: 0.5\n",
      "rdless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent productivity slowdown has been a shered of recines an ine nor bealling the rowes and the pad the are on suct al the are of ono reat th\n",
      "----- diversity: 1.2\n",
      "rdless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent productivity slowdown has been a siache-tam, anmesing b tho soct ho1s. aat wrol 5 norsamr rhiod, thasave usdertilst at  he poonte dion\n",
      "epoch: 23 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.1378 - val_loss: 2.1608\n",
      "----- generating with seed: with hard work, we can improve our own station and watch our children do even better.\n",
      "\n",
      "as abraham li\n",
      "----- diversity: 0.5\n",
      "with hard work, we can improve our own station and watch our children do even better.\n",
      "\n",
      "as abraham ling th in the as and cherther aud wering racins restincing patt al or agre to ale of on wing bitit an\n",
      "----- diversity: 1.2\n",
      "with hard work, we can improve our own station and watch our children do even better.\n",
      "\n",
      "as abraham lichy ralilme cant mod enqoeair. thapi\n",
      "wovenealn o than yhart -sandel cor evenvertem wor obion, m\n",
      "ther\n",
      "epoch: 24 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 2.1214 - val_loss: 2.1542\n",
      "----- generating with seed: proving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "lifti\n",
      "----- diversity: 0.5\n",
      "proving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "lifting in res, of in artiomis the were sofitican in whe thore for in are in or a the soand the poors an \n",
      "----- diversity: 1.2\n",
      "proving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "liftitement e ipomartee nof indel hage rouin, the vowheaf rsovemeng et ano-hjougcm sicong-\n",
      "abcl fabte en \n",
      "epoch: 25 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 49s - loss: 2.1034 - val_loss: 2.1441\n",
      "----- generating with seed: it has hurt. exports helped lead us out of the recession. american firms that export pay their worke\n",
      "----- diversity: 0.5\n",
      "it has hurt. exports helped lead us out of the recession. american firms that export pay their worke nom ho peons on the wore ar and on the ale and growe and the arithe the for ans for the enore ned i\n",
      "----- diversity: 1.2\n",
      "it has hurt. exports helped lead us out of the recession. american firms that export pay their workestrqean sidy2, hfitcnonlens co preasent in puantsutusies yxor na io mhese  rosa:es af?. amuroiths 19\n",
      "epoch: 26 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 2.0801 - val_loss: 2.1356\n",
      "----- generating with seed: students, and ensuring men and women get equal pay for equal work would help to move us in the right\n",
      "----- diversity: 0.5\n",
      "students, and ensuring men and women get equal pay for equal work would help to move us in the right and ald ante the ating the regens. the parting of resers and comeration in the prossentich os the o\n",
      "----- diversity: 1.2\n",
      "students, and ensuring men and women get equal pay for equal work would help to move us in the rightudecrameed cenabkon hathenre tot mcor jolure in whe meteenocut thes forens porstaratincsipy. 11. ib9\n",
      "epoch: 27 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.0586 - val_loss: 2.1375\n",
      "----- generating with seed: ld care and early learning, would add flexibility for employees and employers. reforms to our crimin\n",
      "----- diversity: 0.5\n",
      "ld care and early learning, would add flexibility for employees and employers. reforms to our crimine for onee comante cons ag the are the portitias instesting least the and the ortheas on the lise fo\n",
      "----- diversity: 1.2\n",
      "ld care and early learning, would add flexibility for employees and employers. reforms to our crimine is poranti an geccoluat al treqraliotnqithionsg,i l zonrt tfain, guns teeh ecokontearisuqlale cals\n",
      "epoch: 28 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 2.0496 - val_loss: 2.1236\n",
      "----- generating with seed:  the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the pr\n",
      "----- diversity: 0.5\n",
      " the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the prostering in ande icane and an in ore forle in out encomuring to the probiens in the ant of ar more a\n",
      "----- diversity: 1.2\n",
      " the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the prosteeg bathess. sow past a verhat me opr cremurted and anxmpere choinc2iave forl merensd. in qatoalr\n",
      "epoch: 29 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 2.0316 - val_loss: 2.1329\n",
      "----- generating with seed: heir ability to secure a decent wage. too many potential physicists and engineers spend their career\n",
      "----- diversity: 0.5\n",
      "heir ability to secure a decent wage. too many potential physicists and engineers spend their career an at be wene nong of io rios the part the part chore gore sore the sing enaty an the economie that\n",
      "----- diversity: 1.2\n",
      "heir ability to secure a decent wage. too many potential physicists and engineers spend their careerudry, yfersit manc3 guthumbtie tictfralily inningoasr-tasinat ged inddeeusucate spanecquattians thoc\n",
      "epoch: 30 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 50s - loss: 2.0140 - val_loss: 2.1078\n",
      "----- generating with seed: r ability to secure a decent wage. too many potential physicists and engineers spend their careers s\n",
      "----- diversity: 0.5\n",
      "r ability to secure a decent wage. too many potential physicists and engineers spend their careers suste sat ant rels the tha semoce southeat on the wist the ser1s no the are sunt resure the ared und \n",
      "----- diversity: 1.2\n",
      "r ability to secure a decent wage. too many potential physicists and engineers spend their careers so lewes incmame an thetublelastic stone it reasd ffhre tasy fon enor\n",
      "3f.\n",
      "sf thstratitad to opdsitily\n",
      "epoch: 31 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.9919 - val_loss: 2.1132\n",
      "----- generating with seed: ancial system more stable and supportive of long-term growth, including more capital for american ba\n",
      "----- diversity: 0.5\n",
      "ancial system more stable and supportive of long-term growth, including more capital for american bat wer the the partical tredens mont paritis thas south ard allising of the paritican the workens the\n",
      "----- diversity: 1.2\n",
      "ancial system more stable and supportive of long-term growth, including more capital for american bappre to cpestoar\n",
      "s. tult-eduicus enont mesiclesd on bariins e co geon09 menceveteto ir 6comies ao ih\n",
      "epoch: 32 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.9851 - val_loss: 2.1073\n",
      "----- generating with seed: crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fightin\n",
      "----- diversity: 0.5\n",
      "crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fighting ance of the toe blated the and oum reconomy the econinge sued ant pore that the wore for the preat\n",
      "----- diversity: 1.2\n",
      "crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fightinta ba;ito. thr wurle-incis ald,, thas soidkor the paots cors findldotaves ptojem forobuf torereg, in\n",
      "epoch: 33 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.9619 - val_loss: 2.1010\n",
      "----- generating with seed: enting colleges from pricing out hardworking students, and ensuring men and women get equal pay for \n",
      "----- diversity: 0.5\n",
      "enting colleges from pricing out hardworking students, and ensuring men and women get equal pay for the of and that rating buting ant colle the tore and ceconomicis shore woun the for come that and fo\n",
      "----- diversity: 1.2\n",
      "enting colleges from pricing out hardworking students, and ensuring men and women get equal pay for lava io anddadimy then wherseithe bloodest to eralt-tan siestom at fpat bu selis. ictohel-cane, oanc\n",
      "epoch: 34 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.9478 - val_loss: 2.1106\n",
      "----- generating with seed:  something to all of these and weve made real progress on all these fronts. but i believe that chang\n",
      "----- diversity: 0.5\n",
      " something to all of these and weve made real progress on all these fronts. but i believe that changiin. to for to ee tho pore morkerts in the alle for costorl sessing the cen with encing out har ges \n",
      "----- diversity: 1.2\n",
      " something to all of these and weve made real progress on all these fronts. but i believe that change toos. morkebt oits resorolp doic productiicly grofl, bused ancatmocas, amer ate the, -o ove, thes \n",
      "epoch: 35 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 1.9241 - val_loss: 2.1019\n",
      "----- generating with seed: le of the income distribution than for those at the top see chart 2. under my administration, we wil\n",
      "----- diversity: 0.5\n",
      "le of the income distribution than for those at the top see chart 2. under my administration, we will es beat can sution bo eened in wer tale and resesting to grewall that prodution ind areninang efin\n",
      "----- diversity: 1.2\n",
      "le of the income distribution than for those at the top see chart 2. under my administration, we wilnsthanckey dencinj ineduthol- atling tee fresles als ming gragcan if paid eetor congd,yeo stijurs wo\n",
      "epoch: 36 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 1.9146 - val_loss: 2.0913\n",
      "----- generating with seed:  of the financial system since the 1930s, as well as reforming health care and introducing new rules\n",
      "----- diversity: 0.5\n",
      " of the financial system since the 1930s, as well as reforming health care and introducing new rules ar the goonhes gropt singer and by peas in the and sutican of the parser ghowt painist and als fon \n",
      "----- diversity: 1.2\n",
      " of the financial system since the 1930s, as well as reforming health care and introducing new rules ar phprecams a crcome grpviouletrdnpredathist lang thes a coles crive whog tezrdcono.\n",
      "\n",
      "toar wew los\n",
      "epoch: 37 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.8927 - val_loss: 2.0936\n",
      "----- generating with seed: . we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate con\n",
      "----- diversity: 0.5\n",
      ". we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate cond bating thes the eanco ameres an the and the prestantizans in the all are on comiction the hathen g\n",
      "----- diversity: 1.2\n",
      ". we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate conn-betp arisnt hag bkef the hof suthoruianchiplsddens foon ingers, iakith i. butenimy thincvrentobpit\n",
      "epoch: 38 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.8725 - val_loss: 2.1059\n",
      "----- generating with seed: ier funding they got beforeevidence that the market increasingly understands that they are no longer\n",
      "----- diversity: 0.5\n",
      "ier funding they got beforeevidence that the market increasingly understands that they are no longer and the as of ararean in of ancimange conlong the for dore as share and for enseroing and arereceno\n",
      "----- diversity: 1.2\n",
      "ier funding they got beforeevidence that the market increasingly understands that they are no longer thaw shares roure senotkey fu\n",
      "trevenond whas reeea il, mat is lajinture in whop wive. gurst bhofres\n",
      "epoch: 39 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 1.8582 - val_loss: 2.1036\n",
      "----- generating with seed: \n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of what my adm\n",
      "----- diversity: 0.5\n",
      "\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of what my admenting to aed the are as the are the erors and inare. in the and andeaty that hat censteres and in t\n",
      "----- diversity: 1.2\n",
      "\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am proud of what my admuy r d 1e ik hhysecofy, oud nntegritis. dydostimn conceg ilime, the. snow blomnhefs to hakkilen merk\n",
      "epoch: 40 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 54s - loss: 1.8439 - val_loss: 2.0958\n",
      "----- generating with seed: ransatlantic trade and investment partnership with the eu. these agreements, and stepped-up trade en\n",
      "----- diversity: 0.5\n",
      "ransatlantic trade and investment partnership with the eu. these agreements, and stepped-up trade entrenged haiden to ard the palt and thet of the antreaty sichalls and ald meating and one the refor t\n",
      "----- diversity: 1.2\n",
      "ransatlantic trade and investment partnership with the eu. these agreements, and stepped-up trade enfunestud inneoglhenmis incoicispepmicive poem nioqtirg avmorotntsuby. belcesiin. teane aresris ece o\n",
      "epoch: 41 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 1.8269 - val_loss: 2.0898\n",
      "----- generating with seed: workers up to 18 more on average than companies that do not, according to a report by my council of \n",
      "----- diversity: 0.5\n",
      "workers up to 18 more on average than companies that do not, according to a report by my council of the thal inge tuin seapling the iner ace and wore porst weve at whia shares and mert thes ering that\n",
      "----- diversity: 1.2\n",
      "workers up to 18 more on average than companies that do not, according to a report by my council of tiandcablag bys. 8,, \n",
      "ave male fcasue acditmencmysims cmurkire deve popugy den decalage eftreming fu\n",
      "epoch: 42 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.8148 - val_loss: 2.1063\n",
      "----- generating with seed: aningful opportunities for work for everyone who wants a job. however, america has faced a long-term\n",
      "----- diversity: 0.5\n",
      "aningful opportunities for work for everyone who wants a job. however, america has faced a long-term thas the for thainger bese tive of the a alion and that and the ancing the the ton were tean the wo\n",
      "----- diversity: 1.2\n",
      "aningful opportunities for work for everyone who wants a job. however, america has faced a long-term alds an utebe ald coverceg on vorment that s0arling9, altoflictos tithatien asd notabd-a lablecrame\n",
      "epoch: 43 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 1.7903 - val_loss: 2.1030\n",
      "----- generating with seed:  changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in\n",
      "----- diversity: 0.5\n",
      " changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in americans ane reed ut am conto. in athire the eaplo for in and thith-and insinginit and and the arc\n",
      "----- diversity: 1.2\n",
      " changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in sothe ins recisiordust drmufevifitw. us thfektuw mure conmrsers, sedp. tiraligat ande addimetiyt ob\n",
      "epoch: 44 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.7790 - val_loss: 2.0955\n",
      "----- generating with seed: eem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many are rec\n",
      "----- diversity: 0.5\n",
      "eem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many are recons more eanding fure ration and for the progres for eshrige for are and work the prortent y ant by \n",
      "----- diversity: 1.2\n",
      "eem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many are recarty and gnoment and uceomticulys, the ally 5f pinoriedly mhnowonge. that leaed yhamdsts or the we-k\n",
      "epoch: 45 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 51s - loss: 1.7628 - val_loss: 2.0980\n",
      "----- generating with seed: ave not yet substantially boosted measured productivity growth. over the past decade, america has en\n",
      "----- diversity: 0.5\n",
      "ave not yet substantially boosted measured productivity growth. over the past decade, america has enoonce foun ald in ons ast the ast consuring share sustore fure prowo-thong the astorer for ecaniming\n",
      "----- diversity: 1.2\n",
      "ave not yet substantially boosted measured productivity growth. over the past decade, america has enoull conrameds som. thetttil. oring ivito fxismers a whing peec irecrevestvem forted ofseredes, jobc\n",
      "epoch: 46 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.7452 - val_loss: 2.1113\n",
      "----- generating with seed:  growth. economists have long recognised that markets, left to their own devices, can fail. this can\n",
      "----- diversity: 0.5\n",
      " growth. economists have long recognised that markets, left to their own devices, can fail. this cans alty the ancout and thas in as ancelan the eroment entureas in the americans ale of a censing the \n",
      "----- diversity: 1.2\n",
      " growth. economists have long recognised that markets, left to their own devices, can fail. this cankubto nored ncequatica es loone-gredfitnod.\n",
      "bet-exmomisi muspestt an ti-come sisn sourent reale sime\n",
      "epoch: 47 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.7289 - val_loss: 2.1012\n",
      "----- generating with seed: increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind\n",
      "----- diversity: 0.5\n",
      "increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind whakes bookent the porthating comiliming fur on the parter tho the promuntion sont ares and the sow\n",
      "----- diversity: 1.2\n",
      "increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind astrsesseso, 21 the ole deimundenils fut heltinabise thin, houlg vere fon-reto d paritt tho kec1;, \n",
      "epoch: 48 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 52s - loss: 1.7086 - val_loss: 2.1199\n",
      "----- generating with seed: form too often ignore the progress we have made, instead choosing to condemn the system as a whole. \n",
      "----- diversity: 0.5\n",
      "form too often ignore the progress we have made, instead choosing to condemn the system as a whole. the eat in buted anding the and to keng the tor americas they comerting to ale bise pats the pordent\n",
      "----- diversity: 1.2\n",
      "form too often ignore the progress we have made, instead choosing to condemn the system as a whole. aud ofl parpirisud omomy gomizancowit. ynovll gnotinmedirs jo whlinglubedr afy ecloebse jod thaverto\n",
      "epoch: 49 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 53s - loss: 1.6978 - val_loss: 2.1233\n",
      "----- generating with seed: ca should also do more to prepare for negative shocks before they occur. with todays low interest ra\n",
      "----- diversity: 0.5\n",
      "ca should also do more to prepare for negative shocks before they occur. with todays low interest rave shande the port that and and reses the lose no that of the sicis the prowen the partentitals sore\n",
      "----- diversity: 1.2\n",
      "ca should also do more to prepare for negative shocks before they occur. with todays low interest rauss we hol61bvet ip leig ersianl bhetumarising tie nobiog and poutit-od. muverp\n",
      "tomorcas  tos  remgr\n",
      "epoch: 50 / 50\n",
      "Train on 14567 samples, validate on 3642 samples\n",
      "Epoch 1/1\n",
      "14567/14567 [==============================] - 55s - loss: 1.6797 - val_loss: 2.1212\n",
      "----- generating with seed:  in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had\n",
      "----- diversity: 0.5\n",
      " in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had bes to get reno then whore soat dent for serict on whel chardens then ofur ins ardthelise in wes th\n",
      "----- diversity: 1.2\n",
      " in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had.\n",
      "\n",
      "prop1y, in yoatly shared the whar beanti. bfuis rath os are lobl bes mattils and prition tom 6pfo\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80921528\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
