{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ot fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and anti-refugee sentiment  --> e\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 3.1996 - val_loss: 2.9687\n",
      "----- generating with seed: ifferences in pay between corporate executives and their workers were constrained by a greater degre\n",
      "----- diversity: 0.5\n",
      "ifferences in pay between corporate executives and their workers were constrained by a greater degrei t  s eeeaee  ao t iar iriie ln os  on n ei   meoaotai o antto osiiodyof eisiee  beeanae    t ro e \n",
      "----- diversity: 1.2\n",
      "ifferences in pay between corporate executives and their workers were constrained by a greater degreaz0ekyagt,ohtsvq  vx sdmscn5t .et.dieo  masce mc ,ulh8at9gorsuhi\n",
      "nta6di  ttkewc cys7sapeottn4cido r \n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 3.0210 - val_loss: 2.9280\n",
      "----- generating with seed: by the few and unaccountable to the many is a threat to all. economies are more successful when we c\n",
      "----- diversity: 0.5\n",
      "by the few and unaccountable to the many is a threat to all. economies are more successful when we cp  aoae a oat   s a ort as ria es    r   as    eoentia e  i rttnhre a oa pne e  pat n are i erae   e\n",
      "----- diversity: 1.2\n",
      "by the few and unaccountable to the many is a threat to all. economies are more successful when we cioehkism  bndcxdustenlott ytgttudscsulmmnudie\n",
      "s a9ybni lerce.ynsb a5 yien mdsmrmrdruhignfocuci5ynkie\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.9624 - val_loss: 2.8685\n",
      "----- generating with seed: as reforming health care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "----- diversity: 0.5\n",
      "as reforming health care and introducing new rules cutting emissions from vehicles and power plants. il n aneee   fiiai e i e taitiini r mt eo i n toheta aisis icr s .e t atgesaraetrt iser ants   e u \n",
      "----- diversity: 1.2\n",
      "as reforming health care and introducing new rules cutting emissions from vehicles and power plants.oen .  nxbzhsetannmt yt  :o imdrednr  ne5nealfhyamgmitori83hgftots atu,iean shnntoee-e idso ,noacpml\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.8798 - val_loss: 2.7575\n",
      "----- generating with seed: chnological advances through the internet, mobile broadband and devices, artificial intelligence, ro\n",
      "----- diversity: 0.5\n",
      "chnological advances through the internet, mobile broadband and devices, artificial intelligence, roo elwr  ore en sne s ine s aye  ohisgt ota i emt ere aai sas usith erieu cae astit  hor  at ie eaief\n",
      "----- diversity: 1.2\n",
      "chnological advances through the internet, mobile broadband and devices, artificial intelligence, ro taeucmhwio thu h  hioinphty\n",
      "enic msr8g 0tew af airv rprhl ce-tnebkn wnmbstnehisee eo ge psd myihon \n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 87s - loss: 2.7634 - val_loss: 2.6230\n",
      "----- generating with seed: allenges: boosting productivity growth, combating rising inequality, ensuring that everyone who want\n",
      "----- diversity: 0.5\n",
      "allenges: boosting productivity growth, combating rising inequality, ensuring that everyone who want a er al she ato te ther h alitee aici snece fire non f ean he arn we trece ar iom roce ere srv cert\n",
      "----- diversity: 1.2\n",
      "allenges: boosting productivity growth, combating rising inequality, ensuring that everyone who want ua is eh dhienyeg tvv\n",
      "ercts4p auewnoa derf idrn mreeiuaed o9n oqcdaatt leo -t flmerd -ab swriatorin\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 2.6648 - val_loss: 2.5365\n",
      "----- generating with seed: ehensive rewriting of the rules of the financial system since the 1930s, as well as reforming health\n",
      "----- diversity: 0.5\n",
      "ehensive rewriting of the rules of the financial system since the 1930s, as well as reforming health ed taat ane tor in  ont ant intins an toe and cistire tas  ha al ins sint an ey ae  ond at ahod s p\n",
      "----- diversity: 1.2\n",
      "ehensive rewriting of the rules of the financial system since the 1930s, as well as reforming healthhedingenc,ien:ti phweegaprclnute o bca\n",
      "inydg  .eylceiy, tir ceppacsti:,   anstir-ei,psiveinlewl.,,ms\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.5751 - val_loss: 2.4608\n",
      "----- generating with seed:  would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financia\n",
      "----- diversity: 0.5\n",
      " would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financiat uat eore the toled cothel seather son aos toatitis fore the the san the taal the th the an are not\n",
      "----- diversity: 1.2\n",
      " would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financiab0e tase anglor 8xls ory idveosd me\n",
      "rcpoisl sed wne lsse akike\n",
      "fori2srili5 masy axrdan wsopricivattf\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 90s - loss: 2.5155 - val_loss: 2.4076\n",
      "----- generating with seed: ed a crude populism that promises a return to a past that is not possible to restoreand that, for mo\n",
      "----- diversity: 0.5\n",
      "ed a crude populism that promises a return to a past that is not possible to restoreand that, for moret ins an le the be are the the aus roros to ord thesd ant cole the the the tan eritp te the te ere\n",
      "----- diversity: 1.2\n",
      "ed a crude populism that promises a return to a past that is not possible to restoreand that, for mof ante. p b-sarchodkw focibtrs afcsicil.itjmave ealecore acent torto. oabind0leporiruscddeelrogn den\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.4688 - val_loss: 2.3669\n",
      "----- generating with seed: e past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed acro\n",
      "----- diversity: 0.5\n",
      "e past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed acrons the ne and an erge the se fon-ins an the and on theicas ange sincals and the the the the lol er o\n",
      "----- diversity: 1.2\n",
      "e past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed acro3ald ianbaecgismicrdisesgomy ers elngeass iscerand svnviyiclsobie te inmes concit.e fou5rovengcalles\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.4304 - val_loss: 2.3309\n",
      "----- generating with seed: uld also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial c\n",
      "----- diversity: 0.5\n",
      "uld also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial cond ontigning the andresins falid an inonkes ond te pontee mere an ore the ses res the nou the peron\n",
      "----- diversity: 1.2\n",
      "uld also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial caant steask.lold chhobe chhatum. svhlot 9l trewecrrrst. f oskphocabilh fhbrcogles .xmede,  onr pinit\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 2.4041 - val_loss: 2.3065\n",
      "----- generating with seed: ay a critical role. they help workers get a bigger slice of the pie but they need to be flexible eno\n",
      "----- diversity: 0.5\n",
      "ay a critical role. they help workers get a bigger slice of the pie but they need to be flexible eno the thol the poretom the has he the wang the the the ter fore natilind the the shed oun whe te wre \n",
      "----- diversity: 1.2\n",
      "ay a critical role. they help workers get a bigger slice of the pie but they need to be flexible enot ort bare,s iwc apiicho gewfe oerestragspipethorive lisdaris tham rathised cats silc ff erure, w bu\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.3703 - val_loss: 2.2847\n",
      "----- generating with seed: ort from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy.\n",
      "----- diversity: 0.5\n",
      "ort from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. the the thont an the the that an the aal hot ane the prestien wand pore the to the thet on the the \n",
      "----- diversity: 1.2\n",
      "ort from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. leoxting lowim paali:ut errwvavti\n",
      "r ofvve tuen vuby ducl7 ure w,ha seech, ariproseprinn jiindatednl\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 2.3416 - val_loss: 2.2629\n",
      "----- generating with seed: e world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamentally economic. the anti-i\n",
      "----- diversity: 0.5\n",
      "e world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamentally economic. the anti-ingnes and the the fores ang the tar thae tor wart han an in withe ins aring the hale cond the the re\n",
      "----- diversity: 1.2\n",
      "e world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamentally economic. the anti-icans laamg nac mhapdones 1iv mandimy uno. an 9iadcpridl,y ys cogr, gudepiocr aut tolervi wnsmenead t\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.3161 - val_loss: 2.2354\n",
      "----- generating with seed: ades of declining productivity growth and rising inequality have resulted in slower income growth fo\n",
      "----- diversity: 0.5\n",
      "ades of declining productivity growth and rising inequality have resulted in slower income growth forcas thif and the the powthe se tas the the somering of incentale eon the sort an sereane the wor en\n",
      "----- diversity: 1.2\n",
      "ades of declining productivity growth and rising inequality have resulted in slower income growth fomot and al maxpruxte whet wetle as wr1p fr pers thtihe cujnsiroew unwifd fon narnise poon ebqtor5e x\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.2994 - val_loss: 2.2188\n",
      "----- generating with seed:  by a greater degree of social interaction between employees at all levelsat church, at their childr\n",
      "----- diversity: 0.5\n",
      " by a greater degree of social interaction between employees at all levelsat church, at their children mart or piuse and soctian an tore riste pore cong in the thol the pantitis walle an tho ithe tho \n",
      "----- diversity: 1.2\n",
      " by a greater degree of social interaction between employees at all levelsat church, at their childrdnicem-nopidicist etioulh loft farcin.esc\n",
      "tham  ialy hheo eefimire gorhig to. promic wuive\n",
      "dlostinl \n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.2732 - val_loss: 2.2044\n",
      "----- generating with seed:  closer to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requi\n",
      "----- diversity: 0.5\n",
      " closer to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requile tume that for the thand bet of she eoprat the bat the icant be the the the nces fo the ce the to \n",
      "----- diversity: 1.2\n",
      " closer to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requimth2 werl ank-byet esanty witn liacd bende9eselica, dhond fim ncyelnge sh.slesawino sseebtozitad mo1\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.2508 - val_loss: 2.1856\n",
      "----- generating with seed: wers statutory rates and closes loopholes, and with public investments in basic research and develop\n",
      "----- diversity: 0.5\n",
      "wers statutory rates and closes loopholes, and with public investments in basic research and develops wo th wer cint the bet bleis an the the wall wor thare the that hof romer more tho the pericon bat\n",
      "----- diversity: 1.2\n",
      "wers statutory rates and closes loopholes, and with public investments in basic research and developing,\n",
      "\n",
      "weriing t alt ralds ule th luring imlois orinlulilexsann tar crodecinod 1etha hivoly, adee:  a\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.2196 - val_loss: 2.1747\n",
      "----- generating with seed:  0.1by nearly 7 percentage points, based on calculations by the department of the treasury. while th\n",
      "----- diversity: 0.5\n",
      " 0.1by nearly 7 percentage points, based on calculations by the department of the treasury. while the pores sore the fore the ore and the the on me the wored and mure that to the fresting the poont ba\n",
      "----- diversity: 1.2\n",
      " 0.1by nearly 7 percentage points, based on calculations by the department of the treasury. while thos tn.s ers 2itg itk a pgriveure onder made rongos snhscof or abutydabe a lomecresinc hasj ud\n",
      "crevst\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.2080 - val_loss: 2.1522\n",
      "----- generating with seed:  that should be an argument for building on what we have already done, not undoing it. and those who\n",
      "----- diversity: 0.5\n",
      " that should be an argument for building on what we have already done, not undoing it. and those who thal or imere tha the pocrenge thea the tha le winc casting se to the as liges btasut or porcecente\n",
      "----- diversity: 1.2\n",
      " that should be an argument for building on what we have already done, not undoing it. and those who theucacfob ee tiliry ce. l0b ee moul hassyine of and of lhe gobs indmerime? i tre tom ncy bate fomu\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.1886 - val_loss: 2.1491\n",
      "----- generating with seed: for building on what we have already done, not undoing it. and those who should be rising in defence\n",
      "----- diversity: 0.5\n",
      "for building on what we have already done, not undoing it. and those who should be rising in defencen four elont an acan atining the ande the sore aro and 2e preted et on choree po seatiin an ald og t\n",
      "----- diversity: 1.2\n",
      "for building on what we have already done, not undoing it. and those who should be rising in defence\n",
      " querer pringry bule on breasatitinn plomirssy in wali, a forucof ixhtbealay ox it.q othe-pyecf7orm\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.1679 - val_loss: 2.1269\n",
      "----- generating with seed:  economies see chart 1. without a faster-growing economy, we will not be able to generate the wage g\n",
      "----- diversity: 0.5\n",
      " economies see chart 1. without a faster-growing economy, we will not be able to generate the wage grost  fer comeres fore and will the the and fore uld the and the sored al the poriling to the worl l\n",
      "----- diversity: 1.2\n",
      " economies see chart 1. without a faster-growing economy, we will not be able to generate the wage gadk woplitin. deerat poes in peetprotaat rottheod biuilinl gutiom iacrs6en  lepbrse vitcund ferd nok\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.1513 - val_loss: 2.1182\n",
      "----- generating with seed:  there should no longer be any doubt that a free market only thrives when there are rules to guard a\n",
      "----- diversity: 0.5\n",
      " there should no longer be any doubt that a free market only thrives when there are rules to guard ald the ble the sout en oun en ol the the the tha lese rise for ace or sreres ar ancinges on the por \n",
      "----- diversity: 1.2\n",
      " there should no longer be any doubt that a free market only thrives when there are rules to guard alli-sicessipfs the osepant, ingmor ssirowbctn trae hout, lica epnobteinfsati la9t .jis mhalingt vevp\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.1348 - val_loss: 2.1053\n",
      "----- generating with seed: ee chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. today, \n",
      "----- diversity: 0.5\n",
      "ee chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. today, in anditins and unting and ar the the profting in wile that the regertit ans alle the s omor sat en \n",
      "----- diversity: 1.2\n",
      "ee chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. today, rigreshe ceomft pim ie aredhucicnpo4then wled have ulomrher ithsorst ly shel lomilg fuat bhe ncormec\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.1134 - val_loss: 2.0983\n",
      "----- generating with seed:  from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i \n",
      "----- diversity: 0.5\n",
      " from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i proust on the reant rover the the rencint aricaliss and with int enes and wime ange the tan the the \n",
      "----- diversity: 1.2\n",
      " from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i worig lacesses thaukbithef bnture ef tragbfinslpailboss ws y alh moriret inntiaslhall., biall hive t\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 2.0924 - val_loss: 2.0876\n",
      "----- generating with seed:  and middle of the income distribution than for those at the top see chart 2. under my administratio\n",
      "----- diversity: 0.5\n",
      " and middle of the income distribution than for those at the top see chart 2. under my administration eunssence pat the the hat of and betice for the fard of recursatt on the the the som the wall bat \n",
      "----- diversity: 1.2\n",
      " and middle of the income distribution than for those at the top see chart 2. under my administration 3ved e e0; himes,.dthd-undexthe corpains,.\n",
      "\n",
      "ainun halfioch bad jebipr oftr-om6roc bonded 2y ros ar\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.0754 - val_loss: 2.0808\n",
      "----- generating with seed:  agreements, and stepped-up trade enforcement, will level the playing field for workers and business\n",
      "----- diversity: 0.5\n",
      " agreements, and stepped-up trade enforcement, will level the playing field for workers and business rowe the pave rage tha  ande the inente portimiss at the congato ta the tha es and more the regor f\n",
      "----- diversity: 1.2\n",
      " agreements, and stepped-up trade enforcement, will level the playing field for workers and businessdy\n",
      "dinc longerties, now acard wall aron: ther w of mecanltrour llo7s rametarinchos har ieine ewoledb\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 2.0664 - val_loss: 2.0768\n",
      "----- generating with seed: on that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the paradox that de\n",
      "----- diversity: 0.5\n",
      "on that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the paradox that densens an sucrenof the prowond more of surece tor sore the the for for but tore conte and andicing th\n",
      "----- diversity: 1.2\n",
      "on that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the paradox that delps stof surestte as resty som ad abl, thable anduatithe , propxoultepaogcould tiy 1y qupuri thdetme\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.0465 - val_loss: 2.0649\n",
      "----- generating with seed:  more affordable and expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depen\n",
      "----- diversity: 0.5\n",
      " more affordable and expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depentint buter the for thes to prome sut and the the inored in beateronge fore can we pat the soutte pro\n",
      "----- diversity: 1.2\n",
      " more affordable and expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depend m2chanengis im ri;gh-tudkeest aine tal fie pardsee. thette wer hial ld pirsding fit onop ate \n",
      "antc\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.0234 - val_loss: 2.0585\n",
      "----- generating with seed: tor is one reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful whe\n",
      "----- diversity: 0.5\n",
      "tor is one reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful when the the eporting the persutit eno rest the regenthal semore fhe patitis the rate more and the rest\n",
      "----- diversity: 1.2\n",
      "tor is one reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful whe pavigit omitho tfor avetl .rlgeg.\n",
      "\n",
      "menlto sitea covidet. is provecof on sarili, th, lmawdes raghe c\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 2.0075 - val_loss: 2.0505\n",
      "----- generating with seed: unities for work for everyone who wants a job. however, america has faced a long-term decline in par\n",
      "----- diversity: 0.5\n",
      "unities for work for everyone who wants a job. however, america has faced a long-term decline in par the pave to the for poun the purd portter and insuret able the the the and a thes ond the leating t\n",
      "----- diversity: 1.2\n",
      "unities for work for everyone who wants a job. however, america has faced a long-term decline in parkseprineid thater adl, thac. ald as we up nts rovatihg l6ed, thes potlire for medimnaecticimiss sbte\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.9930 - val_loss: 2.0518\n",
      "----- generating with seed: rom vehicles and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new priv\n",
      "----- diversity: 0.5\n",
      "rom vehicles and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new prives in alico amina conttee than more and in and and insomes the parlicas of to wion hat dentora thas \n",
      "----- diversity: 1.2\n",
      "rom vehicles and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new privesiss-ropr. chetpins acinarde9s ffezlen ans ankatld. ghobese and mpith liveur tarik. challchal nthev\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.9787 - val_loss: 2.0476\n",
      "----- generating with seed: nd reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as ou\n",
      "----- diversity: 0.5\n",
      "nd reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as our comporting past of and the senting the proatens the pont the ad westhas wat or and ta the pansidic\n",
      "----- diversity: 1.2\n",
      "nd reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as out haplparevorwe fe we spar thace\n",
      "sursfys-tabb epontitoe. to col gusp the thes apre checes, st e7 w1l\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.9531 - val_loss: 2.0451\n",
      "----- generating with seed: on and watch our children do even better.\n",
      "\n",
      "as abraham lincoln said, while we do not propose any war \n",
      "----- diversity: 0.5\n",
      "on and watch our children do even better.\n",
      "\n",
      "as abraham lincoln said, while we do not propose any war the prosed the pat rest te to erongand the pater and in on the sa inchat shere the the the tee nobe \n",
      "----- diversity: 1.2\n",
      "on and watch our children do even better.\n",
      "\n",
      "as abraham lincoln said, while we do not propose any war wiong ther pealte. gre;ta hax cance theuns er jereicag arlaxdelk inc1epr th, al-irjoss a. freent,  a\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 1.9367 - val_loss: 2.0441\n",
      "----- generating with seed: hat so many are receptive to the argument that the game is rigged. but amid this understandable frus\n",
      "----- diversity: 0.5\n",
      "hat so many are receptive to the argument that the game is rigged. but amid this understandable frustor inmore the deabt grower incale to prowets the for evertens and lester and for a pragiens the far\n",
      "----- diversity: 1.2\n",
      "hat so many are receptive to the argument that the game is rigged. but amid this understandable frus biscempnownen. relicue cepaling to fulgivici filmes, fithap ensaly nusunces ict hissstregs eowh am \n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.9229 - val_loss: 2.0340\n",
      "----- generating with seed: increase the isolation of corporations and elites, who often seem to live by a different set of rule\n",
      "----- diversity: 0.5\n",
      "increase the isolation of corporations and elites, who often seem to live by a different set of rule the than sourd now potrest the portone the panting the, pald bithe and workert the sof be for and t\n",
      "----- diversity: 1.2\n",
      "increase the isolation of corporations and elites, who often seem to live by a different set of rule dyste mork oirathins ghow ehorey thas d-cmens in esitl-in ace dchans. iw on nkowe 2s thhe oxuchice \n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 1.9012 - val_loss: 2.0346\n",
      "----- generating with seed: ministration have increased the share of income received by all other families by more than the tax \n",
      "----- diversity: 0.5\n",
      "ministration have increased the share of income received by all other families by more than the tax has ertane. by worke that dever sed iond ast and instee the the the hand en iongrowt ons the peorth \n",
      "----- diversity: 1.2\n",
      "ministration have increased the share of income received by all other families by more than the tax rermime the paopll ey olud ow enond baty raries nom issmoblts ins icthate prefisen aod instreal, wil\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 86s - loss: 1.8904 - val_loss: 2.0290\n",
      "----- generating with seed:  to be flexible enough to adapt to global competition. raising the federal minimum wage, expanding t\n",
      "----- diversity: 0.5\n",
      " to be flexible enough to adapt to global competition. raising the federal minimum wage, expanding the sould and werhe wor growe and in are fart hes hall nger more worch and the pating be tho more tha\n",
      "----- diversity: 1.2\n",
      " to be flexible enough to adapt to global competition. raising the federal minimum wage, expanding tu this fattist ond updales work as, ritine 22c13ly, wethe far? hass ald winee lave 14-xctupata-prout\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.8707 - val_loss: 2.0358\n",
      "----- generating with seed: e include providing wage insurance for workers who cannot get a new job that pays as much as their o\n",
      "----- diversity: 0.5\n",
      "e include providing wage insurance for workers who cannot get a new job that pays as much as their of the pard the we the wile the ports that con res er we doun con econsty thes part the the ele the s\n",
      "----- diversity: 1.2\n",
      "e include providing wage insurance for workers who cannot get a new job that pays as much as their on rosens of thac eemont re grymltsurest refing cofurte levinge mnin mure bryuse 18e. 7y fhave nosmer\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 1.8518 - val_loss: 2.0334\n",
      "----- generating with seed: d. that should be an argument for building on what we have already done, not undoing it. and those w\n",
      "----- diversity: 0.5\n",
      "d. that should be an argument for building on what we have already done, not undoing it. and those work res the pass to the the fore the farling that and conte and the poptricis fore sest mene andes a\n",
      "----- diversity: 1.2\n",
      "d. that should be an argument for building on what we have already done, not undoing it. and those with add factfiris and scis-effof endtres sgrtapingrits of isuclatiog uclfmesisiss dal deentbed macky\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 1.8436 - val_loss: 2.0337\n",
      "----- generating with seed: owth thats not only sustainable but shared. to achieve it america must stay committed to working wit\n",
      "----- diversity: 0.5\n",
      "owth thats not only sustainable but shared. to achieve it america must stay committed to working with ald the apress rate and inconges the thale and the proteess ens the provests the probles for a lor\n",
      "----- diversity: 1.2\n",
      "owth thats not only sustainable but shared. to achieve it america must stay committed to working with murl wr than encoro\n",
      "s; ap, erncenmily haj, theins reve. res1 ly-em, mineasiene. migfo tham the sev\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 86s - loss: 1.8180 - val_loss: 2.0433\n",
      "----- generating with seed: ss is possible. last year, income gains were larger for households at the bottom and middle of the i\n",
      "----- diversity: 0.5\n",
      "ss is possible. last year, income gains were larger for households at the bottom and middle of the in equatity and workers the lare or the gat tha and we the the that the pordens reating rele the fore\n",
      "----- diversity: 1.2\n",
      "ss is possible. last year, income gains were larger for households at the bottom and middle of the in gat so wive step. ares mude crustrcan wsopperde cofmastd alus cenreed, inpeleroumfse devpedladimn \n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 1.7942 - val_loss: 2.0571\n",
      "----- generating with seed: alised medicine. but while these innovations have changed lives, they have not yet substantially boo\n",
      "----- diversity: 0.5\n",
      "alised medicine. but while these innovations have changed lives, they have not yet substantially boote-tare and in tomisis pation hand eance es more the mase and tien rasenos bese tacen and en a conga\n",
      "----- diversity: 1.2\n",
      "alised medicine. but while these innovations have changed lives, they have not yet substantially boonestfoully hichhesseso then patt tuasgh oh enidives de2mt0 vistearicu invalder.\n",
      "\n",
      "emprocatee probeen \n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 1.7756 - val_loss: 2.0394\n",
      "----- generating with seed: resent. there should no longer be any doubt that a free market only thrives when there are rules to \n",
      "----- diversity: 0.5\n",
      "resent. there should no longer be any doubt that a free market only thrives when there are rules to es of the ally 200s to cearer americas ard res ampricans worker anding the laves and the for see ad \n",
      "----- diversity: 1.2\n",
      "resent. there should no longer be any doubt that a free market only thrives when there are rules to aces on deaiticad calyy, ip lenatingy pohat in whes bere maclodes than yod sevingien in the taous as\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 1.7674 - val_loss: 2.0531\n",
      "----- generating with seed: bottom and middle of the income distribution than for those at the top see chart 2. under my adminis\n",
      "----- diversity: 0.5\n",
      "bottom and middle of the income distribution than for those at the top see chart 2. under my adminisistome thes go the and ane shaled be in antinitions to the the than an the farine the ald suclesis t\n",
      "----- diversity: 1.2\n",
      "bottom and middle of the income distribution than for those at the top see chart 2. under my adminisinscoomews-cho cing thal lerasd, gte palsiane wor cenductionos and enstilicas sporewfyoscd parthatio\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 1.7494 - val_loss: 2.0459\n",
      "----- generating with seed: n creating a global race to the top in rules for trade. while some communities have suffered from fo\n",
      "----- diversity: 0.5\n",
      "n creating a global race to the top in rules for trade. while some communities have suffered from foreal sonced for conder thand and the love the sos on rever to ner thated beend ty and ementing the f\n",
      "----- diversity: 1.2\n",
      "n creating a global race to the top in rules for trade. while some communities have suffered from fotered prave nensent a soaldial basthe por ty have exonsegbgen ecocona\n",
      "allonges apst.umt on wyste. ;a\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 1.7412 - val_loss: 2.0459\n",
      "----- generating with seed: verse trend.\n",
      "\n",
      "involuntary joblessness takes a toll on life satisfaction, self-esteem, physical healt\n",
      "----- diversity: 0.5\n",
      "verse trend.\n",
      "\n",
      "involuntary joblessness takes a toll on life satisfaction, self-esteem, physical healt rey or is are of the parding that the hall that of inconaly the foren a thar work the poost for the\n",
      "----- diversity: 1.2\n",
      "verse trend.\n",
      "\n",
      "involuntary joblessness takes a toll on life satisfaction, self-esteem, physical healte all-?inuos sond, we ha-taall abkest afline the. but geo mame ic ta.\n",
      "\n",
      "oxpofd aucleas enrioans preio\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.7197 - val_loss: 2.0631\n",
      "----- generating with seed: otics, advanced materials, improvements in energy efficiency and personalised medicine. but while th\n",
      "----- diversity: 0.5\n",
      "otics, advanced materials, improvements in energy efficiency and personalised medicine. but while thes to mulising the reghess the peonten in the pat the and ever secret enonge to inequality and while\n",
      "----- diversity: 1.2\n",
      "otics, advanced materials, improvements in energy efficiency and personalised medicine. but while thar dersutron d7e. mo oibl eem cusdorken aliingingtulgity,s as. theld sewpronty alesmen om hsertta-na\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 1.7054 - val_loss: 2.0602\n",
      "----- generating with seed: ile these innovations have changed lives, they have not yet substantially boosted measured productiv\n",
      "----- diversity: 0.5\n",
      "ile these innovations have changed lives, they have not yet substantially boosted measured productivity on the poont moo watl beer prose to the particincrition and induress the progress the nousthing \n",
      "----- diversity: 1.2\n",
      "ile these innovations have changed lives, they have not yet substantially boosted measured productivy moti finmmreate adkiccus no cean muk puind dovitipalingwt cassnot cedtres  fof the.c. ipasing mede\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 1.6917 - val_loss: 2.0568\n",
      "----- generating with seed: es also depends on creating a global race to the top in rules for trade. while some communities have\n",
      "----- diversity: 0.5\n",
      "es also depends on creating a global race to the top in rules for trade. while some communities have astore enonges that the the beort y ad in for in tean the to tiking the porition. and incating to i\n",
      "----- diversity: 1.2\n",
      "es also depends on creating a global race to the top in rules for trade. while some communities have chargp innaegy, murker. faren tho prolutt-;fint racio githef tep prryventiss dedpled-carne contrels\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 1.6743 - val_loss: 2.0490\n",
      "----- generating with seed: nancial crisis of 2008 only seemed to increase the isolation of corporations and elites, who often s\n",
      "----- diversity: 0.5\n",
      "nancial crisis of 2008 only seemed to increase the isolation of corporations and elites, who often sec tos in seruce ant yerasing the pouttheat pronees in anes ane batice sutcenta less and recenting a\n",
      "----- diversity: 1.2\n",
      "nancial crisis of 2008 only seemed to increase the isolation of corporations and elites, who often syed fur wald. guciniency pemare wark, chavep onserite.mevencod, weal biol buct io viser the tamer ba\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
