{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms to our criminal-justice system and improvements to re-entry into the workforce that have won bipa --> r\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 3.2195 - val_loss: 2.9803\n",
      "----- generating with seed:  economy also depends on meaningful opportunities for work for everyone who wants a job. however, am\n",
      "----- diversity: 0.5\n",
      " economy also depends on meaningful opportunities for work for everyone who wants a job. however, am  iot u rn o ea   n eientee   ane a  r  i  r  gaec te  rie i e     ai  gie  eg sn   ee   ee  aa  n  \n",
      "----- diversity: 1.2\n",
      " economy also depends on meaningful opportunities for work for everyone who wants a job. however, amgeddrilwcaonii2mtcly  ehed h u1i  epe oaomjjkns,ntdw0rs1uetidtmhrrsyge rlcnv nj 5o c7 idmrt1sruo oot\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 102s - loss: 3.0235 - val_loss: 2.9320\n",
      "----- generating with seed: des in the making. while i am proud of what my administration has accomplished these past eight year\n",
      "----- diversity: 0.5\n",
      "des in the making. while i am proud of what my administration has accomplished these past eight yearse  aane   o c    eeeee lssoin ttt  e s e is te oot s ar   iee ete  r e  ialiate oe ie  aiec ro eere\n",
      "----- diversity: 1.2\n",
      "des in the making. while i am proud of what my administration has accomplished these past eight year 1nlyc1-poscgs, mal eueyaesr ntde dh7 ltedejt  ls.la evntet 1rx aiaiclit fluhleeltfhxsfnfeiieevlvnui\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.9538 - val_loss: 2.8611\n",
      "----- generating with seed: ven by fears that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and \n",
      "----- diversity: 0.5\n",
      "ven by fears that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and reo ar ono ts nee rtomott degot gnttme e bire nt hwre eeenr so   ant loo  eni adin  sse  ate iin ns \n",
      "----- diversity: 1.2\n",
      "ven by fears that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and o tol,aonlans,;narudlc hdertofihtenmkent.wqhrtsese ged,ohcsst  eo wre a3ids mtre hdhe av e  ooes est\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 2.8533 - val_loss: 2.7604\n",
      "----- generating with seed: d restore past glory if they just got some group or idea that was threatening america under control.\n",
      "----- diversity: 0.5\n",
      "d restore past glory if they just got some group or idea that was threatening america under control. eey ohoaalantangro rsel e ace aere t se a ten ctoutenl un ee see tesee ee  eeee int s ea iss areni \n",
      "----- diversity: 1.2\n",
      "d restore past glory if they just got some group or idea that was threatening america under control.seah iblshhiwenvnas  oiwrevrkny. cxer.lsuse by -tainmestlaa9ri1dunn alruyrux ncnesciwrolgnautwneliul\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.7348 - val_loss: 2.6332\n",
      "----- generating with seed: ce for the common good, driving businesses to create products that consumers rave about or motivatin\n",
      "----- diversity: 0.5\n",
      "ce for the common good, driving businesses to create products that consumers rave about or motivatins goith  aree eranatoe  ad roor an tan eneree ae  aai s an enin nle are tee is al in aea ae theri uo\n",
      "----- diversity: 1.2\n",
      "ce for the common good, driving businesses to create products that consumers rave about or motivatinitsdoesaincary hoj uk4 cc taq vigaf ie  tioe o5o ctor ydtea aine hoceenwtsi d orevibfwaa  ha sambrr\n",
      "\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 127s - loss: 2.6257 - val_loss: 2.5449\n",
      "----- generating with seed: fully restoring faith in an economy where hardworking americans can get ahead requires addressing fo\n",
      "----- diversity: 0.5\n",
      "fully restoring faith in an economy where hardworking americans can get ahead requires addressing fo tece tis an aat erate the panscorcim ghe theiin ist onn rres thout  and nt eevener te taait esenore\n",
      "----- diversity: 1.2\n",
      "fully restoring faith in an economy where hardworking americans can get ahead requires addressing fonmelenstw hchotmconm air oor ony rbuoctk ao oach1yt ns mnvk nayt 0iinnd;r bleyser oestoti cirbaf\n",
      "mis\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.5512 - val_loss: 2.4841\n",
      "----- generating with seed: ic research and development. policies focused on education are critical both for increasing economic\n",
      "----- diversity: 0.5\n",
      "ic research and development. policies focused on education are critical both for increasing economice the ges ror an than tee tout the the we por on eere the ane fore the a the ges cous con ore to pet\n",
      "----- diversity: 1.2\n",
      "ic research and development. policies focused on education are critical both for increasing economicsetge e lare peprimiy pgfit be mn ohrls altime  vetfw1 ld aasgicnn mhatf a:titga.gu0bh at ola ireocn\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.4992 - val_loss: 2.4373\n",
      "----- generating with seed: cent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of th\n",
      "----- diversity: 0.5\n",
      "cent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of the the ant ye that anse enth be and wer rabe the drong ronas an the th an that an ton es and in e the\n",
      "----- diversity: 1.2\n",
      "cent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of tha-s\n",
      "eoltiger andam sura fshes bh gheantxyckdy icr euacims. ngor  oe vaacgmsoomles pndice top apollgn\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.4521 - val_loss: 2.4028\n",
      "----- generating with seed: ystem so partisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "----- diversity: 0.5\n",
      "ystem so partisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters. she anc ameres and she ore an okeronchat and an porese the the ans porat ouves oad cont he the the \n",
      "----- diversity: 1.2\n",
      "ystem so partisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters.hay amt onk tollssy ox fold apobom an anm gatijl. th far8yowiigs1s ,s o pevorentingicschaled cprefri\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.4198 - val_loss: 2.3790\n",
      "----- generating with seed: to our citizens is vital. curbs to entitlement growth that build on the affordable care acts progres\n",
      "----- diversity: 0.5\n",
      "to our citizens is vital. curbs to entitlement growth that build on the affordable care acts progress the and thece wrele pores an the polat  aot  he the te the thos resse the bulle gof the le wes in \n",
      "----- diversity: 1.2\n",
      "to our citizens is vital. curbs to entitlement growth that build on the affordable care acts progresek fet 6ham umtin ceut ann ralm creon ord sveusait mey efte ,0ee opeofeoig ganxt; boet-e punti, wmir\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.3895 - val_loss: 2.3492\n",
      "----- generating with seed: satlantic trade and investment partnership with the eu. these agreements, and stepped-up trade enfor\n",
      "----- diversity: 0.5\n",
      "satlantic trade and investment partnership with the eu. these agreements, and stepped-up trade enforele enstigcen wan for and onceres tho te to ereres an he ant wat the tall an for te on outing are th\n",
      "----- diversity: 1.2\n",
      "satlantic trade and investment partnership with the eu. these agreements, and stepped-up trade enfore.bt2fon endgeste7 fed yed uirksioty rmmore7nwadd cnalel, lir ont -at9ed hho  aum3ers rethowe.drewtl\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.3512 - val_loss: 2.3296\n",
      "----- generating with seed: artisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could\n",
      "----- diversity: 0.5\n",
      "artisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could and forte the the bes ag be the ame an core to and parderiscang th go and as to re theve tha past i\n",
      "----- diversity: 1.2\n",
      "artisan that previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could,f. gintund 1g andt phort od wevtmwar dates, afd calnopee lporate thace , inergf,qdeb and sotus, a6i\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.3297 - val_loss: 2.3033\n",
      "----- generating with seed: d help finding new jobs would assist. so would making unemployment insurance available to more worke\n",
      "----- diversity: 0.5\n",
      "d help finding new jobs would assist. so would making unemployment insurance available to more workeis and the the the tho perser oud hes furlinan the atiing thes the ans and tho werenor the rale gon \n",
      "----- diversity: 1.2\n",
      "d help finding new jobs would assist. so would making unemployment insurance available to more workes ol suiniun fondoscamey\n",
      ".\n",
      "rorhes waniln coess\n",
      "emfathoal horle-pvrinhuitxunto xxibles fretfivinl. in\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.3006 - val_loss: 2.2888\n",
      "----- generating with seed: in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the futu\n",
      "----- diversity: 0.5\n",
      "in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future for the wes reste an an wuring tha  he rus prerees an ing thal ane for enconethan the to hed the \n",
      "----- diversity: 1.2\n",
      "in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the futut woncebtect bus annes pod, th irkwr\n",
      "es, 4osr nepraly wcovs  moalm, uld cra.sinsont me lun0 y a8s er\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.2782 - val_loss: 2.2697\n",
      "----- generating with seed: ny potential physicists and engineers spend their careers shifting money around in the financial sec\n",
      "----- diversity: 0.5\n",
      "ny potential physicists and engineers spend their careers shifting money around in the financial secon the erang bet and racees ans an aming on the  aop in tor anceuticing ans an poriling se porings a\n",
      "----- diversity: 1.2\n",
      "ny potential physicists and engineers spend their careers shifting money around in the financial secomist ad creresbbling mat qvaelec00, is ofnuplns as sopguf the 2hoty meawint 0namw bamis or lesfofd \n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 2.2522 - val_loss: 2.2511\n",
      "----- generating with seed: ir share, tax changes enacted during my administration have increased the share of income received b\n",
      "----- diversity: 0.5\n",
      "ir share, tax changes enacted during my administration have increased the share of income received butily fof the proutid an aricingre at anu mer and the the f rathe nop onte ins ard ane tog the parse\n",
      "----- diversity: 1.2\n",
      "ir share, tax changes enacted during my administration have increased the share of income received be blpwongelns m?eocusitdyont rziansst?\n",
      "lofewore ecbo dower? yvor canlcune at ak ueclinity nsingrsta\n",
      "\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.2339 - val_loss: 2.2320\n",
      "----- generating with seed: ts at the expense of the deferred maintenance bills we are passing to our children, particularly for\n",
      "----- diversity: 0.5\n",
      "ts at the expense of the deferred maintenance bills we are passing to our children, particularly fore serenom ion abitite bo the to al miged and than the the an the the the pions cors the wort and the\n",
      "----- diversity: 1.2\n",
      "ts at the expense of the deferred maintenance bills we are passing to our children, particularly for ant1 aledybgithe  ow be tholoript, times s,ucste-stroerty enidly thoncecs fomite mese lominc\n",
      "-8rs p\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.1989 - val_loss: 2.2167\n",
      "----- generating with seed: d investment partnership with the eu. these agreements, and stepped-up trade enforcement, will level\n",
      "----- diversity: 0.5\n",
      "d investment partnership with the eu. these agreements, and stepped-up trade enforcement, will level ancoming thal cenal to protte the gromer the the reane for perer bout pronged furt conte for the an\n",
      "----- diversity: 1.2\n",
      "d investment partnership with the eu. these agreements, and stepped-up trade enforcement, will levelings.r.n wgod riesroaven b5am robe perlless wfs erdiey wh fofaly sobre borespty eluevibat edtouv-lus\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1913 - val_loss: 2.2038\n",
      "----- generating with seed: ould not bear the full burden of stabilising our economy. unfortunately, good economics can be overr\n",
      "----- diversity: 0.5\n",
      "ould not bear the full burden of stabilising our economy. unfortunately, good economics can be overrent bome insiste foul aad ar in and tor and icenome thig thaly the ing one the the coned th aced and\n",
      "----- diversity: 1.2\n",
      "ould not bear the full burden of stabilising our economy. unfortunately, good economics can be overr as reeswet  ncarme leye pyecite bl. wir-terciny tuts id in tpexge?ts, mat4rivp,imigp.\n",
      "titaat s oply\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1732 - val_loss: 2.1920\n",
      "----- generating with seed: only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "po\n",
      "----- diversity: 0.5\n",
      "only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "pore be the thal cendenting the fin the resest the hand fint and avery and wrale the the ther an the h\n",
      "----- diversity: 1.2\n",
      "only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "pohrcat eorusciner avereftr1ee we thamod punc9ty. iss thm uschall sobmev ow w.\n",
      "toced, wal lparde troos\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1504 - val_loss: 2.1841\n",
      "----- generating with seed: reaks for the most fortunate can address long-term fiscal challenges without sacrificing investments\n",
      "----- diversity: 0.5\n",
      "reaks for the most fortunate can address long-term fiscal challenges without sacrificing investments-ealle the more cance and wome conanged the prast on eromucing ain sore the the partirest and wall g\n",
      "----- diversity: 1.2\n",
      "reaks for the most fortunate can address long-term fiscal challenges without sacrificing investments, ils ufkems moud, sourc as in tezs buf xfrnnvame iwriggorpasisle hes -yprhmuhlegho goviln, ferveng \n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1322 - val_loss: 2.1795\n",
      "----- generating with seed: re and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are clea\n",
      "----- diversity: 0.5\n",
      "re and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are cleasing the pard mat an wer sount are mality the dones and in the pald and weve the pord cod the the al\n",
      "----- diversity: 1.2\n",
      "re and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are cleass, mene to amal whos, ivevlly gechamely tht ospra-itos tuxthbil forthderctaltieg tox a?ouvicrfustto\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1118 - val_loss: 2.1563\n",
      "----- generating with seed: conomy when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlem\n",
      "----- diversity: 0.5\n",
      "conomy when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlem rowg the sering me tut ingreatising for the the the the whe- sopurto that doprenting our the to per\n",
      "----- diversity: 1.2\n",
      "conomy when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlemamingitn meaqqutiinpist dare ditl, acsy 1odme d0vosumed by.e toos ivinl ad evem, nalh-rove phor chen\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.0961 - val_loss: 2.1577\n",
      "----- generating with seed: orts helped lead us out of the recession. american firms that export pay their workers up to 18 more\n",
      "----- diversity: 0.5\n",
      "orts helped lead us out of the recession. american firms that export pay their workers up to 18 more wore the ar and bebulle sore the mece and inecano thar in and that the leono less and the vered for\n",
      "----- diversity: 1.2\n",
      "orts helped lead us out of the recession. american firms that export pay their workers up to 18 more sf toee casale momt ancuur incaners erower balt nxaul ons mnce con ntrete-\n",
      ":al of thepe fythang oug\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.0785 - val_loss: 2.1470\n",
      "----- generating with seed:  many is a threat to all. economies are more successful when we close the gap between rich and poor \n",
      "----- diversity: 0.5\n",
      " many is a threat to all. economies are more successful when we close the gap between rich and poor the tas in al wat the the the past the be to jor antiingrting the thes maris somer chat the incont i\n",
      "----- diversity: 1.2\n",
      " many is a threat to all. economies are more successful when we close the gap between rich and poor inod ctaninw pakiming inalme bat fcoll.. \n",
      "ancon. toqdpuilial, rnce andq aowte novatiomy thet avthigt\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.0547 - val_loss: 2.1430\n",
      "----- generating with seed: highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising\n",
      "----- diversity: 0.5\n",
      "highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising and intore thas busting the thas ancoun the forem for ande tove to fore reathous and insinis sores \n",
      "----- diversity: 1.2\n",
      "highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising.\n",
      "youl theo gof jatr.\n",
      "paasinm, ts is indvating0ritvjsisunsy nemetsls jod anbrestian, fus nhe reorof \n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.0406 - val_loss: 2.1399\n",
      "----- generating with seed: expense of the deferred maintenance bills we are passing to our children, particularly for infrastru\n",
      "----- diversity: 0.5\n",
      "expense of the deferred maintenance bills we are passing to our children, particularly for infrastruthe than the fand meverle and aticing and iccang the for whan sout whond wation son men the and the \n",
      "----- diversity: 1.2\n",
      "expense of the deferred maintenance bills we are passing to our children, particularly for infrastrun melt a-sagemem comrtos fore thitleft. cobneticam eramcasty lya:es csituuk8 coly. save meper paady4\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.0176 - val_loss: 2.1323\n",
      "----- generating with seed: y have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the \n",
      "----- diversity: 0.5\n",
      "y have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the we the and were to in will growth and anlicing and the in the the racing enoming the gre the the peo\n",
      "----- diversity: 1.2\n",
      "y have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the lene-im aor 1hagk-inte n1t..\n",
      "3is sfouc-ing the ma boml-smwrequsarstlon of ry meret red thic turihs f\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.0040 - val_loss: 2.1216\n",
      "----- generating with seed: r workers were constrained by a greater degree of social interaction between employees at all levels\n",
      "----- diversity: 0.5\n",
      "r workers were constrained by a greater degree of social interaction between employees at all levels an to nepress and the pand and enoresing the and andendency and in for paiting ly and anderengore t\n",
      "----- diversity: 1.2\n",
      "r workers were constrained by a greater degree of social interaction between employees at all levelsinca. o nogt.\n",
      "en aldratr aif nodingithou fhrleg7 hes nhpettconly avedbyon tee thans, alenous mile se\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.9818 - val_loss: 2.1155\n",
      "----- generating with seed: rans-pacific partnership and to conclude a transatlantic trade and investment partnership with the e\n",
      "----- diversity: 0.5\n",
      "rans-pacific partnership and to conclude a transatlantic trade and investment partnership with the erdass, more that the than the the best camer of and tor the on wer ureting to the forming the proste\n",
      "----- diversity: 1.2\n",
      "rans-pacific partnership and to conclude a transatlantic trade and investment partnership with the ero0e rom puruncus of perainime amongetruingb meat to hes. paidy ulming sopat sorfse. jasb aid tmam. \n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.9695 - val_loss: 2.1184\n",
      "----- generating with seed: y policy should not bear the full burden of stabilising our economy. unfortunately, good economics c\n",
      "----- diversity: 0.5\n",
      "y policy should not bear the full burden of stabilising our economy. unfortunately, good economics can the worl dowe the graty the that fan the reversing and icrange in the the the gat efore tha to pe\n",
      "----- diversity: 1.2\n",
      "y policy should not bear the full burden of stabilising our economy. unfortunately, good economics cpaniny , s-e t; wirt it cout, tif wiag sust anf 2617, 9ngo, daligien acchatutiacies th iverome wos p\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.9429 - val_loss: 2.1134\n",
      "----- generating with seed: an address long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "f\n",
      "----- diversity: 0.5\n",
      "an address long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "for the arod workest an the serans ans fould the gromen comenting thas promeven for coul the detican \n",
      "----- diversity: 1.2\n",
      "an address long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "forpcrualial undcmtaimicans cesthmyde camenared ast-omperiness as ise on pabpear grovit jeat souckud \n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.9253 - val_loss: 2.1190\n",
      "----- generating with seed: level the playing field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, i\n",
      "----- diversity: 0.5\n",
      "level the playing field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, inconomiming the worker and ingresting the word unting tore the the pare of enalicy sor and the abe r\n",
      "----- diversity: 1.2\n",
      "level the playing field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, ingare. gaed hive nous fpart ang eclatle. \n",
      "nordimine chdusicc oun sbamings cegssy apto toblty ruca- p\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 117s - loss: 1.9242 - val_loss: 2.1054\n",
      "----- generating with seed: ildrens schools, in civic organisations. thats why ceos took home about 20- to 30-times as much as t\n",
      "----- diversity: 0.5\n",
      "ildrens schools, in civic organisations. thats why ceos took home about 20- to 30-times as much as the lowing and prouting sithan the the lost cencestre the enore pars the mare ther and insstunting th\n",
      "----- diversity: 1.2\n",
      "ildrens schools, in civic organisations. thats why ceos took home about 20- to 30-times as much as thew sallra th-tugend-putit got lest of lros fraoscurdsin, fy 3ogus whon the must nonkentstrestsuwtru\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 1.8959 - val_loss: 2.1082\n",
      "----- generating with seed: it is related to a devastating rise of opioid abuse and an associated increase in overdose deaths an\n",
      "----- diversity: 0.5\n",
      "it is related to a devastating rise of opioid abuse and an associated increase in overdose deaths and the more the fromurs ald the reas se whe dalle in hamit jon that pol enon the whan . jogre the pro\n",
      "----- diversity: 1.2\n",
      "it is related to a devastating rise of opioid abuse and an associated increase in overdose deaths and gat eherogtpingriggeviveanhtre l7ouy incire cos, of  wohl bedilise pa. whht nvitiabs the ppavilee \n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 1.8768 - val_loss: 2.1101\n",
      "----- generating with seed: gap between rich and poor and growth is broadly based. a world in which 1 of humanity controls as mu\n",
      "----- diversity: 0.5\n",
      "gap between rich and poor and growth is broadly based. a world in which 1 of humanity controls as muth red beed and pporting for ald eronoming the lest on aperingres prowthe sore in of and deaten the \n",
      "----- diversity: 1.2\n",
      "gap between rich and poor and growth is broadly based. a world in which 1 of humanity controls as muate ifimiriss fofeimicisminy the specbligg fatioy encayy 205t fmond,isngt bfourd fort dpaintive ammo\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.8572 - val_loss: 2.1079\n",
      "----- generating with seed: d expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depends on creating a gl\n",
      "----- diversity: 0.5\n",
      "d expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depends on creating a globit sere the fore thet thar constinct and beot cinl and a come the predem no worn for and belted an\n",
      "----- diversity: 1.2\n",
      "d expanding high-quality job training.\n",
      "\n",
      "lifting productivity and wages also depends on creating a glecer cevenoby bem ibestrlty what workcod eestur fuchiss cresinines hatghress buch of that cnange s e\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 1.8489 - val_loss: 2.1060\n",
      "----- generating with seed: . progress in america also helped catalyse the historic paris climate agreement, which presents the \n",
      "----- diversity: 0.5\n",
      ". progress in america also helped catalyse the historic paris climate agreement, which presents the prober for comering wor incont the former worke sore the to part destorsis sicce pas be an brester a\n",
      "----- diversity: 1.2\n",
      ". progress in america also helped catalyse the historic paris climate agreement, which presents the joptecings storamy wholl.e to and dotabrss mone bal 8ssite that exol dovens thes iatp by peoiny sheo\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 93s - loss: 1.8316 - val_loss: 2.1200\n",
      "----- generating with seed: eers shifting money around in the financial sector, instead of applying their talents to innovating \n",
      "----- diversity: 0.5\n",
      "eers shifting money around in the financial sector, instead of applying their talents to innovating the faring the furede the werkers ac tople the prostent to ingerest at or parices and averest peasti\n",
      "----- diversity: 1.2\n",
      "eers shifting money around in the financial sector, instead of applying their talents to innovating to coma teputribitr ivesice uctind y-cruwitingts natiuls, mechmingvelils avstint. betwmus te ahus be\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 1.8121 - val_loss: 2.1204\n",
      "----- generating with seed: estore past glory if they just got some group or idea that was threatening america under control. we\n",
      "----- diversity: 0.5\n",
      "estore past glory if they just got some group or idea that was threatening america under control. we dane we pant the the prowst tor buting we have cand could contes in furting leget ana merepportitis\n",
      "----- diversity: 1.2\n",
      "estore past glory if they just got some group or idea that was threatening america under control. wecreophatss,r the, desing, daveustdrl. dedactifanonesen copsotmemslquorl egreastverod dbies: te cbabu\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 1.7967 - val_loss: 2.1127\n",
      "----- generating with seed: er the past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed\n",
      "----- diversity: 0.5\n",
      "er the past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed the dabing in instering that challe cant reconons in bustreing in thet more to mere reanes in the t\n",
      "----- diversity: 1.2\n",
      "er the past decade, america has enjoyed the fastest productivity growth in the g7, but it has slowed ancspiodtining iget ontived fotfaly, oe oxlorge nitheslide veredubino thead grows-trhachever gro , \n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 95s - loss: 1.7806 - val_loss: 2.1269\n",
      "----- generating with seed: trans-pacific partnership and to conclude a transatlantic trade and investment partnership with the \n",
      "----- diversity: 0.5\n",
      "trans-pacific partnership and to conclude a transatlantic trade and investment partnership with the and semicing ay reat ore ant of the merer for efor coule by the one bustermin sertuce and and arenti\n",
      "----- diversity: 1.2\n",
      "trans-pacific partnership and to conclude a transatlantic trade and investment partnership with the ald rewstreist 5o ae imirics 19e. wjorits -ofredy.\n",
      "\n",
      "nforges bore 3avo can, setveran a6 uice cos thal\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 1.7664 - val_loss: 2.1214\n",
      "----- generating with seed:  threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the disc\n",
      "----- diversity: 0.5\n",
      " threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the discen and werker worl and at come and the fal dent reen fore the reas of the bestrecin secting to-e rad\n",
      "----- diversity: 1.2\n",
      " threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the discono\n",
      "poust and br libber, ande mipalels, a nislvinelut. acdiincition, anchallod anomd-tintarns, im lo\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.7472 - val_loss: 2.1364\n",
      "----- generating with seed:  has accomplished these past eight years, i have always acknowledged that the work of perfecting our\n",
      "----- diversity: 0.5\n",
      " has accomplished these past eight years, i have always acknowledged that the work of perfecting our the pering in icalles, the prosten that and inst mere wan proved to in the tha  of entaticne to bat\n",
      "----- diversity: 1.2\n",
      " has accomplished these past eight years, i have always acknowledged that the work of perfecting our dovito has nmomend gutiant eaulde crasimeg5rasiw whol acloole. sofrt dfunaves coresicts, waph the s\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 1.7340 - val_loss: 2.1265\n",
      "----- generating with seed:  12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or\n",
      "----- diversity: 0.5\n",
      " 12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or the ard be wowe farto pertsure and or gatine shered and medery and the fore and cament, an in enont\n",
      "----- diversity: 1.2\n",
      " 12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or ths doguttessisind of 1hateoalt dingcomel the l79iss wo phod comulicas fof al om revecfis netarito \n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.7213 - val_loss: 2.1454\n",
      "----- generating with seed: quality: technology, education, globalisation, declining unions and a falling minimum wage. there is\n",
      "----- diversity: 0.5\n",
      "quality: technology, education, globalisation, declining unions and a falling minimum wage. there is comented for enorestorminis thit. in the americhall fored by the cenamies an thame the progrest con\n",
      "----- diversity: 1.2\n",
      "quality: technology, education, globalisation, declining unions and a falling minimum wage. there is ubnore to ealy beomicien brevoarmy ac to kert ineteningecunow thear ancerecaald mecmitisg, a vonear\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.6945 - val_loss: 2.1496\n",
      "----- generating with seed: er known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen fro\n",
      "----- diversity: 0.5\n",
      "er known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen from the lobsen wo leve of the indemticas and with heve patsthe fin more of rangibatition the tor fanco\n",
      "----- diversity: 1.2\n",
      "er known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen frome inve the gas. beemeg ov userugdersse bo lignsa cof atd sel io. is ctausuledet he womn, arcinaus, \n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.6889 - val_loss: 2.1433\n",
      "----- generating with seed: is more prosperous than ever before and yet our societies are marked by uncertainty and unease. so w\n",
      "----- diversity: 0.5\n",
      "is more prosperous than ever before and yet our societies are marked by uncertainty and unease. so whe for economy and we hand resters to the economy sion the tor that and best acle thes the the the e\n",
      "----- diversity: 1.2\n",
      "is more prosperous than ever before and yet our societies are marked by uncertainty and unease. so whill  hofturiticca, icpfontoul a posunctisttrons rofart o7 ythe wof tom soweldtilnte more ofrmutsina\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.6820 - val_loss: 2.1403\n",
      "----- generating with seed:  a capitalism shaped by the few and unaccountable to the many is a threat to all. economies are more\n",
      "----- diversity: 0.5\n",
      " a capitalism shaped by the few and unaccountable to the many is a threat to all. economies are more andiciss and in boult nor the whone that the and thet giting the to proste cuncont alicale the more\n",
      "----- diversity: 1.2\n",
      " a capitalism shaped by the few and unaccountable to the many is a threat to all. economies are more kurdw breab it catding that domesy nol gzent that of otomryge-ceraandt, on sakite the sserankentb i\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 1.6574 - val_loss: 2.1470\n",
      "----- generating with seed:  all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very ess\n",
      "----- diversity: 0.5\n",
      " all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very essured for enory tho forres and ertiting the our hast werl meat peen in serses and patichos seatse and\n",
      "----- diversity: 1.2\n",
      " all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very essrated. ans pfikifig thvaligg of thad propregrtitiasidl-ommore snoliles thay mure whree shorpe, ratic\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
