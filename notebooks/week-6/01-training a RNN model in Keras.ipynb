{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ericans in the labour market when they fall on hard times. these include providing wage insurance fo --> r\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 3.2398 - val_loss: 2.9576\n",
      "----- generating with seed: \n",
      "as abraham lincoln said, while we do not propose any war upon capital, we do wish to allow the humb\n",
      "----- diversity: 0.5\n",
      "\n",
      "as abraham lincoln said, while we do not propose any war upon capital, we do wish to allow the humb eteedteg na ep eoae sooa ii a  iee eh r hodmc o  aeaio lt eia e  lt  . nteou   reoe i t e s    tntt\n",
      "----- diversity: 1.2\n",
      "\n",
      "as abraham lincoln said, while we do not propose any war upon capital, we do wish to allow the humbo s8rfaitott arao tccc e br z; ae w\n",
      "hnhtcanlrlny1hs,et to y te,ud i9x snuha  tebmlannlsey ihohnhooon\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 3.0336 - val_loss: 2.9192\n",
      "----- generating with seed: scal expansion than many appreciated in recovering from our crisismore than a dozen bills provided 1\n",
      "----- diversity: 0.5\n",
      "scal expansion than many appreciated in recovering from our crisismore than a dozen bills provided 1eoe rgh asg  ialsoetr a  rftu s ne tnesa li a eneern nernne toltcee oe reaarot  lae erte r as ro tet\n",
      "----- diversity: 1.2\n",
      "scal expansion than many appreciated in recovering from our crisismore than a dozen bills provided 1leranru ou9 lhsotm hyao:ndimaopcd mfh vrait dyccnmplde, juhygoccarwauny lma,eayei aigt-mruselbtw 3ga\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 80s - loss: 2.9841 - val_loss: 2.8730\n",
      "----- generating with seed: rced austerity on the economy prematurely by threatening a historic debt default. my successors shou\n",
      "----- diversity: 0.5\n",
      "rced austerity on the economy prematurely by threatening a historic debt default. my successors shoutrn cea t ont anp tceo snensira pait anit eh nr s ct ene ree ta  auu one  o  eano enn  a  nte ueenea\n",
      "----- diversity: 1.2\n",
      "rced austerity on the economy prematurely by threatening a historic debt default. my successors shoun,teeuw.sanre.oajcpoogeacetsewl2   yaeismh4d. o adistcg  af err3t ec as?inusti t ites pneuaesdtfdmdo\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.9151 - val_loss: 2.7822\n",
      "----- generating with seed: at it is shared broadly. these include everything from boosting funding for early childhood educatio\n",
      "----- diversity: 0.5\n",
      "at it is shared broadly. these include everything from boosting funding for early childhood educatioa and iens ie t isi grnn eca nee itgo tto  hon c stenc iuie  letnr  aeas an  ee nthe s osr sd mtala \n",
      "----- diversity: 1.2\n",
      "at it is shared broadly. these include everything from boosting funding for early childhood educatiot ntpsmeluad iderrlc. t oysocnfoonr-tea snnl:dmms,s6r.evspe tihminiuosl ma enbtr yei htyyckti , onxs\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 87s - loss: 2.8043 - val_loss: 2.6670\n",
      "----- generating with seed: e, should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the\n",
      "----- diversity: 0.5\n",
      "e, should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for theo pte t or iros fare s on  ur aaret toctite  oes toor aner te arf tode  eaa ois an tfaer iay tat he \n",
      "----- diversity: 1.2\n",
      "e, should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for thevmesbfiaedeseihsinebmitwomtlp trs h,tl,osetfhmi\n",
      "sgtcienrefeo it de stapho wflt is3 h pmeosi pcxpu th\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 95s - loss: 2.6955 - val_loss: 2.5781\n",
      "----- generating with seed: s were larger for households at the bottom and middle of the income distribution than for those at t\n",
      "----- diversity: 0.5\n",
      "s were larger for households at the bottom and middle of the income distribution than for those at te etn the bas oeher ao teti secicn innd nane ton the tiond wod marear topoe the tin cele o tote poon\n",
      "----- diversity: 1.2\n",
      "s were larger for households at the bottom and middle of the income distribution than for those at to7 m.nraielrtssq6rnun t8: pwohofa nn-er eety h1r oaslvodraniitalgre  tieuhtgiy trroe bslreebgeriscoe\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 2.6075 - val_loss: 2.4819\n",
      "----- generating with seed: t for a range of institutions and markets. big american financial institutions no longer get the typ\n",
      "----- diversity: 0.5\n",
      "t for a range of institutions and markets. big american financial institutions no longer get the typeret conthe tore cmosit in  oremens to and one the in to reo prer wpore ce eint the anrtet oa mher f\n",
      "----- diversity: 1.2\n",
      "t for a range of institutions and markets. big american financial institutions no longer get the typrkues\n",
      "thlsrilunmsark1rp ;ioorrcgvvhiony phereged, tvie tam.-s4es oherelimagetconlrstse fiuegid antom\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 2.5443 - val_loss: 2.4272\n",
      "----- generating with seed: ade and investment partnership with the eu. these agreements, and stepped-up trade enforcement, will\n",
      "----- diversity: 0.5\n",
      "ade and investment partnership with the eu. these agreements, and stepped-up trade enforcement, wille an onl  beoisang tore an and inad the are te the perin an eetin ferter sne cmeren ycins ond the th\n",
      "----- diversity: 1.2\n",
      "ade and investment partnership with the eu. these agreements, and stepped-up trade enforcement, willimulr tatc ncesaat 1h merh itich g ol a7rog onq1ra, ,ien fun,hal wulds nwey bcbas f orrin;uldnd bavy\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.5037 - val_loss: 2.3932\n",
      "----- generating with seed: e. increasing access to high-quality community colleges, proven job-training models and help finding\n",
      "----- diversity: 0.5\n",
      "e. increasing access to high-quality community colleges, proven job-training models and help finding tho ghe tion wo mer fase de to an te therp nes than pertinc, the s ore mor pasit in  and ing ond be\n",
      "----- diversity: 1.2\n",
      "e. increasing access to high-quality community colleges, proven job-training models and help finding mhrtalgegosion angorwqoavr geosi feworec yeavinj wog babdeg, lectong ha m0osgth \n",
      "ott. at accrintmop\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.4591 - val_loss: 2.3514\n",
      "----- generating with seed: banking system still present vulnerabilities and the housing-finance system has not been reformed. t\n",
      "----- diversity: 0.5\n",
      "banking system still present vulnerabilities and the housing-finance system has not been reformed. th an the nesres foming and re er an bad and gor tos the thee the palise the and anatiting the an te \n",
      "----- diversity: 1.2\n",
      "banking system still present vulnerabilities and the housing-finance system has not been reformed. theronto u beriisacf ppibs, an y notititas wme  u1 lemos alae\n",
      " ato, ang\n",
      "atlinleme otl olad fte,. bhas\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 91s - loss: 2.4205 - val_loss: 2.3260\n",
      "----- generating with seed: e.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead requires \n",
      "----- diversity: 0.5\n",
      "e.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead requires th wofud on ther the cucand por bicing al enduane fore whis an wuore se lurte porere the the pand to\n",
      "----- diversity: 1.2\n",
      "e.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead requires  lifuanucem p9r ghocqsd. iacnbve. tyot; 1he of mat fu? broatimitxhallots thh ?0 iadscint 1.c0r n iy \n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.3910 - val_loss: 2.2985\n",
      "----- generating with seed:  a moral argument. research shows that growth is more fragile and recessions more frequent in countr\n",
      "----- diversity: 0.5\n",
      " a moral argument. research shows that growth is more fragile and recessions more frequent in countre y an re an the wincan  on the the than d ore wal ceo enin suring in pore suate in the wan er to ri\n",
      "----- diversity: 1.2\n",
      " a moral argument. research shows that growth is more fragile and recessions more frequent in countrtivec vetnornastal socitirgical thos piticeshonend 3it.\n",
      "4\n",
      "edusinphy, she d belrpeuceesam noofnyen en\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.3623 - val_loss: 2.2754\n",
      "----- generating with seed: ities for work for everyone who wants a job. however, america has faced a long-term decline in parti\n",
      "----- diversity: 0.5\n",
      "ities for work for everyone who wants a job. however, america has faced a long-term decline in parting in eforing and baling ar core an an the at our fure the ralithe gar ans and and incen oreind the \n",
      "----- diversity: 1.2\n",
      "ities for work for everyone who wants a job. however, america has faced a long-term decline in particniugdhk thoe gfuleecevcsgins ;adsdpr3ting a:u.pel ro\n",
      "ldisire. to panet ig,gekiticine thifhise there\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.3373 - val_loss: 2.2510\n",
      "----- generating with seed: ertain anxiety over the forces of globalisation, immigration, technology, even change itself, has ta\n",
      "----- diversity: 0.5\n",
      "ertain anxiety over the forces of globalisation, immigration, technology, even change itself, has ta th pred an sot or panle that an on porles on the reres an wore cor bulite he the arereso. the froge\n",
      "----- diversity: 1.2\n",
      "ertain anxiety over the forces of globalisation, immigration, technology, even change itself, has taan gar ca tungrthat oad chaflitas, whle, mrticint. s0e af -coter, ric; pfondiand.s sthegr afrl eabur\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.3137 - val_loss: 2.2351\n",
      "----- generating with seed:  poor and growth is broadly based. a world in which 1 of humanity controls as much wealth as the oth\n",
      "----- diversity: 0.5\n",
      " poor and growth is broadly based. a world in which 1 of humanity controls as much wealth as the oth th at for meres pold ant the palt an wal the at on the moran in eserecant an the ip oul the an fnol\n",
      "----- diversity: 1.2\n",
      " poor and growth is broadly based. a world in which 1 of humanity controls as much wealth as the othe, all1gresweos anpbec-nobt meald recatile cytratt ir albisis is rthub, havule rora en-toro 9ob anua\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 2.2897 - val_loss: 2.2154\n",
      "----- generating with seed: ognising that americas economy is an enormously complicated mechanism. as appealing as some more rad\n",
      "----- diversity: 0.5\n",
      "ognising that americas economy is an enormously complicated mechanism. as appealing as some more rad more fore os tho mo icen nomes paod in siantre the porte sof om ber tha tor emrerthe sesurre more f\n",
      "----- diversity: 1.2\n",
      "ognising that americas economy is an enormously complicated mechanism. as appealing as some more radepredilns boy ia narsingos passim1 chiovero and cofedronor aomres ho peilitas ih rowe coutiovith. po\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 86s - loss: 2.2696 - val_loss: 2.1985\n",
      "----- generating with seed: each of us to do our part to bring the country closer to its highest aspirations. so where does my s\n",
      "----- diversity: 0.5\n",
      "each of us to do our part to bring the country closer to its highest aspirations. so where does my set ros con wore cogiting the cong the the tat inc ato cof and the cor at the the the amere the wean \n",
      "----- diversity: 1.2\n",
      "each of us to do our part to bring the country closer to its highest aspirations. so where does my sso lnof whed yobes mame thint io jep pverbicillilo blion. wol wh, h aily m an do, thef nsew 6ve lefq\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.2529 - val_loss: 2.1930\n",
      "----- generating with seed: se deaths and suicides among non-college-educated americansthe group where labour-force participatio\n",
      "----- diversity: 0.5\n",
      "se deaths and suicides among non-college-educated americansthe group where labour-force participation at the ars recore tis ar pro tominigis sapde the fore se the brate prosle. an the the, wines an mi\n",
      "----- diversity: 1.2\n",
      "se deaths and suicides among non-college-educated americansthe group where labour-force participation esaarr,.ta . to-ti\n",
      "zhre-asticgatiels icans monist \n",
      "hict-hatekeans aadttamisgungdamine afste nmomas\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.2327 - val_loss: 2.1697\n",
      "----- generating with seed:  denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepa\n",
      "----- diversity: 0.5\n",
      " denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepate and al the thas end an the perith th that eross the encemes fer and in the lame. the the to the g\n",
      "----- diversity: 1.2\n",
      " denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepang wos thang wurel oh es the\n",
      "\n",
      ";e alhe llakemmit anemind bul--osos nned ae .lrass ar . 5lliricc\n",
      "iacis\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.2057 - val_loss: 2.1544\n",
      "----- generating with seed: ty. concentrated wealth at the top means less of the broad-based consumer spending that drives marke\n",
      "----- diversity: 0.5\n",
      "ty. concentrated wealth at the top means less of the broad-based consumer spending that drives marke wer and to the merid escanist wher cound the to con roe the y ore res ceastiring that or prowin ce \n",
      "----- diversity: 1.2\n",
      "ty. concentrated wealth at the top means less of the broad-based consumer spending that drives markeint of pralpne wo caullicb of trsarereg to coreanth enmmye t97 rdvelsed sarm axave.ugiegrigkebe to p\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 86s - loss: 2.1858 - val_loss: 2.1376\n",
      "----- generating with seed: w deal and oversaw the most comprehensive rewriting of the rules of the financial system since the 1\n",
      "----- diversity: 0.5\n",
      "w deal and oversaw the most comprehensive rewriting of the rules of the financial system since the 150 3 the s ale rengess co the s apres an the the ling to emorert an reable and wer mare nof the ress\n",
      "----- diversity: 1.2\n",
      "w deal and oversaw the most comprehensive rewriting of the rules of the financial system since the 1dwexrhazon jrast mijlican woonky yhcis0siim patie the  afore\n",
      "hevpoppo7 oud dolnint marrdaves. tho th\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 88s - loss: 2.1723 - val_loss: 2.1361\n",
      "----- generating with seed: lobalisation, immigration, technology, even change itself, has taken hold in america. its not new, n\n",
      "----- diversity: 0.5\n",
      "lobalisation, immigration, technology, even change itself, has taken hold in america. its not new, not shas the best and more er protice on wer cot the whe paliin pand bat ap on and to wat er and ssem\n",
      "----- diversity: 1.2\n",
      "lobalisation, immigration, technology, even change itself, has taken hold in america. its not new, n ned and beenty cgruause co sose so mer 20.\n",
      "\n",
      ",ucines by wircem doosepdonkail s pord gad desm fenoter\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 87s - loss: 2.1460 - val_loss: 2.1236\n",
      "----- generating with seed:  since the 1970s. these gains would have been impossible without the globalisation and technological\n",
      "----- diversity: 0.5\n",
      " since the 1970s. these gains would have been impossible without the globalisation and technological edenons sact cono te profiti the the more en prones the lanl the and to ce porisit on the peprost g\n",
      "----- diversity: 1.2\n",
      " since the 1970s. these gains would have been impossible without the globalisation and technological ghfutemop atincs pepisgen rtti jndes montemin th oge.rbectsoclm witi nowjiphast wiva p ugrof redeso\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 85s - loss: 2.1402 - val_loss: 2.1144\n",
      "----- generating with seed: rcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns ab\n",
      "----- diversity: 0.5\n",
      "rcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns abe the serous past the ald on bustingres fres ford fall s and baticn as the wore and the the pollisin\n",
      "----- diversity: 1.2\n",
      "rcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns aby eriving in foruecro ipsent emarl lede veredini gsobbing.\n",
      "\n",
      "ivco: make,-rosiog the ksond the wond nd\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.1141 - val_loss: 2.1030\n",
      "----- generating with seed: nd opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressing climate change. over the p\n",
      "----- diversity: 0.5\n",
      "nd opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressing climate change. over the poot en the lost the and the secals rat on buted lisale cressent en wor thal shest but at riscon the \n",
      "----- diversity: 1.2\n",
      "nd opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressing climate change. over the pryyubt bud ersiasit3 qqoucti buto,in, mon hant, prtingiss kegthmortonget-putteccal 0re om enxreseusc\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.1064 - val_loss: 2.0969\n",
      "----- generating with seed: ing the federal minimum wage, expanding the earned income tax credit for workers without dependent c\n",
      "----- diversity: 0.5\n",
      "ing the federal minimum wage, expanding the earned income tax credit for workers without dependent conaden for worker the ard the bith the prosterting for and in and ant mertant fan centicingor to in \n",
      "----- diversity: 1.2\n",
      "ing the federal minimum wage, expanding the earned income tax credit for workers without dependent chsetring tyeverog eaqueen.\n",
      "ne some. busdkard; palmicppiale rain. darinh gaodlances mrconputisesty se\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.0731 - val_loss: 2.0937\n",
      "----- generating with seed: reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as our e\n",
      "----- diversity: 0.5\n",
      "reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as our eanise the panthe and presten and watl that enomor and that entore to ent of poriting lithe to and an\n",
      "----- diversity: 1.2\n",
      "reducing emissions has been put to rest. america has cut energy-sector emissions by 6, even as our eake,,  reagre,-dares covig.ecs, to evty jdabs fjfelec ee corsilved wian tout dhalee mer ane corainm \n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.0682 - val_loss: 2.0802\n",
      "----- generating with seed:  rejects virtually all sources of new public funding; a fixation on deficits at the expense of the d\n",
      "----- diversity: 0.5\n",
      " rejects virtually all sources of new public funding; a fixation on deficits at the expense of the danor not tho bows the porcent an in that enon wurth ard workes is ald resresing that economicing tha\n",
      "----- diversity: 1.2\n",
      " rejects virtually all sources of new public funding; a fixation on deficits at the expense of the dexlepsrow.\n",
      "2f. wh sad? ibations gy rke-sequpulle coble ercbisingtigale afereg au congtow pur bay cee\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.0469 - val_loss: 2.0714\n",
      "----- generating with seed: artment of the treasury. while the top 1 of households now pay more of their fair share, tax changes\n",
      "----- diversity: 0.5\n",
      "artment of the treasury. while the top 1 of households now pay more of their fair share, tax changes to and buredering thes thall arest an are als mare that of inowing the besureing to the prowthe for\n",
      "----- diversity: 1.2\n",
      "artment of the treasury. while the top 1 of households now pay more of their fair share, tax changes pooutthis  qopomy y aldep ly elssres beo lssy, buth inlydey pyotidane gsttomis. graest oforme, paty\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 81s - loss: 2.0217 - val_loss: 2.0720\n",
      "----- generating with seed: ngress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment pa\n",
      "----- diversity: 0.5\n",
      "ngress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment panded in instientsing pordeas of the reses in of ally enstorice and 2000; the andingse forle canting \n",
      "----- diversity: 1.2\n",
      "ngress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment partading botkengdencand aditys\n",
      "abipgpporncive tho pnoluticlingstty demifilamevo bewinkyn delbunssics \n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 2.0121 - val_loss: 2.0631\n",
      "----- generating with seed: at progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for nega\n",
      "----- diversity: 0.5\n",
      "at progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for negating the perontes an the as beling and wed in or borecontimntitanis concins for and the forders, we \n",
      "----- diversity: 1.2\n",
      "at progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for negaony werst, 4ivitioy ans oufwutt. vey byke sosr?ve and fanty tu mand cromibits-fuitire matist in reic\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.9898 - val_loss: 2.0570\n",
      "----- generating with seed: e than two centuries of economic and social progress. the progress of the past eight years should al\n",
      "----- diversity: 0.5\n",
      "e than two centuries of economic and social progress. the progress of the past eight years should al for andemict in antinesserting the recons more of the fat co prede that aring censteis an the tor w\n",
      "----- diversity: 1.2\n",
      "e than two centuries of economic and social progress. the progress of the past eight years should alus wuker ched eor 2 1is. hatenasyure cave shaw chappe it enan tha wom thers rp3dset thisy bet ontorc\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 1.9733 - val_loss: 2.0545\n",
      "----- generating with seed: se most pronounced in the united states. in 1979, the top 1 of american families received 7 of all a\n",
      "----- diversity: 0.5\n",
      "se most pronounced in the united states. in 1979, the top 1 of american families received 7 of all antire suoser and copled to ge but our the for and the eon the fore by the warker whe poopre ard to a\n",
      "----- diversity: 1.2\n",
      "se most pronounced in the united states. in 1979, the top 1 of american families received 7 of all antwirling oex forec. the-tow onc incasililising suh and compurt\n",
      ",siteony biohlevarchavingdd wl 2hatt\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.9497 - val_loss: 2.0481\n",
      "----- generating with seed: cing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressin\n",
      "----- diversity: 0.5\n",
      "cing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressing the proveriss the prome tece part ce proded yee recanes copred resering the that the encore to ere\n",
      "----- diversity: 1.2\n",
      "cing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable economic growth requires addressinss lotr\n",
      "-uvede ap mekle and bur aleoiny not 255 mehce -.owo fate wady amadejsan toy arcenes foor hol\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.9438 - val_loss: 2.0452\n",
      "----- generating with seed: ency is a relay race, requiring each of us to do our part to bring the country closer to its highest\n",
      "----- diversity: 0.5\n",
      "ency is a relay race, requiring each of us to do our part to bring the country closer to its highest and and in to ane for thar ecound more the popiticg toat im at in the the bothes and and pelliting \n",
      "----- diversity: 1.2\n",
      "ency is a relay race, requiring each of us to do our part to bring the country closer to its highest cengetoy insorsede yusssthmand o7 ound am in-mwesen toivand il calle .\n",
      "i1 7for ampariess to teken.\n",
      "\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.9221 - val_loss: 2.0392\n",
      "----- generating with seed: ve the world some measure of hope. despite all manner of division and discord, a second great depres\n",
      "----- diversity: 0.5\n",
      "ve the world some measure of hope. despite all manner of division and discord, a second great deprest an eronting the probles for the mant the growth and in couples aderess the pablita loshor cond not\n",
      "----- diversity: 1.2\n",
      "ve the world some measure of hope. despite all manner of division and discord, a second great depress on cur in1sktath te rkatusy conccouns, ow tre ured endengriag id the ta grsoband llove semodet rap\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.9108 - val_loss: 2.0445\n",
      "----- generating with seed:  increasingly understands that they are no longer too big to fail. and we created a first-of-its-kin\n",
      "----- diversity: 0.5\n",
      " increasingly understands that they are no longer too big to fail. and we created a first-of-its-king the anderty the poruts in the and costing on ous and betareing that enol on the pariting and ant a\n",
      "----- diversity: 1.2\n",
      " increasingly understands that they are no longer too big to fail. and we created a first-of-its-king onecans stohads- arkio. in sudmertinegrint guos eminesion , ixauncs mole-afirer whlalely pronlofme\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.8911 - val_loss: 2.0410\n",
      "----- generating with seed:  america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is ro\n",
      "----- diversity: 0.5\n",
      " america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is roule poor enor to the work resome thes more somade the ton and we chatser on houd probiting insemines\n",
      "----- diversity: 1.2\n",
      " america under control. we overcame those fears and we will again.\n",
      "\n",
      "but some of the discontent is rome cenmwint\n",
      "biincal is peptilit datm ball sunced thay bhit ol pnefedtiffhing ecciost lrecounge. acha\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 1.8756 - val_loss: 2.0420\n",
      "----- generating with seed: is a threat to all. economies are more successful when we close the gap between rich and poor and gr\n",
      "----- diversity: 0.5\n",
      "is a threat to all. economies are more successful when we close the gap between rich and poor and growth thas and alficling the furce sate paritist in the the patt ane prored in the in of the or ereco\n",
      "----- diversity: 1.2\n",
      "is a threat to all. economies are more successful when we close the gap between rich and poor and growth hocg th,oqu2seis trever padured. bet cesuanved h oo kcopringdes ay powbict3hes av hogelekres wh\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.8664 - val_loss: 2.0441\n",
      "----- generating with seed: in pay between corporate executives and their workers were constrained by a greater degree of social\n",
      "----- diversity: 0.5\n",
      "in pay between corporate executives and their workers were constrained by a greater degree of sociald and mering in and the fore sorge sonce that chalice shew hals pooble sonte non thas that the tone \n",
      "----- diversity: 1.2\n",
      "in pay between corporate executives and their workers were constrained by a greater degree of social snecins be \n",
      "rebrianirgshes guthir ligs wcomicition coimaring over davel and batsitinn hrowthpines w\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 1.8369 - val_loss: 2.0481\n",
      "----- generating with seed: ress long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally\n",
      "----- diversity: 0.5\n",
      "ress long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally and mering and bele in the but xectoris of the the los of the y and the tha to the  of resens mort \n",
      "----- diversity: 1.2\n",
      "ress long-term fiscal challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally aleime, pbaquativy thoul omouncectuiisgs. in cadlily bis aln1bsersus resperonivy ir ppehdins, the..\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 1.8179 - val_loss: 2.0385\n",
      "----- generating with seed: when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlement gro\n",
      "----- diversity: 0.5\n",
      "when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlement growth that chat and and reseant an to exprost ploy ace of the peonting the were add to ar and more fur\n",
      "----- diversity: 1.2\n",
      "when needed and to meet our long-term obligations to our citizens is vital. curbs to entitlement grorthtdutshat-tinncagle insethas es. lopiting an erecitidfroet.\n",
      "\n",
      "the oun the womupl aded at it muneter\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.7949 - val_loss: 2.0401\n",
      "----- generating with seed: ustice undermines peoples faith in the system. without trust, capitalism and markets cannot continue\n",
      "----- diversity: 0.5\n",
      "ustice undermines peoples faith in the system. without trust, capitalism and markets cannot continued wthe workes do effreste pitination hast and porled by secres, fos the past ped forle ba that or in\n",
      "----- diversity: 1.2\n",
      "ustice undermines peoples faith in the system. without trust, capitalism and markets cannot continues mortsest oxntedrmismas forggors. bforating tem esens te fynelt iccullse. arday in araes. gmo7boes \n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.7882 - val_loss: 2.0478\n",
      "----- generating with seed: tion of people living in extreme poverty has fallen from nearly 40 to under 10. last year, american \n",
      "----- diversity: 0.5\n",
      "tion of people living in extreme poverty has fallen from nearly 40 to under 10. last year, american and enor to at or hase the growth and werse al evenyweme growth the for and ther of the band and of \n",
      "----- diversity: 1.2\n",
      "tion of people living in extreme poverty has fallen from nearly 40 to under 10. last year, american inore paombis goopresmes rool rat 7crinco rataef, the  yopriss to a morecturd 1;.\n",
      "\n",
      "n2w1is treose rev\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 1.7667 - val_loss: 2.0481\n",
      "----- generating with seed: be frustrating. believe me, i know. but it has been the source of more than two centuries of economi\n",
      "----- diversity: 0.5\n",
      "be frustrating. believe me, i know. but it has been the source of more than two centuries of economics on the for and more ard paplica edpprting that the portor and mer canseric and resemong to the no\n",
      "----- diversity: 1.2\n",
      "be frustrating. believe me, i know. but it has been the source of more than two centuries of economica rove-ose macacy growhes on efarmest, the arorlofostoquglons as, alft covobiesten fao chorsertm. i\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 83s - loss: 1.7513 - val_loss: 2.0588\n",
      "----- generating with seed: t lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the an\n",
      "----- diversity: 0.5\n",
      "t lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the antiren far all geonter more furate the probatiding seast rane for the butadens and wing that er and w\n",
      "----- diversity: 1.2\n",
      "t lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the and tap. raopes raser th fppy live even adkrequale coscopion reilitirits thd ti9icvare2 unst tuingy al\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 86s - loss: 1.7351 - val_loss: 2.0512\n",
      "----- generating with seed:  strengthening economy have offset ageing and retiring baby-boomers since the end of 2013, stabilisi\n",
      "----- diversity: 0.5\n",
      " strengthening economy have offset ageing and retiring baby-boomers since the end of 2013, stabilising in ane the even to were that of has were al ouc workers the to probee to in on recans for the pad\n",
      "----- diversity: 1.2\n",
      " strengthening economy have offset ageing and retiring baby-boomers since the end of 2013, stabilisisl bechirgat eace purverce in reaseml0dy or comed.\n",
      "\n",
      "aid:la to oos y arcemactanguourdonsingly mafe th\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 89s - loss: 1.7191 - val_loss: 2.0659\n",
      "----- generating with seed: ed wholesale and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully \n",
      "----- diversity: 0.5\n",
      "ed wholesale and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully sing the progresse for more and bestuning the the peracedos ar the porting that suce on ecenortom th\n",
      "----- diversity: 1.2\n",
      "ed wholesale and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully save to by adtr aipladet, wo fapmerta5n efory ins aipsoiighar councre? showt-sencur, iaply fore, mun\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 89s - loss: 1.7069 - val_loss: 2.0629\n",
      "----- generating with seed: ns with health insurance, while health-care costs grow at the slowest rate in 50 years; annual defic\n",
      "----- diversity: 0.5\n",
      "ns with health insurance, while health-care costs grow at the slowest rate in 50 years; annual deficing, the porins that lan the prodading the fore the prodenting toperem to the and in american farp t\n",
      "----- diversity: 1.2\n",
      "ns with health insurance, while health-care costs grow at the slowest rate in 50 years; annual defictisg toragec now the.\n",
      "\n",
      " fabling sathpughongdustanedaticunolt wishor arsl 0ag stfatens od thourd.biis\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 87s - loss: 1.6850 - val_loss: 2.0808\n",
      "----- generating with seed: aid over 250-times more.\n",
      "\n",
      "economies are more successful when we close the gap between rich and poor \n",
      "----- diversity: 0.5\n",
      "aid over 250-times more.\n",
      "\n",
      "economies are more successful when we close the gap between rich and poor and bet oul of the erans ono the buth and contiritipiss and more resuritn contore tome rat of rofati\n",
      "----- diversity: 1.2\n",
      "aid over 250-times more.\n",
      "\n",
      "economies are more successful when we close the gap between rich and poor degrenve of the ande ixpseve be hauveranc-tuncteb fangurmt-orifnort anmersly, evilosed toce paldimg \n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
