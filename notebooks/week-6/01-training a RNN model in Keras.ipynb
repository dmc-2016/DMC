{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oly and rent-seeking that this newspaper has documented, the failure of businesses to take into acco --> u\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 3.2197 - val_loss: 2.9753\n",
      "----- generating with seed: ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen f\n",
      "----- diversity: 0.5\n",
      "ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen f .ey s s   rei t eao  t tarude ao ea on ee   tro  so e e      a iieteeiesnd dn iroare  a  ts ar rttt\n",
      "----- diversity: 1.2\n",
      "ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fallen funa ecf xaoogfnehdpgatp l9aaituishahana nmtclh;r 19eabkrnau bte0 se,er6sdeh a h ceevdat\n",
      "turcdlvghhbg\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 3.0231 - val_loss: 2.9315\n",
      "----- generating with seed: at home or abroad, people ask me the same question: what is happening in the american political syst\n",
      "----- diversity: 0.5\n",
      "at home or abroad, people ask me the same question: what is happening in the american political systm efee a i e ror s   o eos  le ete ee l eo reon g i nhtermh n oa i poateny    t  ti emnfs aer im    \n",
      "----- diversity: 1.2\n",
      "at home or abroad, people ask me the same question: what is happening in the american political systiwa0np fddn psun-goou \n",
      "ei lth eteiowoshskivtr swgm9ts eicvceiktt j9 gryes yf tgyaoireio9hpo.:hachii \n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.9583 - val_loss: 2.8649\n",
      "----- generating with seed:  write. it must be one of economic growth thats not only sustainable but shared. to achieve it ameri\n",
      "----- diversity: 0.5\n",
      " write. it must be one of economic growth thats not only sustainable but shared. to achieve it ameri dr rtoys n  ees en  tes e te  arse rea ee irno mee  ant , eeespne ags reh n oce aea n ata s t ecet \n",
      "----- diversity: 1.2\n",
      " write. it must be one of economic growth thats not only sustainable but shared. to achieve it ameria :c  ghrag,vwp lekihnrscthd efdsn at au.shcp rob sde fs bfe igvnntl wrmdd\n",
      " se guutrw on edoyieewd t\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.8514 - val_loss: 2.7542\n",
      "----- generating with seed:  affordable care acts progress in reducing health-care costs and limiting tax breaks for the most fo\n",
      "----- diversity: 0.5\n",
      " affordable care acts progress in reducing health-care costs and limiting tax breaks for the most fo too ne neoe  one enr ee ans loi r gr na u e tar an  e let aes te te laonl ino os es noe ath  treais\n",
      "----- diversity: 1.2\n",
      " affordable care acts progress in reducing health-care costs and limiting tax breaks for the most foreen fk i:eec t mlsttnehs tuuintfilc  lorolal\n",
      "f, gatnbeerl rle-eoeso lh tm  wredt. gmen rfnsmtlg9yai\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.7489 - val_loss: 2.6540\n",
      "----- generating with seed: n in any since the 1970s. these gains would have been impossible without the globalisation and techn\n",
      "----- diversity: 0.5\n",
      "n in any since the 1970s. these gains would have been impossible without the globalisation and techntinan ine lrite aand ntate ton ar thars an ind lh oetr te fisul to te aan te ahe ea the  oery oots o\n",
      "----- diversity: 1.2\n",
      "n in any since the 1970s. these gains would have been impossible without the globalisation and technatveobe.rsno adr hthotat ind phergartf, ehed wesj1me acuear  naian egn met hvtale ohn5gne jovbndy1nf\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.6554 - val_loss: 2.5730\n",
      "----- generating with seed: ed by some americans today echoes nativist lurches of the pastthe alien and sedition acts of 1798, t\n",
      "----- diversity: 0.5\n",
      "ed by some americans today echoes nativist lurches of the pastthe alien and sedition acts of 1798, tin oonin tins ae dor utios \n",
      "n ansl ine porotho e font ales ohon at ain sheo tho th ore whs ull ino  \n",
      "----- diversity: 1.2\n",
      "ed by some americans today echoes nativist lurches of the pastthe alien and sedition acts of 1798, tir ayenn cod ; toecal fwg, nu r,se lnnnal hutitr-llvotiha eiion in unsfret s,hth 0hfn sfora veetiiit\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.5864 - val_loss: 2.5092\n",
      "----- generating with seed: ons i sought and congress forced austerity on the economy prematurely by threatening a historic debt\n",
      "----- diversity: 0.5\n",
      "ons i sought and congress forced austerity on the economy prematurely by threatening a historic debtint oar the gnces thl oe ue tor ron ere ac hos ian ion re on hoote  fe inen cedures tor tars tad kor\n",
      "----- diversity: 1.2\n",
      "ons i sought and congress forced austerity on the economy prematurely by threatening a historic debtesate monog4roant tpofty t rotu raxioe od bipodu,hapri.e. xnenuho arracmoeerio or was lucplim wmprhi\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.5245 - val_loss: 2.4570\n",
      "----- generating with seed: eived 7 of all after-tax income. by 2007, that share had more than doubled to 17. this challenges th\n",
      "----- diversity: 0.5\n",
      "eived 7 of all after-tax income. by 2007, that share had more than doubled to 17. this challenges the thos ind aos tore fere in ion the cone uf the ar tor an an wooo fitis oald on the os rarete  oitl \n",
      "----- diversity: 1.2\n",
      "eived 7 of all after-tax income. by 2007, that share had more than doubled to 17. this challenges thevpmettg owlle  e areret msb lv itus , pr ing foiftmin,etad noviensiui  hrhamhernnclu-njans and7alis\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.4843 - val_loss: 2.4253\n",
      "----- generating with seed: w pay more of their fair share, tax changes enacted during my administration have increased the shar\n",
      "----- diversity: 0.5\n",
      "w pay more of their fair share, tax changes enacted during my administration have increased the share ans pals ate en iono cand aus tre artint en manterecans tho and the wate santo thes boces bore ar \n",
      "----- diversity: 1.2\n",
      "w pay more of their fair share, tax changes enacted during my administration have increased the shard ucadrestertal aw in\n",
      "isict gon7cialsucanwirnoicolnlissithas asmmodaty sa3,l oh uloumnta i\n",
      "dxat anrt\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.4532 - val_loss: 2.3903\n",
      "----- generating with seed: ate how best to build on these rules, but denying that progress leaves us more vulnerable, not less \n",
      "----- diversity: 0.5\n",
      "ate how best to build on these rules, but denying that progress leaves us more vulnerable, not less be the cans incilican rreant res and on one te the pores the pe bee the cale wres fore tho th on tho\n",
      "----- diversity: 1.2\n",
      "ate how best to build on these rules, but denying that progress leaves us more vulnerable, not less vte1aswd -odra8fprkheenr hbnosigt ky cote . pays an,-he. de who.stye0, rg. toiy kics 1o imhmomn dent\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.4121 - val_loss: 2.3602\n",
      "----- generating with seed: l challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable econom\n",
      "----- diversity: 0.5\n",
      "l challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable economein and the the an ardeert and and to the sore the the te sors the wins an mate manin  andethe son w\n",
      "----- diversity: 1.2\n",
      "l challenges without sacrificing investments in growth and opportunity.\n",
      "\n",
      "finally, sustainable economevranthefy\n",
      " scarxinessae socwgun9 tho. 6o pgomhs 3o bauiny afchsigurit?y  f pewcicon3 lhesscin sisx \n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.3847 - val_loss: 2.3378\n",
      "----- generating with seed: a critical role. they help workers get a bigger slice of the pie but they need to be flexible enough\n",
      "----- diversity: 0.5\n",
      "a critical role. they help workers get a bigger slice of the pie but they need to be flexible enoughet incomise hat fhares we ave cor ind can mencigintinate the the tore sumere al ouncend tant wian or\n",
      "----- diversity: 1.2\n",
      "a critical role. they help workers get a bigger slice of the pie but they need to be flexible enoughilhiontuted fnoxt avnomt  hevdefot. ca. raibke rouces amlive.om icnderbdesnastoe taald erefesy thes \n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.3630 - val_loss: 2.3092\n",
      "----- generating with seed: to prepare for negative shocks before they occur. with todays low interest rates, fiscal policy must\n",
      "----- diversity: 0.5\n",
      "to prepare for negative shocks before they occur. with todays low interest rates, fiscal policy must ane les ant the corereg th ar the sares an wer incing the 1on roce tho anle ing an anconiting the m\n",
      "----- diversity: 1.2\n",
      "to prepare for negative shocks before they occur. with todays low interest rates, fiscal policy mustrates .re-wlygn duting umolltadl\n",
      "dgkullstacbsth meontero astievesing titcetg on beolictoalcicanien a\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.3366 - val_loss: 2.2880\n",
      "----- generating with seed: or jobs since early 2010; rising wages, falling poverty, and the beginnings of a reversal in inequal\n",
      "----- diversity: 0.5\n",
      "or jobs since early 2010; rising wages, falling poverty, and the beginnings of a reversal in inequale the wore the alr ous an thale bes on the and reare pasd ine bemining ins pord the weome an were an\n",
      "----- diversity: 1.2\n",
      "or jobs since early 2010; rising wages, falling poverty, and the beginnings of a reversal in inequalise2og heebperatnoxnolo an :er-ithe 1 be4ikcaselinnssroiyexteit faes ons poeminf peswerin ybost zhes\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.3163 - val_loss: 2.2701\n",
      "----- generating with seed: e pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentim\n",
      "----- diversity: 0.5\n",
      "e pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiming hes leode core mot and antite the sore is pos and chesperit an lo unetitinican wor the saces the\n",
      "----- diversity: 1.2\n",
      "e pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentimgos mage or tyatithy sfoouf posiceno cad reaias ;fovees bthhirodont.\n",
      "coin orsg avsanateno. ne esaund\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.2944 - val_loss: 2.2500\n",
      "----- generating with seed: were constrained by a greater degree of social interaction between employees at all levelsat church,\n",
      "----- diversity: 0.5\n",
      "were constrained by a greater degree of social interaction between employees at all levelsat church, te the an the and the pint retion the pad the when micant in thas ine the and ind the res hun son e\n",
      "----- diversity: 1.2\n",
      "were constrained by a greater degree of social interaction between employees at all levelsat church,e d. wel 50es7, tres, fad chtozrit, prate lempontit wowt toye wpatf anlsye-pmafing-palttyene ic ha b\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.2671 - val_loss: 2.2324\n",
      "----- generating with seed: -esteem, physical health and mortality. it is related to a devastating rise of opioid abuse and an a\n",
      "----- diversity: 0.5\n",
      "-esteem, physical health and mortality. it is related to a devastating rise of opioid abuse and an ald the the ant mion  are bes on wore for and erter and the ars or alpeos an rout res un the reand th\n",
      "----- diversity: 1.2\n",
      "-esteem, physical health and mortality. it is related to a devastating rise of opioid abuse and an arp are uisy age y02or, op,tt olurr ats apeacan t ialelvotogedmyicengbidy cavermapceterradscy iminana\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.2452 - val_loss: 2.2199\n",
      "----- generating with seed: ?\n",
      "\n",
      "further progress requires recognising that americas economy is an enormously complicated mechanis\n",
      "----- diversity: 0.5\n",
      "?\n",
      "\n",
      "further progress requires recognising that americas economy is an enormously complicated mechanis the bate the thet pors antiling wing the the the ther the aran the the for the by the thed the wor \n",
      "----- diversity: 1.2\n",
      "?\n",
      "\n",
      "further progress requires recognising that americas economy is an enormously complicated mechanisn ad arine cenosredanmane: by. to beicino ped atentc3-csle the ad. metic etsist retyice om mls3 elew\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.2329 - val_loss: 2.2064\n",
      "----- generating with seed: ren, particularly for infrastructure; and a political system so partisan that previously bipartisan \n",
      "----- diversity: 0.5\n",
      "ren, particularly for infrastructure; and a political system so partisan that previously bipartisan s por the sot in raves be por the porting bes in ares han be te an ing an the tot in are eond the  o\n",
      "----- diversity: 1.2\n",
      "ren, particularly for infrastructure; and a political system so partisan that previously bipartisan ixsealan\n",
      "od pingerte romibtre. tliran irkpistore. to werte the it the pertigce ecptoy, theug davhuid\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.2182 - val_loss: 2.1997\n",
      "----- generating with seed: onomies or press forward, acknowledging the inequality that can come with globalisation while commit\n",
      "----- diversity: 0.5\n",
      "onomies or press forward, acknowledging the inequality that can come with globalisation while commitino hest om colise pore cassint fall andeen to the and ederene the were werd ol whal orecanter tins \n",
      "----- diversity: 1.2\n",
      "onomies or press forward, acknowledging the inequality that can come with globalisation while commitj,, regals pidlunct linsy akentncltiegl ovulrcabl lenerisa2ider ardenylwalte aoc poove nasttire., fs\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.1955 - val_loss: 2.1810\n",
      "----- generating with seed: ound in the financial sector, instead of applying their talents to innovating in the real economy. a\n",
      "----- diversity: 0.5\n",
      "ound in the financial sector, instead of applying their talents to innovating in the real economy. acane wor gest an ahe rote thas tor and congeting in the to and ic ons on incale resstreting mar cal \n",
      "----- diversity: 1.2\n",
      "ound in the financial sector, instead of applying their talents to innovating in the real economy. aod. kar oregcil uns recitomisloro crmoftiol, bdeus ef alkem wan. thae tee poat mare calit. enrgryoqe\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.1658 - val_loss: 2.1725\n",
      "----- generating with seed: in anxiety over the forces of globalisation, immigration, technology, even change itself, has taken \n",
      "----- diversity: 0.5\n",
      "in anxiety over the forces of globalisation, immigration, technology, even change itself, has taken to the alty the teale and be fore court parige so thad the  hore the the nored the bate this pad the\n",
      "----- diversity: 1.2\n",
      "in anxiety over the forces of globalisation, immigration, technology, even change itself, has taken fhet, the svadomes cferugabsivelncentatealye adss orvey agdomsndecanr 0invercane theivthaavim0 lorei\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.1512 - val_loss: 2.1673\n",
      "----- generating with seed: increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind\n",
      "----- diversity: 0.5\n",
      "increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind inthe tho thar bes pore the the an and copomicing conone tho the and under tore ment the patising t\n",
      "----- diversity: 1.2\n",
      "increasingly understands that they are no longer too big to fail. and we created a first-of-its-kind congrest, uns ylefgronast und ondublly. pardy anc relsto urcen od fun, lels ges, con peuseforttr ve\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.1292 - val_loss: 2.1510\n",
      "----- generating with seed: estoreand that, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the\n",
      "----- diversity: 0.5\n",
      "estoreand that, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the werd ant on and and calitica tho of the ter sore the and butian incong the rante cor bate that dati\n",
      "----- diversity: 1.2\n",
      "estoreand that, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over thepr terw6rons 7n chisicon-oydec\n",
      "atuxind tos negregeane cangsensticistaulsiwul nicice a diecl obticmli\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 2.1173 - val_loss: 2.1491\n",
      "----- generating with seed: nterest rates, fiscal policy must play a bigger role in combating future downturns; monetary policy \n",
      "----- diversity: 0.5\n",
      "nterest rates, fiscal policy must play a bigger role in combating future downturns; monetary policy and tor con the pat the port or ar hand the prove tho fure the lest the sowhing yece and worke on re\n",
      "----- diversity: 1.2\n",
      "nterest rates, fiscal policy must play a bigger role in combating future downturns; monetary policy riane jihtr.t-siss hald butp ert ul. ud bureili, theod-tins fu-tre tae ino.:s ece oo thad f.xeatisad\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.0933 - val_loss: 2.1326\n",
      "----- generating with seed: ture; and a political system so partisan that previously bipartisan ideas like bridge and airport up\n",
      "----- diversity: 0.5\n",
      "ture; and a political system so partisan that previously bipartisan ideas like bridge and airport up ald conte the proven and to furle the sonte the palderens for thas and worke and the wher conomitis\n",
      "----- diversity: 1.2\n",
      "ture; and a political system so partisan that previously bipartisan ideas like bridge and airport uproptiga so-fol 196bstratty.\n",
      "\n",
      "res uztr cict amy anrico conge whepdesemss sus lmo pingity misrivay the\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.0775 - val_loss: 2.1324\n",
      "----- generating with seed: aster in real terms during this business cycle than in any since the 1970s. these gains would have b\n",
      "----- diversity: 0.5\n",
      "aster in real terms during this business cycle than in any since the 1970s. these gains would have bould and les to oun the wore os an the can eal on alld te the pat anine al the the ros but in the ra\n",
      "----- diversity: 1.2\n",
      "aster in real terms during this business cycle than in any since the 1970s. these gains would have bouticin,t\n",
      "es if onth 20 hevardeingusti vloy-alby taade porlese mre work ly pupcresstisth\n",
      "tel toimirg\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.0562 - val_loss: 2.1271\n",
      "----- generating with seed: le. we dont begrudge success, we aspire to it and admire those who achieve it. in fact, weve often a\n",
      "----- diversity: 0.5\n",
      "le. we dont begrudge success, we aspire to it and admire those who achieve it. in fact, weve often as ald in comerons wo ker ale and to har for ensers to hive tae rever ins in the and ore the the labe\n",
      "----- diversity: 1.2\n",
      "le. we dont begrudge success, we aspire to it and admire those who achieve it. in fact, weve often athe pre seraing sonems-tictcotibinn intryestaticudsercy an thet orte incecaysw.rid 1le\n",
      "mingromecouyi\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 2.0384 - val_loss: 2.1178\n",
      "----- generating with seed: new but just as the child in a slum can see the skyscraper nearby, technology allows anyone with a s\n",
      "----- diversity: 0.5\n",
      "new but just as the child in a slum can see the skyscraper nearby, technology allows anyone with a soroung the sat on prising the to buse on the fas the fur and mere conter as in the werl and the bali\n",
      "----- diversity: 1.2\n",
      "new but just as the child in a slum can see the skyscraper nearby, technology allows anyone with a s tho prithenecagaleemt of is of whes edimaition tisereiseld pepvorice sof on ay whor chfottiings in \n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.0262 - val_loss: 2.1190\n",
      "----- generating with seed: ften seem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many a\n",
      "----- diversity: 0.5\n",
      "ften seem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many are ant more ant the hof the for wer for the rowe move for werl ad the wat an soul eond the ats and b\n",
      "----- diversity: 1.2\n",
      "ften seem to live by a different set of rules to ordinary citizens.\n",
      "\n",
      "so its no wonder that so many as anoui andee finsteat and amlining haset ant  puprdincprestiy incwals txakl oip or pre?ustial ine1s\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.0120 - val_loss: 2.1041\n",
      "----- generating with seed: insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by near\n",
      "----- diversity: 0.5\n",
      "insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by neare and rescrestaris con licis and reugrento the to bere of heverseng the the rave gorken the that of \n",
      "----- diversity: 1.2\n",
      "insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by near begfo gepeauimt ans t8e rhobilede fogerant bullodany 2vel sonpendading s ono nevulis tss balw acari\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 1.9915 - val_loss: 2.1100\n",
      "----- generating with seed: n, we will have boosted incomes for families in the bottom fifth of the income distribution by 18 by\n",
      "----- diversity: 0.5\n",
      "n, we will have boosted incomes for families in the bottom fifth of the income distribution by 18 by the ins ono ale alde the fare so chage some the tas in the the ad yath proble the hever the the har\n",
      "----- diversity: 1.2\n",
      "n, we will have boosted incomes for families in the bottom fifth of the income distribution by 18 by, insiltithe anp abpuitita? r,tupet in fully fuer, an l24w2rce haremetist ad inindsrad tow ox ons th\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9708 - val_loss: 2.1055\n",
      "----- generating with seed: r idea that was threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but s\n",
      "----- diversity: 0.5\n",
      "r idea that was threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but so th in the 1909f on the ere for murich in wht as sercentent ale ans the but on mecomengat of intere\n",
      "----- diversity: 1.2\n",
      "r idea that was threatening america under control. we overcame those fears and we will again.\n",
      "\n",
      "but sxelo e tho wexcy in the that batirsillost.\n",
      "\n",
      "thesume un fmendeuy.\n",
      "oae w fimper af ferxamatee chable n\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9533 - val_loss: 2.1044\n",
      "----- generating with seed:  america. its not new, nor is it dissimilar to a discontent spreading throughout the world, often ma\n",
      "----- diversity: 0.5\n",
      " america. its not new, nor is it dissimilar to a discontent spreading throughout the world, often mane ard al anderens on seare to ese reate to the dering the werkes in that dore that the regale and t\n",
      "----- diversity: 1.2\n",
      " america. its not new, nor is it dissimilar to a discontent spreading throughout the world, often maolreald ebanemy acr, ozcins, nesd probtore; o5 :rive it avand\n",
      "min tore thaw fous oh ffrb bao bed yen\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9369 - val_loss: 2.1169\n",
      "----- generating with seed: o secure a decent wage. too many potential physicists and engineers spend their careers shifting mon\n",
      "----- diversity: 0.5\n",
      "o secure a decent wage. too many potential physicists and engineers spend their careers shifting mons reall rourden the wurs of the rever in reant the moren ant whal dese pors in pootien with of to er\n",
      "----- diversity: 1.2\n",
      "o secure a decent wage. too many potential physicists and engineers spend their careers shifting monanley ors4t lotalinve ant racthagcneng tr1k-ecunsdimg to\n",
      "co1e., ge.windve. bich seolong peustor of m\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9184 - val_loss: 2.0974\n",
      "----- generating with seed: es. decades of declining productivity growth and rising inequality have resulted in slower income gr\n",
      "----- diversity: 0.5\n",
      "es. decades of declining productivity growth and rising inequality have resulted in slower income growth from anerteas an hale on the the fan meon and work dever ce to the fat in oud forte the the for\n",
      "----- diversity: 1.2\n",
      "es. decades of declining productivity growth and rising inequality have resulted in slower income grttreg. buve poy astuliitg is wowe 9detl ads pakingy lhas gromerta soflo wo kur ih now tea p oof nke \n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.8941 - val_loss: 2.0999\n",
      "----- generating with seed: that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and anti-refugee \n",
      "----- diversity: 0.5\n",
      "that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and anti-refugee to sould the porged ses and werken com ale aver incol the proinco tor tor alle the ant in the the ba\n",
      "----- diversity: 1.2\n",
      "that are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and anti-refugee thitimar eforutiy bpoadlst andpallts the dadesat jaancilad od, s?owbithits bad ey menity wode waykty\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.8792 - val_loss: 2.0972\n",
      "----- generating with seed: aying field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, inequality ha\n",
      "----- diversity: 0.5\n",
      "aying field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, inequality han beted an the anseriming resenste challe sevendent tho torpere th the wald where amere of in ale po\n",
      "----- diversity: 1.2\n",
      "aying field for workers and businesses alike.\n",
      "\n",
      "second, alongside slowing productivity, inequality hav3 haw but worrksed gig thlrewant ,athon, hasl wtha us ryctallicl anirl. ta. sa13 nhe. usehards nor \n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.8649 - val_loss: 2.1009\n",
      "----- generating with seed: t 1. without a faster-growing economy, we will not be able to generate the wage gains people want, r\n",
      "----- diversity: 0.5\n",
      "t 1. without a faster-growing economy, we will not be able to generate the wage gains people want, ralene the the ins ald and tich oromed in presting yead in alle ase souce of the the ans averece the \n",
      "----- diversity: 1.2\n",
      "t 1. without a faster-growing economy, we will not be able to generate the wage gains people want, rakens gopdencttexiacing weonld takanter hanobriigarevinote naawhingutareung se. burwy, mesuunors van\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.8486 - val_loss: 2.1025\n",
      "----- generating with seed: e the world some measure of hope. despite all manner of division and discord, a second great depress\n",
      "----- diversity: 0.5\n",
      "e the world some measure of hope. despite all manner of division and discord, a second great depressing acores fital the rase sout comuliss a desl the  more wher wher woul resarteon the wand reatirisi\n",
      "----- diversity: 1.2\n",
      "e the world some measure of hope. despite all manner of division and discord, a second great depress at realfr conznrs for catire pow\n",
      "roves aides buty esinaiug pronintery al oudlo aky issalvinge weklg\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.8286 - val_loss: 2.1053\n",
      "----- generating with seed: silient economy, one that grows sustainably without plundering the future at the service of the pres\n",
      "----- diversity: 0.5\n",
      "silient economy, one that grows sustainably without plundering the future at the service of the prestire of wore wal the tor as moth pronthat plobutheat on the incalise cas and incene the ande to bull\n",
      "----- diversity: 1.2\n",
      "silient economy, one that grows sustainably without plundering the future at the service of the presetrejst moth ie besy?p allity igcac oa gors acreeg congbutumerismors in apherepferins wast id erabik\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.8173 - val_loss: 2.0906\n",
      "----- generating with seed:  participation among prime-age workers see chart 3. in 1953, just 3 of men between 25 and 54 years o\n",
      "----- diversity: 0.5\n",
      " participation among prime-age workers see chart 3. in 1953, just 3 of men between 25 and 54 years of the fon parines are and aled tho faning the for emerich for colles and the prover sing thear hald \n",
      "----- diversity: 1.2\n",
      " participation among prime-age workers see chart 3. in 1953, just 3 of men between 25 and 54 years ouz hich u6forfel fan ar?o nieps pretpextto borudt, dicloisen se walo het sexest of the ialed chind n\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.7944 - val_loss: 2.1097\n",
      "----- generating with seed: t aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising that \n",
      "----- diversity: 0.5\n",
      "t aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising that and int and as and in buteder insrecinis the thal the prosting thai that tho form now bit the asd pa\n",
      "----- diversity: 1.2\n",
      "t aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising that fhanlering an verkentodainc, licco condeunte nve? ele tiom bredes urver con bte thborimant kecponime\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.7887 - val_loss: 2.1077\n",
      "----- generating with seed: d economies, with that increase most pronounced in the united states. in 1979, the top 1 of american\n",
      "----- diversity: 0.5\n",
      "d economies, with that increase most pronounced in the united states. in 1979, the top 1 of american thes that the for furtira in tor ins revering the lavente conduties of the antiless the enaste of t\n",
      "----- diversity: 1.2\n",
      "d economies, with that increase most pronounced in the united states. in 1979, the top 1 of americans cfamer chaldecory wucr it ruladd; on greiting atedudisisant ycoms chombliatyome taeniseming ontwii\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 70s - loss: 1.7550 - val_loss: 2.0940\n",
      "----- generating with seed: han many other nations because we are convinced that with hard work, we can improve our own station \n",
      "----- diversity: 0.5\n",
      "han many other nations because we are convinced that with hard work, we can improve our own station the ande to bet be the reases and bulise to mere wark as anderend the sofutidisn the rewing the pore\n",
      "----- diversity: 1.2\n",
      "han many other nations because we are convinced that with hard work, we can improve our own station ogle sto leso tus mens pesffibien waker, suvedee wor  balelh om redanicges wiyt assl-cesming an dol9\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.7527 - val_loss: 2.1067\n",
      "----- generating with seed: er than governments can deliver and a pervasive sense of injustice undermines peoples faith in the s\n",
      "----- diversity: 0.5\n",
      "er than governments can deliver and a pervasive sense of injustice undermines peoples faith in the sercould porting to ard ansertecing mey to ming the tome por corture and aled the forme and prinathee\n",
      "----- diversity: 1.2\n",
      "er than governments can deliver and a pervasive sense of injustice undermines peoples faith in the secfnestiong in wale towerete tiv nopestiof the faulla-insgatity ols wer deave yof tobye, mong tra3 r\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.7422 - val_loss: 2.1152\n",
      "----- generating with seed: e, tax changes enacted during my administration have increased the share of income received by all o\n",
      "----- diversity: 0.5\n",
      "e, tax changes enacted during my administration have increased the share of income received by all oncert of morica lote that nes peod in ant a lore ald the thar the the lese the the fabere far the in\n",
      "----- diversity: 1.2\n",
      "e, tax changes enacted during my administration have increased the share of income received by all oxuhe sas sice pheslen dmenom-bigeninpaletions the wernta is lyecisnon, if initacls, cisumiin thith i\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.7135 - val_loss: 2.1196\n",
      "----- generating with seed: ther than better, it is important to remember that capitalism has been the greatest driver of prospe\n",
      "----- diversity: 0.5\n",
      "ther than better, it is important to remember that capitalism has been the greatest driver of prospertiant ond the leversed the forles on will thear workes eroncens thee the tours on the lowart the in\n",
      "----- diversity: 1.2\n",
      "ther than better, it is important to remember that capitalism has been the greatest driver of prospeating over mone baneg tome olrt cmustit ghourd we ktuln and horce. batingay seapeis, uf the pestimol\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.7080 - val_loss: 2.1152\n",
      "----- generating with seed: and better oversight for a range of institutions and markets. big american financial institutions no\n",
      "----- diversity: 0.5\n",
      "and better oversight for a range of institutions and markets. big american financial institutions nos murede. the easticas rase workers secthont the andese the rate in uceres of the igeration, in amer\n",
      "----- diversity: 1.2\n",
      "and better oversight for a range of institutions and markets. big american financial institutions noquentilies. eshanes pi9 welu elotonby gowitho tidnuj rewstof, with dofstm suprays. cos hejvew cos or\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.6865 - val_loss: 2.1288\n",
      "----- generating with seed: ts, and ensuring men and women get equal pay for equal work would help to move us in the right direc\n",
      "----- diversity: 0.5\n",
      "ts, and ensuring men and women get equal pay for equal work would help to move us in the right directoro lare worke. the portoment and ande canere sericans are tor ins ored the tomire and andeqiality \n",
      "----- diversity: 1.2\n",
      "ts, and ensuring men and women get equal pay for equal work would help to move us in the right direcs. toan fmam nongeti-inges y peotpe amel. werlas heved lowe gass noi thid mes\n",
      "\n",
      "ydhamicus buthzen are\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
