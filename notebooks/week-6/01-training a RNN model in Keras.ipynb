{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ombating rising inequality, ensuring that everyone who wants a job can get one and building a resili --> e\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 3.2222 - val_loss: 3.0082\n",
      "----- generating with seed: ransformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the para\n",
      "----- diversity: 0.5\n",
      "ransformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the paran eiy nati tse  t n ed   u oebeia t  o  he ener s  re tmgei  hooonht tt rsty    o n  mo gr h  re\n",
      " e \n",
      "----- diversity: 1.2\n",
      "ransformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the parayrlr l  le-hrlt:iroetrkhcer,ryleh1rutuvtnrln;hthlxnrtityt5ew ovt ow -sb6elirbcoeanofteorme0tt ueaycd\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 94s - loss: 3.0153 - val_loss: 2.9691\n",
      "----- generating with seed: previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could also help pr\n",
      "----- diversity: 0.5\n",
      "previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could also help prfe lt efoai t  tste  ughr soe bco ocoa ofo ns tdit  r  ct e etsio ot etl roeem ooene  oi ttn e w t n\n",
      "----- diversity: 1.2\n",
      "previously bipartisan ideas like bridge and airport upgrades are nonstarters.\n",
      "\n",
      "we could also help prjes\n",
      "ipaim hdc nziagifs 7rw-c hemot3otar m\n",
      "c oonssi bdd hy iacorla  o0rp uavlwoyss hieedropa etircol \n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.9642 - val_loss: 2.9191\n",
      "----- generating with seed: nding new jobs would assist. so would making unemployment insurance available to more workers. paid \n",
      "----- diversity: 0.5\n",
      "nding new jobs would assist. so would making unemployment insurance available to more workers. paid ao  at eimcene  ea  ja ee  sa a  rciaoti tlt dfo oeeefaertheieltteeae n ireoe r on   ott et hes e ai\n",
      "----- diversity: 1.2\n",
      "nding new jobs would assist. so would making unemployment insurance available to more workers. paid sreedark etrrwlttefoaonmfu yhaim.  d ea gskmcwa r b edrqcd1pe\n",
      " sibooteiog epvswyihr7evefifidalsmdwnm\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 2.8929 - val_loss: 2.8318\n",
      "----- generating with seed: o 17. this challenges the very essence of who americans are as a people. we dont begrudge success, w\n",
      "----- diversity: 0.5\n",
      "o 17. this challenges the very essence of who americans are as a people. we dont begrudge success, wtnon  oe ioetn e aei aar  ae eos e toe ee ess oea ditie ess tooe veie  the aie poe  sse  ni e erdi l\n",
      "----- diversity: 1.2\n",
      "o 17. this challenges the very essence of who americans are as a people. we dont begrudge success, wtd fir-pwtneusj aac ate.fi 2ihraso oidlei2hechra  s6esreaehmpea oerft\n",
      "ojv eegass hef6?mys  deoher nh\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 2.7876 - val_loss: 2.7199\n",
      "----- generating with seed: 979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had more\n",
      "----- diversity: 0.5\n",
      "979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had more me int on ahe th as pfm criee aont ra tte aan eree care cp fne terr ar lt as a taps un pe tdean sot\n",
      "----- diversity: 1.2\n",
      "979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had moreoslo fctiryatge sfdumcpars bti hr an\n",
      "dir tin dohrexebcrplb fhu tor nhhe,wpsa rgdws.afes ha.rjetnnanu\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 116s - loss: 2.6893 - val_loss: 2.6241\n",
      "----- generating with seed: th business-tax reform that lowers statutory rates and closes loopholes, and with public investments\n",
      "----- diversity: 0.5\n",
      "th business-tax reform that lowers statutory rates and closes loopholes, and with public investments ire te tle ane meas aoi ts and eohe phalerien on antin srsiauli thu eret ontte ritie ate eati t he \n",
      "----- diversity: 1.2\n",
      "th business-tax reform that lowers statutory rates and closes loopholes, and with public investmentsggsbwesb  mer.eo wakm fond hiu. on hgyfmcun uoupswchzli ton merp agingbli thrrkulr  e re rae we cmsu\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 112s - loss: 2.6036 - val_loss: 2.5552\n",
      "----- generating with seed: globalisation, immigration, technology, even change itself, has taken hold in america. its not new, \n",
      "----- diversity: 0.5\n",
      "globalisation, immigration, technology, even change itself, has taken hold in america. its not new, int ae t are ponte ate sn bele on  at ore on ont aes the aedm ans fos an aconi ns oremocicbnton sate\n",
      "----- diversity: 1.2\n",
      "globalisation, immigration, technology, even change itself, has taken hold in america. its not new, aay faelauindy kernl 9erioak tho .oncnghel abaede8tefoia apa fm.d cakerha\n",
      "iapromde ev-mraseal y ol4t\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.5402 - val_loss: 2.4956\n",
      "----- generating with seed: ourselves to making the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a for\n",
      "----- diversity: 0.5\n",
      "ourselves to making the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a for and an as ore an come the the thin sos an berito the ore and to ae  odote eun eot oormepomles ratis\n",
      "----- diversity: 1.2\n",
      "ourselves to making the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a for-uinsa bagmede .afs retheaty have  cfmowti 4fwucosy begvowy ted dhkemuoce thibgsd 9ocha pere lesaper\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.4920 - val_loss: 2.4584\n",
      "----- generating with seed: d that the work of perfecting our union would take far longer. the presidency is a relay race, requi\n",
      "----- diversity: 0.5\n",
      "d that the work of perfecting our union would take far longer. the presidency is a relay race, requins whe  aren the prethe an ther meanticon be toe thad int ore an the sas the prorit uris ar anm ang \n",
      "----- diversity: 1.2\n",
      "d that the work of perfecting our union would take far longer. the presidency is a relay race, requivl woedd ulliinvaali.d fee oe at,m,rcamak the irfrer, sa mle tnwmboltianf af oceetpridren,oe acorhge\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 2.4566 - val_loss: 2.4156\n",
      "----- generating with seed: nd the rise of populist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that a\n",
      "----- diversity: 0.5\n",
      "nd the rise of populist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that an faltind to the than tho wor th eomeng the to t on pon the than ccandens an tual the the soro it an\n",
      "----- diversity: 1.2\n",
      "nd the rise of populist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that aas tiat foptdi\n",
      "taa mhei foca6nis eqbrftif m roppele th f net hn lesisy:. bt pertin anpmlmoe tondepat\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.4156 - val_loss: 2.3895\n",
      "----- generating with seed: e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with th\n",
      "----- diversity: 0.5\n",
      "e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with thitit  nouret ant an th fore aon the at hor eresltuming and fithe the sentit and anthe the tat thas a\n",
      "----- diversity: 1.2\n",
      "e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with the came ovesd bossss-dvefessrllceis mof anrcaeatbbasot urint rfavehe dteqgre cearaybebe 1fut elator m\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 110s - loss: 2.3851 - val_loss: 2.3563\n",
      "----- generating with seed: ocused on education are critical both for increasing economic growth and for ensuring that it is sha\n",
      "----- diversity: 0.5\n",
      "ocused on education are critical both for increasing economic growth and for ensuring that it is shas the whe conthe wo that an an the the ton e cored the whe ind fot an the angite mere canis the foru\n",
      "----- diversity: 1.2\n",
      "ocused on education are critical both for increasing economic growth and for ensuring that it is shaels.s ans, man rigegico imant omitisgg, guclutifr nt worher. bpigo thec artesthrs.o le tu ed cceor e\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 2.3563 - val_loss: 2.3310\n",
      "----- generating with seed: d power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector jobs\n",
      "----- diversity: 0.5\n",
      "d power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector jobs the eande withe them and alu the autin prose fo fon thas res. al or and ratic an ore an are thas ch\n",
      "----- diversity: 1.2\n",
      "d power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector jobsicad mof parepail am rengaclcadeestat maoy rine grto atmifnitug-ntt9c,?eyctregart savews prodest all\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 2.3341 - val_loss: 2.3043\n",
      "----- generating with seed: families in the bottom fifth of the income distribution by 18 by 2017, while raising the average tax\n",
      "----- diversity: 0.5\n",
      "families in the bottom fifth of the income distribution by 18 by 2017, while raising the average tax ore an the palls in artins an fout bale te who the the ant core inssene  heas in conte an tha in an\n",
      "----- diversity: 1.2\n",
      "families in the bottom fifth of the income distribution by 18 by 2017, while raising the average taxne to; o 1gfarud ecsamis -omes8 oudsbuse tye cmkartos ave yuosatd in whratce fon fas eroolitoi atrin\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 113s - loss: 2.3037 - val_loss: 2.2787\n",
      "----- generating with seed: le and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring \n",
      "----- diversity: 0.5\n",
      "le and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring of ardece te prome thal whare the thant an buritis mes to alico the that and ind recinn in paritis s\n",
      "----- diversity: 1.2\n",
      "le and put back together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring wf thesregdy. inom okr pevets mesly g anr kichis, ow th sufsele ivoi\n",
      "tu, afo.l \n",
      "yap..\n",
      "inisngy galmel\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.2849 - val_loss: 2.2603\n",
      "----- generating with seed: es are nonstarters.\n",
      "\n",
      "we could also help private investment and innovation with business-tax reform t\n",
      "----- diversity: 0.5\n",
      "es are nonstarters.\n",
      "\n",
      "we could also help private investment and innovation with business-tax reform the tome sande tho sereting wer and ans the forte an the patida dusty and fin le ore we an resing the\n",
      "----- diversity: 1.2\n",
      "es are nonstarters.\n",
      "\n",
      "we could also help private investment and innovation with business-tax reform tho boprol agaulipo  \n",
      "rbamm canee.. \n",
      "res. te 1a lnotceleng fsilesust eag9 inior.\n",
      "tild ors lusaby and \n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.2653 - val_loss: 2.2436\n",
      "----- generating with seed: es with greater inequality. concentrated wealth at the top means less of the broad-based consumer sp\n",
      "----- diversity: 0.5\n",
      "es with greater inequality. concentrated wealth at the top means less of the broad-based consumer spore in nores and ans conter as ore who g on and congatit ean the and eans porotumen thal ans in ans \n",
      "----- diversity: 1.2\n",
      "es with greater inequality. concentrated wealth at the top means less of the broad-based consumer spversthee. ane or1n3, ixoylesee.,.r bicant,om wofrdtoengi wolaty an, bery 7us. thap o99 yljegress ano\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.2304 - val_loss: 2.2365\n",
      "----- generating with seed: ays acknowledged that the work of perfecting our union would take far longer. the presidency is a re\n",
      "----- diversity: 0.5\n",
      "ays acknowledged that the work of perfecting our union would take far longer. the presidency is a rere sore the an mor1 nod the with and resessted and inccantiig the then buthe anthe in the sessens th\n",
      "----- diversity: 1.2\n",
      "ays acknowledged that the work of perfecting our union would take far longer. the presidency is a rerermer mmernennoty veralien fartworin lmig4l yexacdepaven thahayc5r veuytanltra9sulc an fdomiet eoei\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 119s - loss: 2.2207 - val_loss: 2.2153\n",
      "----- generating with seed: hen we close the gap between rich and poor and growth is broadly based. a world in which 1 of humani\n",
      "----- diversity: 0.5\n",
      "hen we close the gap between rich and poor and growth is broadly based. a world in which 1 of humanit of thin thave to manduriclity the wor thes and and wart por are sande the poresing og or the erong\n",
      "----- diversity: 1.2\n",
      "hen we close the gap between rich and poor and growth is broadly based. a world in which 1 of humanikasde thed erealx a desuol fgploteing rt hz havet worace  us inguoneq,-eles gavile anqpes scomodics \n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 110s - loss: 2.1959 - val_loss: 2.2067\n",
      "----- generating with seed: y by threatening a historic debt default. my successors should not have to fight for emergency measu\n",
      "----- diversity: 0.5\n",
      "y by threatening a historic debt default. my successors should not have to fight for emergency measure to ereast oh ale on al eoprond tar inone tha desanitingoritith and alisy and toit an the eansemut\n",
      "----- diversity: 1.2\n",
      "y by threatening a historic debt default. my successors should not have to fight for emergency measun\n",
      "thales live lace aonteverellery griabure  onsctuceis s. thalidnovt 1fillsbretteanala,y.\n",
      "\n",
      "itimalea \n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.1771 - val_loss: 2.1911\n",
      "----- generating with seed: ality; 20m more americans with health insurance, while health-care costs grow at the slowest rate in\n",
      "----- diversity: 0.5\n",
      "ality; 20m more americans with health insurance, while health-care costs grow at the slowest rate in and the profition his porties the the the luster sored the als reale tha the at on the ale in to th\n",
      "----- diversity: 1.2\n",
      "ality; 20m more americans with health insurance, while health-care costs grow at the slowest rate in smuat finnthepaltse camthipas w hekconec f amiore ti th sexeain un icoquamtuciras arei-thoes unkovl\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 2.1574 - val_loss: 2.1754\n",
      "----- generating with seed: ed the position of workers and their ability to secure a decent wage. too many potential physicists \n",
      "----- diversity: 0.5\n",
      "ed the position of workers and their ability to secure a decent wage. too many potential physicists and ander and ane pares our the the ant ilise sconot, on the pantins the with ore masencans te ther \n",
      "----- diversity: 1.2\n",
      "ed the position of workers and their ability to secure a decent wage. too many potential physicists iond doequat inorimas witw-haseat on paptode  forndrcinnsmed es to greoly\n",
      " yonle more were hoh thimi\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 2.1329 - val_loss: 2.1708\n",
      "----- generating with seed:  to fail. and we created a first-of-its-kind watchdogthe consumer financial protection bureauto hold\n",
      "----- diversity: 0.5\n",
      " to fail. and we created a first-of-its-kind watchdogthe consumer financial protection bureauto hold fat ce the wor hes ore the pandes the the worle the the parter and anteress im more has the sfered \n",
      "----- diversity: 1.2\n",
      " to fail. and we created a first-of-its-kind watchdogthe consumer financial protection bureauto hold, everte fut ovolut2es prowgetweat th iupleg aver indles anenpfratzed te restheos ome istlobmiropntt\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.1239 - val_loss: 2.1517\n",
      "----- generating with seed: economics can be overridden by bad politics. my administration secured much more fiscal expansion th\n",
      "----- diversity: 0.5\n",
      "economics can be overridden by bad politics. my administration secured much more fiscal expansion the armerining contite that for als, and ingmealis thet andime tho hand for amerost and in one for ome\n",
      "----- diversity: 1.2\n",
      "economics can be overridden by bad politics. my administration secured much more fiscal expansion thes uprd id icnate\n",
      "frorur, the mopta bfot-menichand pmeecincin fedives woht romeraks axsusengiov ches\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.1128 - val_loss: 2.1367\n",
      "----- generating with seed: the most privileged live. expectations rise faster than governments can deliver and a pervasive sens\n",
      "----- diversity: 0.5\n",
      "the most privileged live. expectations rise faster than governments can deliver and a pervasive sensen the provcenthe the particito in the and ald partica the and recenting the on pording of and on me\n",
      "----- diversity: 1.2\n",
      "the most privileged live. expectations rise faster than governments can deliver and a pervasive sensd ianlty fomolling mer1fr\n",
      "ermars in the lemin dh hansh,sbs, at ancessst amurutcmfon erases, fhaldiig\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.0854 - val_loss: 2.1289\n",
      "----- generating with seed: eem, physical health and mortality. it is related to a devastating rise of opioid abuse and an assoc\n",
      "----- diversity: 0.5\n",
      "eem, physical health and mortality. it is related to a devastating rise of opioid abuse and an assocly the por in the pote the bate he leante the fust poull the portititg net an shes wer he pantinn ba\n",
      "----- diversity: 1.2\n",
      "eem, physical health and mortality. it is related to a devastating rise of opioid abuse and an assoce invpigprtatiain ann;. andntubjoits lomittbicy surss is and, ,iuns rowrtt pard theus tre f cnmod. r\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.0608 - val_loss: 2.1382\n",
      "----- generating with seed: ble frustration, much of it fanned by politicians who would actually make the problem worse rather t\n",
      "----- diversity: 0.5\n",
      "ble frustration, much of it fanned by politicians who would actually make the problem worse rather than ale and the wore the the ingritis the to es one comer the esorica the ard alitian by enanos of t\n",
      "----- diversity: 1.2\n",
      "ble frustration, much of it fanned by politicians who would actually make the problem worse rather the unsbrto ficr veonheosd 3tre oh 2yhe. 1094. o\n",
      "eobme rboiuit thans more raly ant vescalmu\n",
      "taparingy\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 110s - loss: 2.0498 - val_loss: 2.1184\n",
      "----- generating with seed: would add flexibility for employees and employers. reforms to our criminal-justice system and improv\n",
      "----- diversity: 0.5\n",
      "would add flexibility for employees and employers. reforms to our criminal-justice system and improvten and at congore serecens oud palinge and and reas in more the gat the the prote the buste and ica\n",
      "----- diversity: 1.2\n",
      "would add flexibility for employees and employers. reforms to our criminal-justice system and improvemingmons ialdreity in r siculs abt oble ve sasbes vasticm bes be tem car eptompe aivr wath rive nen\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 94s - loss: 2.0366 - val_loss: 2.1104\n",
      "----- generating with seed: ing money around in the financial sector, instead of applying their talents to innovating in the rea\n",
      "----- diversity: 0.5\n",
      "ing money around in the financial sector, instead of applying their talents to innovating in the reade to the ware hat hak fromentoriss to manicg onneragthean here and we pout the tope the patitiin en\n",
      "----- diversity: 1.2\n",
      "ing money around in the financial sector, instead of applying their talents to innovating in the reas frtepue lxoindi-gsuitino, oburull., tthice oncadee ond de decumiot watto turl sajing amditmasta:di\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.0096 - val_loss: 2.1111\n",
      "----- generating with seed:  without dependent children, limiting tax breaks for high-income households, preventing colleges fro\n",
      "----- diversity: 0.5\n",
      " without dependent children, limiting tax breaks for high-income households, preventing colleges frow res and a loret renand and the porte in the ander and to and wio the mork and wore uns roge the to\n",
      "----- diversity: 1.2\n",
      " without dependent children, limiting tax breaks for high-income households, preventing colleges frow. um anw sew mys. it pap nrewerlhald trisg potincurtas mo ve tes pestructon ta uf obes om aic sedre\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.0032 - val_loss: 2.1027\n",
      "----- generating with seed: my prematurely by threatening a historic debt default. my successors should not have to fight for em\n",
      "----- diversity: 0.5\n",
      "my prematurely by threatening a historic debt default. my successors should not have to fight for emoros the hus cont ene wet erons mear sande man the of tre of the lisand tha the and ropord the poond\n",
      "----- diversity: 1.2\n",
      "my prematurely by threatening a historic debt default. my successors should not have to fight for emirtidmive, bethave moxe mas, rcerent forkenty. thliriss one omatica codle,.\n",
      "bregiis ta indringf neph\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 1.9889 - val_loss: 2.0955\n",
      "----- generating with seed: hout costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-loade\n",
      "----- diversity: 0.5\n",
      "hout costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-loade st in content the gion al the and in the pore the rose of the and an the porot on the gor bat of th\n",
      "----- diversity: 1.2\n",
      "hout costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-loade sudiec and curm patiticaity .5 bapbicine gooh be ol thevebuded stoutivar mosjonfyw-fparirigat hes i\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 1.9639 - val_loss: 2.0990\n",
      "----- generating with seed: results are clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; ri\n",
      "----- diversity: 0.5\n",
      "results are clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; ris whe arease the andace and workerg the sound th ad protice and the forube ins and in the antore and\n",
      "----- diversity: 1.2\n",
      "results are clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; rissse feingh, axcsmanst in ofrer jodmnim1. tof in hourld intres incute gorade me. fualere ftte fis ra\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.9452 - val_loss: 2.0922\n",
      "----- generating with seed: gree of social interaction between employees at all levelsat church, at their childrens schools, in \n",
      "----- diversity: 0.5\n",
      "gree of social interaction between employees at all levelsat church, at their childrens schools, in the prost mork the cond and the pportift ald enveres on whid whar ald and the pensort for the and th\n",
      "----- diversity: 1.2\n",
      "gree of social interaction between employees at all levelsat church, at their childrens schools, in curnod is yuaddi.\n",
      "\n",
      "\n",
      "\n",
      "aisker ne that the fird merotinniscod thisbvine s0tr te mpate mesluned. thac o \n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.9324 - val_loss: 2.0854\n",
      "----- generating with seed: f us to do our part to bring the country closer to its highest aspirations. so where does my success\n",
      "----- diversity: 0.5\n",
      "f us to do our part to bring the country closer to its highest aspirations. so where does my successer the for ardere not our contine the nore cops come the beitis on that and mes in the pood the pati\n",
      "----- diversity: 1.2\n",
      "f us to do our part to bring the country closer to its highest aspirations. so where does my successudat. thit 18 yy alkorens predatitm ard ont noris-fof livo hto. the f parbsserauf thj merpig hingidu\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 1.9140 - val_loss: 2.0876\n",
      "----- generating with seed: -hit families and the economy, like unemployment insurance, should rise automatically.\n",
      "\n",
      "maintaining \n",
      "----- diversity: 0.5\n",
      "-hit families and the economy, like unemployment insurance, should rise automatically.\n",
      "\n",
      "maintaining the toane hald an the income the worker as the lobe the eas ouc for the and in mere the aris gatinn \n",
      "----- diversity: 1.2\n",
      "-hit families and the economy, like unemployment insurance, should rise automatically.\n",
      "\n",
      "maintaining fofly mitanes nopwers sqtire seadlp lpanealkgt. il sy wen along, be pheced crualg. an a cabrult hon \n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 1.9022 - val_loss: 2.0822\n",
      "----- generating with seed: about 20- to 30-times as much as their average worker. the reduction or elimination of this constrai\n",
      "----- diversity: 0.5\n",
      "about 20- to 30-times as much as their average worker. the reduction or elimination of this constrait so se but the pored that are the tha besed the five than erono sund the wart rese the inche acinit\n",
      "----- diversity: 1.2\n",
      "about 20- to 30-times as much as their average worker. the reduction or elimination of this constrait sher fareccial tipw tobnamles -won da8titit: conamicar fattry tak dininw tonseqmilensipat, forccea\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.8836 - val_loss: 2.0777\n",
      "----- generating with seed: tands that they are no longer too big to fail. and we created a first-of-its-kind watchdogthe consum\n",
      "----- diversity: 0.5\n",
      "tands that they are no longer too big to fail. and we created a first-of-its-kind watchdogthe consumen shese conticm are and nobury rese grond the ald the palice sore the wer sot the worle and cemensy\n",
      "----- diversity: 1.2\n",
      "tands that they are no longer too big to fail. and we created a first-of-its-kind watchdogthe consumeins..\n",
      "\n",
      "enoblays 2013 tarlaicoe dnganeel have pant.\n",
      "6oe rodwen the morsure or iol mitier for fatlobu\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 1.8629 - val_loss: 2.0790\n",
      "----- generating with seed:  past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am prou\n",
      "----- diversity: 0.5\n",
      " past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am prout or ome to the pore the work we the and and in oul for the retion the ander of the poredt and peali\n",
      "----- diversity: 1.2\n",
      " past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decades in the making. while i am prout enorce, nee patd ad doprevthe thak the anitiy afd por fia thoul ogd wy cheseling-atrebmeney coroec\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.8378 - val_loss: 2.0818\n",
      "----- generating with seed: and social progress. the progress of the past eight years should also give the world some measure of\n",
      "----- diversity: 0.5\n",
      "and social progress. the progress of the past eight years should also give the world some measure of rowe bes ingre ger of the poplthes unding to the potion thot american rase can inte tor ager and pa\n",
      "----- diversity: 1.2\n",
      "and social progress. the progress of the past eight years should also give the world some measure of thas ny8 anstriming cumdledivis..\n",
      "ternicel,, store. leimass foe bpatity and ehpoomprh toclnt ven mo\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.8324 - val_loss: 2.0744\n",
      "----- generating with seed: icies focused on education are critical both for increasing economic growth and for ensuring that it\n",
      "----- diversity: 0.5\n",
      "icies focused on education are critical both for increasing economic growth and for ensuring that it has sica the and of enormest for angere portudem in on the to mofe the uforet in the sof the anf ex\n",
      "----- diversity: 1.2\n",
      "icies focused on education are critical both for increasing economic growth and for ensuring that itrehany ive ton pyowter sand it proutee myhatd-imay hy aqepaliting utocismy wane amnecinttey.\n",
      "\n",
      "avencp\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 102s - loss: 1.8108 - val_loss: 2.0893\n",
      "----- generating with seed: gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decade\n",
      "----- diversity: 0.5\n",
      "gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decade in the and be te-fenturces and in more parted for erever thas ard or in fourte reveredes the whal i\n",
      "----- diversity: 1.2\n",
      "gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has been decader ec-nomirisg fowthr.eg wemy moneprouvastan. culsorlconessen pot weed to cedreat, ind ansigbliwity i\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.7894 - val_loss: 2.0779\n",
      "----- generating with seed: er be any doubt that a free market only thrives when there are rules to guard against systemic failu\n",
      "----- diversity: 0.5\n",
      "er be any doubt that a free market only thrives when there are rules to guard against systemic failun the warkes be work the tow tha poration for forte in the wert or over and ancorees in for icantiti\n",
      "----- diversity: 1.2\n",
      "er be any doubt that a free market only thrives when there are rules to guard against systemic failund 2f, amsion uul, aty aclrevantems the koit gresans yyay l wasirmessols  onver uvhingetidl. dingrlt\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 1.7742 - val_loss: 2.0893\n",
      "----- generating with seed: tween rich and poor are not new but just as the child in a slum can see the skyscraper nearby, techn\n",
      "----- diversity: 0.5\n",
      "tween rich and poor are not new but just as the child in a slum can see the skyscraper nearby, technonesses the pavered the walk the resince and while hesled and andiliss to and the hane to the rowe t\n",
      "----- diversity: 1.2\n",
      "tween rich and poor are not new but just as the child in a slum can see the skyscraper nearby, technongamenoy andy thichnegemiy to bundvalaling ent eiples. wn a dtheredion o mone rfomonit ig oad chill\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 1.7633 - val_loss: 2.1006\n",
      "----- generating with seed: out of the labour force. today, it is 26. people joining or rejoining the workforce in a strengtheni\n",
      "----- diversity: 0.5\n",
      "out of the labour force. today, it is 26. people joining or rejoining the workforce in a strengthenist the wald neal bust of and with gre areing the and mere that shore. ithand a foreer for and reas b\n",
      "----- diversity: 1.2\n",
      "out of the labour force. today, it is 26. people joining or rejoining the workforce in a strengthenith ygred. h pvevelant wculo escereno for 19as net menandy veredadigrivate, wrolhilvistun thas apy pe\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.7473 - val_loss: 2.0930\n",
      "----- generating with seed: e seen in britains recent vote to leave the european union and the rise of populist parties around t\n",
      "----- diversity: 0.5\n",
      "e seen in britains recent vote to leave the european union and the rise of populist parties around the a pecondert for the pean more and iver the to beate in oul the past economat profost and and beto\n",
      "----- diversity: 1.2\n",
      "e seen in britains recent vote to leave the european union and the rise of populist parties around tte , beppds f tebheles th of mratom-ming ifuibpitizins 2hisdenjiys te. im allseo grogt, to lraves an\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 109s - loss: 1.7382 - val_loss: 2.0877\n",
      "----- generating with seed: powerful force for the common good, driving businesses to create products that consumers rave about \n",
      "----- diversity: 0.5\n",
      "powerful force for the common good, driving businesses to create products that consumers rave about and of the list prigitithont storean in ans to ame in the bes mever and the pastandent thes one domi\n",
      "----- diversity: 1.2\n",
      "powerful force for the common good, driving businesses to create products that consumers rave about eof iom castips ay ea to as; acinartee ast westancasde ress in libs tow.\n",
      "\n",
      "tyally has bheresusnd nxp-\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.7127 - val_loss: 2.1025\n",
      "----- generating with seed: hange itself, has taken hold in america. its not new, nor is it dissimilar to a discontent spreading\n",
      "----- diversity: 0.5\n",
      "hange itself, has taken hold in america. its not new, nor is it dissimilar to a discontent spreading the poutt and on the buplobly to eed of the has frece more ever but our dorting the workes and the \n",
      "----- diversity: 1.2\n",
      "hange itself, has taken hold in america. its not new, nor is it dissimilar to a discontent spreading in omp tichitt--int aftisorsgotinn frombismaot sand pon butjeitx tfatins were emaro a ger am ryeim \n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 1.7071 - val_loss: 2.1006\n",
      "----- generating with seed: l people.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead re\n",
      "----- diversity: 0.5\n",
      "l people.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead restoress the rage the pogees that than the prosting atd and efponting the forted to noble to the and \n",
      "----- diversity: 1.2\n",
      "l people.\n",
      "\n",
      "instead, fully restoring faith in an economy where hardworking americans can get ahead reperlias eriust ymeresith hosy ouplecharvew rerlestis, wouc indrfatimity, watl huive. thes hap has re\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 1.6824 - val_loss: 2.0914\n",
      "----- generating with seed: the tax changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well sh\n",
      "----- diversity: 0.5\n",
      "the tax changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well shart the porserto and recention that progeting it astrican on or contining thith and buting then the \n",
      "----- diversity: 1.2\n",
      "the tax changes in any previous administration since at least 1960.\n",
      "\n",
      "even these efforts fall well shuld than w reoblly, gre tho tem pirto-aif eforos -londem-trance tih ave has folgrigg wirh raclica fo\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
