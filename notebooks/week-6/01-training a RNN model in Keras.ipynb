{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ut energy-sector emissions by 6, even as our economy has grown by 11 see chart 4. progress in americ --> a\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 49s - loss: 3.1911 - val_loss: 2.9652\n",
      "----- generating with seed: d a major role. in the past, differences in pay between corporate executives and their workers were \n",
      "----- diversity: 0.5\n",
      "d a major role. in the past, differences in pay between corporate executives and their workers were  u t ien  ea oaamtl eto  tnnatces nt rh vo etasini nit we eie in or c  duaitan t  iea  tr antetsde e\n",
      "----- diversity: 1.2\n",
      "d a major role. in the past, differences in pay between corporate executives and their workers were mt l0s tlsn.eakcg  rm\n",
      "teugt nti9cd, ttoiooa ak t rla ;c8a  rfnrauercrtgdikeh0k5dshbhssteniw--;nsttto\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 3.0172 - val_loss: 2.9324\n",
      "----- generating with seed: uard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street have \n",
      "----- diversity: 0.5\n",
      "uard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street have e  ancaintit aenesa   eaa  oae t e gsat  s se res i ic i ratt lan nfa  cst e ton    rroaasgso nn ine\n",
      "----- diversity: 1.2\n",
      "uard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street have ngrpernunivatps6 nesrkpint8 nr u t9igiroe8  ihgmton fko uotnytwdush s beer r sdtfsfhipfbi lfs \n",
      " eyia\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 49s - loss: 2.9633 - val_loss: 2.8812\n",
      "----- generating with seed: e presidency is a relay race, requiring each of us to do our part to bring the country closer to its\n",
      "----- diversity: 0.5\n",
      "e presidency is a relay race, requiring each of us to do our part to bring the country closer to itsps ala les  aot   re   iliiti tt  ntnet igtor a  ain t roatpe  oce eth  en ec ent es  he oon tarl in\n",
      "----- diversity: 1.2\n",
      "e presidency is a relay race, requiring each of us to do our part to bring the country closer to itsod nwec e;exait  rwansr qahgten-  vreygllcivuorscoohk.iel.1eeedie t jt rse,h2tysto a d rrdtshootetcc\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 2.8845 - val_loss: 2.7754\n",
      "----- generating with seed: ers see chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. to\n",
      "----- diversity: 0.5\n",
      "ers see chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. toniu  cate in ctocte onorat t oe tao ce rta tte fin oed rreh orre iho  or yte rorbd te  oane tall eo \n",
      "----- diversity: 1.2\n",
      "ers see chart 3. in 1953, just 3 of men between 25 and 54 years old were out of the labour force. toia  sornirq ioveripaodiuuiuf upxinsintpe r 3eneriresuorlevlis,xobawas .rl orhle. crmsdl h bmwe hpfo \n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.7754 - val_loss: 2.6615\n",
      "----- generating with seed: ont-loaded fiscal stimulus than even president roosevelts new deal and oversaw the most comprehensiv\n",
      "----- diversity: 0.5\n",
      "ont-loaded fiscal stimulus than even president roosevelts new deal and oversaw the most comprehensivs at enuor hee tore te anes on pas seit an mih fcer con nolenh oas rethe art se toren an nore inure \n",
      "----- diversity: 1.2\n",
      "ont-loaded fiscal stimulus than even president roosevelts new deal and oversaw the most comprehensivthemn rsd sniler tuwcrcaunr howod-sine-retesetmmetyewgiyarny- rgds rlyotc aws:almitcaoraltousmi lhdn\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 2.6753 - val_loss: 2.5639\n",
      "----- generating with seed:  development. policies focused on education are critical both for increasing economic growth and for\n",
      "----- diversity: 0.5\n",
      " development. policies focused on education are critical both for increasing economic growth and fore res as re aas tore an e sond pae ion ag the bus get tee tos re oor ao ins er eleane tiat anoute ao\n",
      "----- diversity: 1.2\n",
      " development. policies focused on education are critical both for increasing economic growth and forcof ton  ans\n",
      "teo dhe oooms cs anpetors ompositre re7 bthuai1w teyadlltinl anrulit i6t p vtiscs baad \n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.5934 - val_loss: 2.4902\n",
      "----- generating with seed: bilising our economy. unfortunately, good economics can be overridden by bad politics. my administra\n",
      "----- diversity: 0.5\n",
      "bilising our economy. unfortunately, good economics can be overridden by bad politics. my administras noros the antine ind an an on har ble aocide touuican shas to anl the ar ono al ehe toal ind uceit\n",
      "----- diversity: 1.2\n",
      "bilising our economy. unfortunately, good economics can be overridden by bad politics. my administrait bwthopt-ing wos lte o te fogtec man yocbed.s vmhoalpbemeomun:e9oacme mverl2d y male, eldasu domi \n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.5261 - val_loss: 2.4433\n",
      "----- generating with seed: embraced a crude populism that promises a return to a past that is not possible to restoreand that, \n",
      "----- diversity: 0.5\n",
      "embraced a crude populism that promises a return to a past that is not possible to restoreand that, the toall sonle sade the thar canse inte ins merer the and tina te femer inl the ter fome poines an \n",
      "----- diversity: 1.2\n",
      "embraced a crude populism that promises a return to a past that is not possible to restoreand that, ian  begtdaisr ipte lectse d1s fomas ar unncamtremi aetmict;eccfn lh oi8lenur am olgc?l urteot toed \n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.4764 - val_loss: 2.4013\n",
      "----- generating with seed:  states. in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that s\n",
      "----- diversity: 0.5\n",
      " states. in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that sare an inthe se the acoremcesting on reecthe tins be wole eand ce the the oar inas and were that ant\n",
      "----- diversity: 1.2\n",
      " states. in 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that snmextirb romem1wis ac korcey ans cesuresoke so?e poruee inry mhinougtacsgrtesicr pusgower asc ug fca\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 2.4384 - val_loss: 2.3732\n",
      "----- generating with seed: econd, alongside slowing productivity, inequality has risen in most advanced economies, with that in\n",
      "----- diversity: 0.5\n",
      "econd, alongside slowing productivity, inequality has risen in most advanced economies, with that inge the be that an ar aat ees meres one pond the tre the thatle thes iolo tan ancomem the porlete the\n",
      "----- diversity: 1.2\n",
      "econd, alongside slowing productivity, inequality has risen in most advanced economies, with that ingdwcinsantonc thl. ;ithaliwyachat oand mheltrereo e7ejlbove goses,dinn a9st in., tose wer orn thor e\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.4109 - val_loss: 2.3457\n",
      "----- generating with seed: , at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-times\n",
      "----- diversity: 0.5\n",
      ", at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-times pand tha s are the in the onoun alis iul eve py tho thes to to the ar the engroat . be oulina se th\n",
      "----- diversity: 1.2\n",
      ", at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-timesim smangy 8erion wot ur ristg iasyb a adac in sanmwnndlatireedabicte thmics yocfelsn bt,wurusgfemeve\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.3820 - val_loss: 2.3173\n",
      "----- generating with seed: d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism a\n",
      "----- diversity: 0.5\n",
      "d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism and ane pore the the deald pant ecato th ald crest at ar fer the les wor mere the the the pall ins co\n",
      "----- diversity: 1.2\n",
      "d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism ay anqudtilst,\n",
      "inb y9taly ooswbmelbemor\n",
      "inessartowlhbl jlthint riyenit yhatoani\n",
      "s\n",
      "ndoptg le n bcen 3a\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.3523 - val_loss: 2.2957\n",
      "----- generating with seed: not continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progre\n",
      "----- diversity: 0.5\n",
      "not continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progre worus on the gar on bere an toe the the tho acreincentint ans rome the to an the forwe the the and \n",
      "----- diversity: 1.2\n",
      "not continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progreus bcolanmongiun mra0t kaoka ge ubissdutsiunty thesmpofonmin smorld trec acsi ig thss us, movar dy.i\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.3282 - val_loss: 2.2792\n",
      "----- generating with seed: rden of stabilising our economy. unfortunately, good economics can be overridden by bad politics. my\n",
      "----- diversity: 0.5\n",
      "rden of stabilising our economy. unfortunately, good economics can be overridden by bad politics. my ehte the and perter an thanle ghan 4re ingredes in pretot on mare an re che the d aod the and are a\n",
      "----- diversity: 1.2\n",
      "rden of stabilising our economy. unfortunately, good economics can be overridden by bad politics. mytey tad;tuast hat koty. inycwellis cor 8sett, sadms buredee theom eunane wej\n",
      "wthupid f3.t porneorime\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 49s - loss: 2.3039 - val_loss: 2.2613\n",
      "----- generating with seed: a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent y\n",
      "----- diversity: 0.5\n",
      "a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent y beas of incers and the onores mase tho n3es and the poreche store te the and the mevereng in rapre \n",
      "----- diversity: 1.2\n",
      "a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent youd gboutr-stoeses of damunedt, np puriorysaled se thloden?niny th cl;auning pureincltakisgwand fyei\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.2796 - val_loss: 2.2434\n",
      "----- generating with seed: t yet substantially boosted measured productivity growth. over the past decade, america has enjoyed \n",
      "----- diversity: 0.5\n",
      "t yet substantially boosted measured productivity growth. over the past decade, america has enjoyed th of the iang constere cor wer ther of the patins for end the for the wore fore the to and in fores\n",
      "----- diversity: 1.2\n",
      "t yet substantially boosted measured productivity growth. over the past decade, america has enjoyed mus  e0ssumg fote saorl ar1-attore veat abpedeicmmpowt0psy volm atur theh ls, soog the ofy palles qe\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.2514 - val_loss: 2.2264\n",
      "----- generating with seed:  successful when we close the gap between rich and poor and growth is broadly based. this is not jus\n",
      "----- diversity: 0.5\n",
      " successful when we close the gap between rich and poor and growth is broadly based. this is not just mere corest has the ins the the hand withe the the for the patistiris anoum arecand the and hald t\n",
      "----- diversity: 1.2\n",
      " successful when we close the gap between rich and poor and growth is broadly based. this is not just sasshing, rasine oho copuhlideve wain ogioncenes, eoucad west t. then sanst. ale9wplt ooviwonnses,\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.2271 - val_loss: 2.2092\n",
      "----- generating with seed: ed a major role. in the past, differences in pay between corporate executives and their workers were\n",
      "----- diversity: 0.5\n",
      "ed a major role. in the past, differences in pay between corporate executives and their workers were mhes ee pore thes enangures enomer bate mor worle singe and decint om the the tha ge the be of arl \n",
      "----- diversity: 1.2\n",
      "ed a major role. in the past, differences in pay between corporate executives and their workers were mgr-egfant5 ie omvilacs, ouse buidlscerstwets sante thab fesktamed to,ry onxincom arcamise pnontt b\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 2.2159 - val_loss: 2.1989\n",
      "----- generating with seed: e and an associated increase in overdose deaths and suicides among non-college-educated americansthe\n",
      "----- diversity: 0.5\n",
      "e and an associated increase in overdose deaths and suicides among non-college-educated americansthe purcon the enould as in wilh ald the enon grome the beatd and withe and un be that ad precone the w\n",
      "----- diversity: 1.2\n",
      "e and an associated increase in overdose deaths and suicides among non-college-educated americansthe thwheu?dod ghave, atky.\n",
      "tuhe int fe waobd sonc-uuds,e,abd graticat foadingm weredecence ufaru pamir\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.1946 - val_loss: 2.1838\n",
      "----- generating with seed: s climate agreement, which presents the best opportunity to save the planet for future generations.\n",
      "\n",
      "----- diversity: 0.5\n",
      "s climate agreement, which presents the best opportunity to save the planet for future generations.\n",
      "\n",
      "at on encertions cortereis and in in oul sy sow yel on elomes and mor the porting co are red predit\n",
      "----- diversity: 1.2\n",
      "s climate agreement, which presents the best opportunity to save the planet for future generations.\n",
      "\n",
      "heveutheve, t7e o0s the motgerelg, lesive eigeht py auntiop bedasuntcaled ontorresssess catins inss\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 2.1750 - val_loss: 2.1690\n",
      "----- generating with seed: f prime-age women were out of the labour force. today, it is 26. people joining or rejoining the wor\n",
      "----- diversity: 0.5\n",
      "f prime-age women were out of the labour force. today, it is 26. people joining or rejoining the worl the at on thes and bevere sor corumes son the and the that the pald tha eroress the pranting are a\n",
      "----- diversity: 1.2\n",
      "f prime-age women were out of the labour force. today, it is 26. people joining or rejoining the wornlo6 by oreioutime sabse fofinmeasion m ionreipabge worgemert encolevatt thac pr orvearidy s cantiri\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.1525 - val_loss: 2.1644\n",
      "----- generating with seed: he income distribution by 18 by 2017, while raising the average tax rates on households projected to\n",
      "----- diversity: 0.5\n",
      "he income distribution by 18 by 2017, while raising the average tax rates on households projected to enout the catrecand the efor ace andaticave tat  hevex the ser of the wor the the bees fardes and t\n",
      "----- diversity: 1.2\n",
      "he income distribution by 18 by 2017, while raising the average tax rates on households projected tok he werd cencutsenfdim -palbly ial fhcf the pwfsrt9uitt\n",
      " andetacoato dhiss  o bicedes at hupp t wt \n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.1331 - val_loss: 2.1453\n",
      "----- generating with seed: es, can fail. this can happen through the tendency towards monopoly and rent-seeking that this newsp\n",
      "----- diversity: 0.5\n",
      "es, can fail. this can happen through the tendency towards monopoly and rent-seeking that this newsprest of and resturt the prowend are wore of ore the the on alle tale and the are ard and bicing ol l\n",
      "----- diversity: 1.2\n",
      "es, can fail. this can happen through the tendency towards monopoly and rent-seeking that this newspebtepriqt aid focd anidy 1agsouis coptoly io ingrurine thatts npapeonicajs gforte monsiin on wutrome\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.1223 - val_loss: 2.1453\n",
      "----- generating with seed: ght embraced a crude populism that promises a return to a past that is not possible to restoreand th\n",
      "----- diversity: 0.5\n",
      "ght embraced a crude populism that promises a return to a past that is not possible to restoreand the aredtins and insumeros manl ins oul prolise for buticisy and seconge to ens concessent dowists con\n",
      "----- diversity: 1.2\n",
      "ght embraced a crude populism that promises a return to a past that is not possible to restoreand th ceding thit b-enutile o? preses1ng, dicilicat cald coty. the fous thingncen aund;li and iulllyasims\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.1058 - val_loss: 2.1256\n",
      "----- generating with seed: ning a historic debt default. my successors should not have to fight for emergency measures in a tim\n",
      "----- diversity: 0.5\n",
      "ning a historic debt default. my successors should not have to fight for emergency measures in a time and the anitimaning walle sulit. the the anated more the werbe and the port the that bat oum to pr\n",
      "----- diversity: 1.2\n",
      "ning a historic debt default. my successors should not have to fight for emergency measures in a tim 12;, th. lsturom r6s anfretigst ysresticaun cutsoon siwe in cienecegol wifilte in niblive of thesua\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.0836 - val_loss: 2.1280\n",
      "----- generating with seed: s we have made, instead choosing to condemn the system as a whole. americans should debate how best \n",
      "----- diversity: 0.5\n",
      "s we have made, instead choosing to condemn the system as a whole. americans should debate how best at an andion eroule cores in the and enoures and wererage the enomeng thes and wore of poldica averi\n",
      "----- diversity: 1.2\n",
      "s we have made, instead choosing to condemn the system as a whole. americans should debate how best s0iicetis treas is nos ene ilirliginansy heowh to tae  fonleqdetpis tavides and mand iopmonting in i\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.0705 - val_loss: 2.1211\n",
      "----- generating with seed: t vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of this \n",
      "----- diversity: 0.5\n",
      "t vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of this anconens on werk for manis-alles bo work ho the pare se pao the the pors at e hall giso eatiin and a\n",
      "----- diversity: 1.2\n",
      "t vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of this igwertemataf tixsulalntr- rim oner-gauty og micoas ud lestn leme. hagmore more farce the avinm an fr\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.0508 - val_loss: 2.1099\n",
      "----- generating with seed: aped by the few and unaccountable to the many is a threat to all. economies are more successful when\n",
      "----- diversity: 0.5\n",
      "aped by the few and unaccountable to the many is a threat to all. economies are more successful when the res more sound to ping to eat enstores and that that of rewary reserthe so wer shatte res const\n",
      "----- diversity: 1.2\n",
      "aped by the few and unaccountable to the many is a threat to all. economies are more successful whend alluden bet calst gzexpardaty fore indyss-amc es2ryetn apsous eri he s.\n",
      "\n",
      "\n",
      "bnutr th gser move colow\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.0316 - val_loss: 2.1042\n",
      "----- generating with seed: ti-muslim and anti-refugee sentiment expressed by some americans today echoes nativist lurches of th\n",
      "----- diversity: 0.5\n",
      "ti-muslim and anti-refugee sentiment expressed by some americans today echoes nativist lurches of the one beitimitical hance angerele ans ale consese tho word the soced consticing in chastist and mare\n",
      "----- diversity: 1.2\n",
      "ti-muslim and anti-refugee sentiment expressed by some americans today echoes nativist lurches of the wemiad nome,enibles untertaudt dhpberernis se sfromuss non urd ald alnist. bleulof car averebcover\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 2.0179 - val_loss: 2.0924\n",
      "----- generating with seed: 1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or rejoini\n",
      "----- diversity: 0.5\n",
      "1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or rejoining and the bating pising to ofat the for and and the pore to eras and in cheteer for are rise of the\n",
      "----- diversity: 1.2\n",
      "1999, 23 of prime-age women were out of the labour force. today, it is 26. people joining or rejoinif sagtais incellyublis for afd aun eot ens.\n",
      "\n",
      "eibo ens yequout oe grfaclisicin puoy the. eesonilisati\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9961 - val_loss: 2.0963\n",
      "----- generating with seed:  guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street hav\n",
      "----- diversity: 0.5\n",
      " guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street have woul prothes to mere mest and recous far wer seveled be past aud alpares fur the that patiting pat\n",
      "----- diversity: 1.2\n",
      " guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis reforms to wall street have foaty calr americho ge6ert ghatd wh fo tarde woll4. takencishew ble wgreghafurd thalnmeruncon witr\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.9769 - val_loss: 2.0860\n",
      "----- generating with seed: ild in a slum can see the skyscraper nearby, technology allows anyone with a smartphone to see how t\n",
      "----- diversity: 0.5\n",
      "ild in a slum can see the skyscraper nearby, technology allows anyone with a smartphone to see how the that ange parte the tho pecono is ast wor for congen des porting and rowe some and to senald and \n",
      "----- diversity: 1.2\n",
      "ild in a slum can see the skyscraper nearby, technology allows anyone with a smartphone to see how thee ddapl2, thead e amingaity adlaicito duld wapr zenfthon sati7s solle mlstty.bum ragyeall the etha\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9642 - val_loss: 2.0892\n",
      "----- generating with seed: red the need for a more resilient economy, one that grows sustainably without plundering the future \n",
      "----- diversity: 0.5\n",
      "red the need for a more resilient economy, one that grows sustainably without plundering the future that insore sortore the best the porstull and andelisens more and thiting to tor goun the incoule be\n",
      "----- diversity: 1.2\n",
      "red the need for a more resilient economy, one that grows sustainably without plundering the future oo thax njo thd 1ve 10tn,kono sg otat lovto-neucd biledto morid alling ua upder-ianty ivitry feis fo\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9463 - val_loss: 2.0769\n",
      "----- generating with seed: estments in basic research and development. policies focused on education are critical both for incr\n",
      "----- diversity: 0.5\n",
      "estments in basic research and development. policies focused on education are critical both for incracing and and economy cand in sour enored pporitit insungeration and with and ardecting to indering \n",
      "----- diversity: 1.2\n",
      "estments in basic research and development. policies focused on education are critical both for incr hatted., lte fonden blin, yuale wath ecoqhialse gnow the dave dotute-ars bo tqeine fyeceo. cbyfect \n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9295 - val_loss: 2.0837\n",
      "----- generating with seed: technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionis\n",
      "----- diversity: 0.5\n",
      "technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectioniss and actof and ian encond in the for for eroum to part deal hes tores. the and and ater of probe at\n",
      "----- diversity: 1.2\n",
      "technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionis, ay anes, tho fver were of tham qasexing-fust the cenoud ivse incon. alure to dveuncanp that mutina\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9121 - val_loss: 2.0814\n",
      "----- generating with seed:  the isolation of corporations and elites, who often seem to live by a different set of rules to ord\n",
      "----- diversity: 0.5\n",
      " the isolation of corporations and elites, who often seem to live by a different set of rules to ord es for and for the leve conte in the fat nom thes in the 19o0, wo les arengerenss in ameromant dong\n",
      "----- diversity: 1.2\n",
      " the isolation of corporations and elites, who often seem to live by a different set of rules to ordtr fretare; aswruned petubiog apabe reriest proiecins.t ef aprenuwing thalb-acleqicenttimy toahersta\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.9014 - val_loss: 2.0771\n",
      "----- generating with seed:  the poverty rate fell faster than at any point since the 1960s. wages have risen faster in real ter\n",
      "----- diversity: 0.5\n",
      " the poverty rate fell faster than at any point since the 1960s. wages have risen faster in real ter jot thal gresing of heas rese and the erofian a bored fore to chonss and were to and conoment intor\n",
      "----- diversity: 1.2\n",
      " the poverty rate fell faster than at any point since the 1960s. wages have risen faster in real terest. be-menades, l ancouncid wats mhouc handf-alejos to eulipn ir chpaidrcca nac wente imated and bu\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.8771 - val_loss: 2.0747\n",
      "----- generating with seed:  only seemed to increase the isolation of corporations and elites, who often seem to live by a diffe\n",
      "----- diversity: 0.5\n",
      " only seemed to increase the isolation of corporations and elites, who often seem to live by a differtee the dusticing the part cons rese the pablit. the botens cens mere the for coreans the portingt \n",
      "----- diversity: 1.2\n",
      " only seemed to increase the isolation of corporations and elites, who often seem to live by a diffever\n",
      ";wn fof th in in ast hame tha borting dumanite bxighas ard, bihumqreacond dfpatdernd8 ac acdabre\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.8533 - val_loss: 2.0765\n",
      "----- generating with seed: working students, and ensuring men and women get equal pay for equal work would help to move us in t\n",
      "----- diversity: 0.5\n",
      "working students, and ensuring men and women get equal pay for equal work would help to move us in the in for and on comeles mer aal son mees and and in the in ous prowing of the por 1979,  e porkes s\n",
      "----- diversity: 1.2\n",
      "working students, and ensuring men and women get equal pay for equal work would help to move us in the wale sicredatinet 4unt, bnist f-ormiss of oun wgrees privitict fnis-mfoudt ed aot centsudit, bula\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.8425 - val_loss: 2.0788\n",
      "----- generating with seed: any other nations because we are convinced that with hard work, we can improve our own station and w\n",
      "----- diversity: 0.5\n",
      "any other nations because we are convinced that with hard work, we can improve our own station and with hes of the recont of reaer stor ald and at to the the sed to the stor that pating in the batith \n",
      "----- diversity: 1.2\n",
      "any other nations because we are convinced that with hard work, we can improve our own station and we hethaitss add incoalislet thas geonisei, thtimass votufarlatsanot, atkerpeo, gyowit eavlotgy, dami\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.8233 - val_loss: 2.0799\n",
      "----- generating with seed: in legitimate concerns about long-term economic forces. decades of declining productivity growth and\n",
      "----- diversity: 0.5\n",
      "in legitimate concerns about long-term economic forces. decades of declining productivity growth and rest porther for cansele the prosint a more ur ard a our dector and the eraties and in and in the r\n",
      "----- diversity: 1.2\n",
      "in legitimate concerns about long-term economic forces. decades of declining productivity growth and pard uen yempiand-fir shides. \n",
      "endrkteasd gepldyines awsout, the tant edulit sam rovely:? ge peop w\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.8194 - val_loss: 2.0726\n",
      "----- generating with seed: america also helped catalyse the historic paris climate agreement, which presents the best opportuni\n",
      "----- diversity: 0.5\n",
      "america also helped catalyse the historic paris climate agreement, which presents the best opportunits and the hearen and ande longert ensement and acenome that pating arde the rowe not were conteme t\n",
      "----- diversity: 1.2\n",
      "america also helped catalyse the historic paris climate agreement, which presents the best opportunimn on the pypyt-imere pulduliol  istrecunos, t ate.\n",
      "\n",
      "l 20.t ddbitibxeaticinhss, prercessticm s1ftems\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.7932 - val_loss: 2.0821\n",
      "----- generating with seed: ing future downturns; monetary policy should not bear the full burden of stabilising our economy. un\n",
      "----- diversity: 0.5\n",
      "ing future downturns; monetary policy should not bear the full burden of stabilising our economy. undings and andering the more thes be and inancanting the pare the panting the fore the the past to ma\n",
      "----- diversity: 1.2\n",
      "ing future downturns; monetary policy should not bear the full burden of stabilising our economy. uncous simonce on work bmitielturysus mate nf bivemud nostectonithad worldonnec storcob tha amreccwpnt\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.7581 - val_loss: 2.0930\n",
      "----- generating with seed: g a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more resi\n",
      "----- diversity: 0.5\n",
      "g a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more resinteritions to ever jon the pares an the enorecen the a porger the the bestores and to second ind chi\n",
      "----- diversity: 1.2\n",
      "g a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more resinnaming purthitifit of aad warkofxple toisd the ,eocf innderenanse thiy nines ridex worges 3iond..\n",
      "\n",
      "\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.7650 - val_loss: 2.0947\n",
      "----- generating with seed: unt the impact of their decisions on others through pollution, the ways in which disparities of info\n",
      "----- diversity: 0.5\n",
      "unt the impact of their decisions on others through pollution, the ways in which disparities of inforcent perediting thein sounced for on econome and betion and dilincest ondering the tore to mere par\n",
      "----- diversity: 1.2\n",
      "unt the impact of their decisions on others through pollution, the ways in which disparities of inforconty tualalnimalt, sreucus fol furantit-ngalcavixgomthod the indrolitiest achint averungite;, cotn\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.7479 - val_loss: 2.0889\n",
      "----- generating with seed: advanced economies see chart 1. without a faster-growing economy, we will not be able to generate th\n",
      "----- diversity: 0.5\n",
      "advanced economies see chart 1. without a faster-growing economy, we will not be able to generate the more the tor and the pars the fon rate reat efor whald bete lont the proster the for the aconom of\n",
      "----- diversity: 1.2\n",
      "advanced economies see chart 1. without a faster-growing economy, we will not be able to generate thiam m hiching on wapd the hald howl whers sur ald biencect orid baxpuling.\n",
      "\n",
      "thith oy prout no gower \n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.7343 - val_loss: 2.0940\n",
      "----- generating with seed:  americans can get ahead requires addressing four major structural challenges: boosting productivity\n",
      "----- diversity: 0.5\n",
      " americans can get ahead requires addressing four major structural challenges: boosting productivity to the ande to for sulice for echarnes hard wert and ansentren ins incoled concres and and and inco\n",
      "----- diversity: 1.2\n",
      " americans can get ahead requires addressing four major structural challenges: boosting productivity- ror worstr canit eforce sfaxuclisg overdecind the incore farp chalke wo mpragians worthe ther ox a\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 48s - loss: 1.7211 - val_loss: 2.0878\n",
      "----- generating with seed: ual chance to get rich with everybody else. thats the problem with increased inequalityit diminishes\n",
      "----- diversity: 0.5\n",
      "ual chance to get rich with everybody else. thats the problem with increased inequalityit diminishes the s ore to to in prodicis for wer of allorest and estine thay presing to epraly the enorteden sto\n",
      "----- diversity: 1.2\n",
      "ual chance to get rich with everybody else. thats the problem with increased inequalityit diminishest fot mobreuestenotpy sonican ase prices avermabtiinsafiiss.\n",
      "\n",
      "fourcc bauta9ghit ancome tirequoricy o\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.7081 - val_loss: 2.0929\n",
      "----- generating with seed:  through the internet, mobile broadband and devices, artificial intelligence, robotics, advanced mat\n",
      "----- diversity: 0.5\n",
      " through the internet, mobile broadband and devices, artificial intelligence, robotics, advanced matiting for and ave the enotse for erecode in and and thete the at and candest and the poot of for ans\n",
      "----- diversity: 1.2\n",
      " through the internet, mobile broadband and devices, artificial intelligence, robotics, advanced mate2 gr owk citseall holk ressedecy fthe intorannings chenram dissctem fisre suvita. unsert and angert\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.6817 - val_loss: 2.1137\n",
      "----- generating with seed: , much of it fanned by politicians who would actually make the problem worse rather than better, it \n",
      "----- diversity: 0.5\n",
      ", much of it fanned by politicians who would actually make the problem worse rather than better, it ande to the bate mout no dable to the porsting and the ardeting to ainits and the enowing or wishoul\n",
      "----- diversity: 1.2\n",
      ", much of it fanned by politicians who would actually make the problem worse rather than better, it houds, kth fur rhacm enaricg, liset depboys, the idenomees mors prosedris cartony, by wehoxsempetire\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
