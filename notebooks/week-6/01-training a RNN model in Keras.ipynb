{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is more fragile and recessions more frequent in countries with greater inequality. concentrated wea --> l\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 3.2344 - val_loss: 2.9762\n",
      "----- generating with seed: s. the progress of the past eight years should also give the world some measure of hope. despite all\n",
      "----- diversity: 0.5\n",
      "s. the progress of the past eight years should also give the world some measure of hope. despite all\n",
      "a eni   ere   nn  ie o esaag v i ehi  e re npromaf lepl e   ine nto r ee  m irenhe  etta  eo  uaiws\n",
      "----- diversity: 1.2\n",
      "s. the progress of the past eight years should also give the world some measure of hope. despite alldentfuutac, suepe9, -yiil-yh ye lt el o.rgomiveskea? r fv,ci eee;idp r:neftrplw diap meod rtoooyann \n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 3.0237 - val_loss: 2.9372\n",
      "----- generating with seed: faster than governments can deliver and a pervasive sense of injustice undermines peoples faith in t\n",
      "----- diversity: 0.5\n",
      "faster than governments can deliver and a pervasive sense of injustice undermines peoples faith in ts n e a esooe if erin onn aa w   tehoga  ent ttsb sron rne ee  s n er e ir t  et  oioto ntn to e rtt\n",
      "----- diversity: 1.2\n",
      "faster than governments can deliver and a pervasive sense of injustice undermines peoples faith in t ns etennnidpnggtlimshe0n  zits\n",
      " jhtt-hor6mfisao ar de,f eptm momombel7xo asls qugr ftrkdaote5atplt \n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.9666 - val_loss: 2.8853\n",
      "----- generating with seed: all on hard times. these include providing wage insurance for workers who cannot get a new job that \n",
      "----- diversity: 0.5\n",
      "all on hard times. these include providing wage insurance for workers who cannot get a new job that e aonf imn nia tr e d,i ehr ioh  oo  eoe  tsees ite o t eas s ae   to b haeer n  et aie re n ai to n\n",
      "----- diversity: 1.2\n",
      "all on hard times. these include providing wage insurance for workers who cannot get a new job that b2lrcua, erepi eqrneedau ofenleorg.ts e:ulfg pthut  si noytrrihhev t n2, aqezoeessarnd\n",
      " ev5ikond re \n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.8774 - val_loss: 2.7620\n",
      "----- generating with seed: -entry into the workforce that have won bipartisan support would also improve participation, if enac\n",
      "----- diversity: 0.5\n",
      "-entry into the workforce that have won bipartisan support would also improve participation, if enacoo  oere  enm e tery go  lo n a te oar tn  aear n an o oos e ea nte a hlio se ein ord st eat gr 1an \n",
      "----- diversity: 1.2\n",
      "-entry into the workforce that have won bipartisan support would also improve participation, if enac\n",
      "a ietn y,dihoivaa 7prowergtiisln ato2 ne aneecs acamd totse nq araiere n njsodt9hea gesyso nvtyeter\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 102s - loss: 2.7595 - val_loss: 2.6611\n",
      "----- generating with seed:  of the expansions i sought and congress forced austerity on the economy prematurely by threatening \n",
      "----- diversity: 0.5\n",
      " of the expansions i sought and congress forced austerity on the economy prematurely by threatening elantsueit  yiceta won ths whet won e sare tot ani limt ion noledu  aor toie th themes not eant eat \n",
      "----- diversity: 1.2\n",
      " of the expansions i sought and congress forced austerity on the economy prematurely by threatening 2ewnndrnbv dimoha mle;ltsebge toyo nnos prrge3tm avyl rterifd- tuu se td onlpfwince firyreginfcsoole\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 2.6698 - val_loss: 2.5624\n",
      "----- generating with seed: ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes f\n",
      "----- diversity: 0.5\n",
      "ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes fos the est anl monm an an in the and the to n cete ceu fan on tovo re smuse the thon the tant the t \n",
      "----- diversity: 1.2\n",
      "ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes fnbt elmtswi7issbcsirow2st cacstdb tren onbcian ceivnt tont noci7\n",
      "s ns sut in toilnt s\n",
      "?gmovs a chhop\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 103s - loss: 2.5778 - val_loss: 2.4880\n",
      "----- generating with seed:  world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty h\n",
      "----- diversity: 0.5\n",
      " world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty han oor por fald the tor ond lop anion ot the mhe omicen sat theron totit ant an ba tap arerits an on\n",
      "----- diversity: 1.2\n",
      " world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty han asd inc. cabdpasye,s ws cote o h oammtrtto1cd dn rtwtidveuatogal -old\n",
      "lpaltnsta3sin whpodse1cedes\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.5264 - val_loss: 2.4361\n",
      "----- generating with seed: lobal economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the profit m\n",
      "----- diversity: 0.5\n",
      "lobal economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the profit maol dot f orore satithe onnd bf eand toe ang the an ar fore gem in cas ao the thar oe the se fore th\n",
      "----- diversity: 1.2\n",
      "lobal economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the profit mlucdnek arittg patat tuf ce.e ?o-w.uvife,du are\n",
      "tnlrice owe aniwd btpuwntssd d-mhod thes car haptthe\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 2.4827 - val_loss: 2.3923\n",
      "----- generating with seed: ard times. these include providing wage insurance for workers who cannot get a new job that pays as \n",
      "----- diversity: 0.5\n",
      "ard times. these include providing wage insurance for workers who cannot get a new job that pays as and en ove tho le innd the cot eces to the the an os ost hand the alt an ore tore the thee ar an ore\n",
      "----- diversity: 1.2\n",
      "ard times. these include providing wage insurance for workers who cannot get a new job that pays as bn on whal tooy.n thed ickabye wtaercy win; an dvanly, the nf thinheigershsweny teis ven anmin aue e\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.4426 - val_loss: 2.3779\n",
      "----- generating with seed: rance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly t\n",
      "----- diversity: 0.5\n",
      "rance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly ta trake pand the ar fore on dorore and eantion nrerealing sranderong then toe wan er ore ande beal f\n",
      "----- diversity: 1.2\n",
      "rance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly tharl mparivdhakee fans rhat ducogs haudmerd thalu er99bruyidtedt:iterot desuures. geamej or war fgen\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 2.4127 - val_loss: 2.3320\n",
      "----- generating with seed: xpand support for the economy when needed and to meet our long-term obligations to our citizens is v\n",
      "----- diversity: 0.5\n",
      "xpand support for the economy when needed and to meet our long-term obligations to our citizens is ver tate the temin, al sore bure an aus ull opel the rerester rostatith th oleres or thore the ticing\n",
      "----- diversity: 1.2\n",
      "xpand support for the economy when needed and to meet our long-term obligations to our citizens is volt 3tetut teo lhbies alyute tha w  urpro1f ard hegari-nor;, mblodi acssdenslit og0dew a. cnon ol re\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.3812 - val_loss: 2.3097\n",
      "----- generating with seed: scal stimulus than even president roosevelts new deal and oversaw the most comprehensive rewriting o\n",
      "----- diversity: 0.5\n",
      "scal stimulus than even president roosevelts new deal and oversaw the most comprehensive rewriting ou the er aute wane teacicato taliin an inangest on ye porit ehe ins ine the ant or the gale ant an t\n",
      "----- diversity: 1.2\n",
      "scal stimulus than even president roosevelts new deal and oversaw the most comprehensive rewriting ou toccoledlise\n",
      "\n",
      "dimerguunn.y \n",
      "alulesa dprechins hoeled 2ribiccusc atd te2rs dend thee id the yheeny \n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.3556 - val_loss: 2.2871\n",
      "----- generating with seed: novations have changed lives, they have not yet substantially boosted measured productivity growth. \n",
      "----- diversity: 0.5\n",
      "novations have changed lives, they have not yet substantially boosted measured productivity growth. the tecant in and forad in sotere mand ing and wicis thet the wer on bule theut or on the that the d\n",
      "----- diversity: 1.2\n",
      "novations have changed lives, they have not yet substantially boosted measured productivity growth. von temuilse gosar:. wthat as, fnsadis ff rcritotlenstes cocgul eog, itis ss ao3y f fcy toviig erdrc\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.3335 - val_loss: 2.2666\n",
      "----- generating with seed:  highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognisin\n",
      "----- diversity: 0.5\n",
      " highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognising the bion gower an ore al ing eeso to mecind the the thane ind corating bored prosane and te as are\n",
      "----- diversity: 1.2\n",
      " highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires recognisinastiay foulm ane-taum, mermeu-ut ?r emarticlarig o4f eiconttige , ractith ued toing codous, becrures\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.3092 - val_loss: 2.2488\n",
      "----- generating with seed: e full burden of stabilising our economy. unfortunately, good economics can be overridden by bad pol\n",
      "----- diversity: 0.5\n",
      "e full burden of stabilising our economy. unfortunately, good economics can be overridden by bad pole the to w rome for ine te d onore for the the res ard and teve wilind tiling ferting the the the an\n",
      "----- diversity: 1.2\n",
      "e full burden of stabilising our economy. unfortunately, good economics can be overridden by bad polilo\n",
      "ant mcaresroed sco6y im tare, t5 ve ry tomm teshises 5d, mes apurisscficind nfep deefivglcs como\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.2908 - val_loss: 2.2300\n",
      "----- generating with seed: never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of globalisation, immigratio\n",
      "----- diversity: 0.5\n",
      "never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of globalisation, immigration ingore in pronget and whal ind the pobte tha an poro pure tore hare the wore mors wor hos the the \n",
      "----- diversity: 1.2\n",
      "never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of globalisation, immigrationg ft7rwinztry wen. demrisist on leor\n",
      "uthe ther em corleco jeo2 coust jat aitehat thigk rowt wupd en\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.2620 - val_loss: 2.2099\n",
      "----- generating with seed: y 20th centuries, and any number of eras in which americans were told they could restore past glory \n",
      "----- diversity: 0.5\n",
      "y 20th centuries, and any number of eras in which americans were told they could restore past glory fore the the ole chare the sinmore the wand the port at tor ereting an wop prestating an deerto in w\n",
      "----- diversity: 1.2\n",
      "y 20th centuries, and any number of eras in which americans were told they could restore past glory ux avinkeg ous chat tsip o lycener hontiignw the mhoktoamco\n",
      " bar pnotme thes, im rsaampraduaven, bul\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.2412 - val_loss: 2.2005\n",
      "----- generating with seed: ctbreaking up all the biggest banks or erecting prohibitively steep tariffs on importsthe economy is\n",
      "----- diversity: 0.5\n",
      "ctbreaking up all the biggest banks or erecting prohibitively steep tariffs on importsthe economy is or and and wald inl eanatiins an presteris the tore the the the the secon the arle to the incande t\n",
      "----- diversity: 1.2\n",
      "ctbreaking up all the biggest banks or erecting prohibitively steep tariffs on importsthe economy isthesapr bhquwcey are tdug aagbeowhiev ecninesver, sutteagas mindesbol hanging comitorh pronea betoe \n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 2.2270 - val_loss: 2.1863\n",
      "----- generating with seed: ical advances through the internet, mobile broadband and devices, artificial intelligence, robotics,\n",
      "----- diversity: 0.5\n",
      "ical advances through the internet, mobile broadband and devices, artificial intelligence, robotics, ane that the betrest in f0re and roncens the and bate thal th on heno toulicas in thal that on wond\n",
      "----- diversity: 1.2\n",
      "ical advances through the internet, mobile broadband and devices, artificial intelligence, robotics, opmergfonesbat oad censeydey th kace famm..\n",
      "\n",
      "ore csent c ancorlo botuetion issr nowe. nstucn the nt\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.2010 - val_loss: 2.1699\n",
      "----- generating with seed: e economy when needed and to meet our long-term obligations to our citizens is vital. curbs to entit\n",
      "----- diversity: 0.5\n",
      "e economy when needed and to meet our long-term obligations to our citizens is vital. curbs to entitian so secon es is ang port are of rene fer an were at and pore the aris and innste the ingress sowe\n",
      "----- diversity: 1.2\n",
      "e economy when needed and to meet our long-term obligations to our citizens is vital. curbs to entite fmicinc-forthag oprugncokthnwy af rewist iccind ge falit idarucemsal h.el. lewhon?p vft at co ate \n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.1883 - val_loss: 2.1582\n",
      "----- generating with seed: at home or abroad, people ask me the same question: what is happening in the american political syst\n",
      "----- diversity: 0.5\n",
      "at home or abroad, people ask me the same question: what is happening in the american political syste the that erine in the thear and ancit the the and rest be to enserily eronoult and icand are the m\n",
      "----- diversity: 1.2\n",
      "at home or abroad, people ask me the same question: what is happening in the american political systy thaub steas ary om, 3iver, te bey atirlo1 for hinlog 20cjutiimevar for uhatios il:uloqlo slofe- aj\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.1620 - val_loss: 2.1467\n",
      "----- generating with seed: tains recent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "mu\n",
      "----- diversity: 0.5\n",
      "tains recent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "mure sormetion sad that and incens recint ald the of incondes it al on the lore sorow challd, fandibie\n",
      "----- diversity: 1.2\n",
      "tains recent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "musdenn.uphcrisit adhurt be0tment of ry guud rowerthag th tes bo eo da7in fsidngeving mercu sale them \n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.1473 - val_loss: 2.1303\n",
      "----- generating with seed: m boosting funding for early childhood education to improving high schools, making college more affo\n",
      "----- diversity: 0.5\n",
      "m boosting funding for early childhood education to improving high schools, making college more afforkert inciniss ace hor wor the deon mere and reald that the e inate hor and and are and and corote t\n",
      "----- diversity: 1.2\n",
      "m boosting funding for early childhood education to improving high schools, making college more afforpers be tece ad the ens wueseranecke. dot manisit ald adeints, aile is notog\n",
      "imlnoldy lape copringi\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.1278 - val_loss: 2.1262\n",
      "----- generating with seed: lly, the financial crisis painfully underscored the need for a more resilient economy, one that grow\n",
      "----- diversity: 0.5\n",
      "lly, the financial crisis painfully underscored the need for a more resilient economy, one that growt ha the grome somend the fore paring of rate rens poo decon ing to en ator of the lase mari in are \n",
      "----- diversity: 1.2\n",
      "lly, the financial crisis painfully underscored the need for a more resilient economy, one that growe thyr mequghrgegtobm cootibe hocppotuin here rolud thiits wu th d heveve eiln avour fav rseut cenim\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.1114 - val_loss: 2.1118\n",
      "----- generating with seed: l pay for equal work would help to move us in the right direction too.\n",
      "\n",
      "\n",
      "third, a successful economy\n",
      "----- diversity: 0.5\n",
      "l pay for equal work would help to move us in the right direction too.\n",
      "\n",
      "\n",
      "third, a successful economy hase wer proveting the wone rofer aring the ald ant amereast chandise for and the thes and and and \n",
      "----- diversity: 1.2\n",
      "l pay for equal work would help to move us in the right direction too.\n",
      "\n",
      "\n",
      "third, a successful economy yatvl. ty anc urnvum losceassiy\n",
      ":chanssint more the gcoswerker ffot echpopdegy acpuralasnquabliyad \n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.0955 - val_loss: 2.1107\n",
      "----- generating with seed:  financial crisis painfully underscored the need for a more resilient economy, one that grows sustai\n",
      "----- diversity: 0.5\n",
      " financial crisis painfully underscored the need for a more resilient economy, one that grows sustain and thee sorss matinges and the perent ders in revet en the enonnt the wer forl weraling ald cenor\n",
      "----- diversity: 1.2\n",
      " financial crisis painfully underscored the need for a more resilient economy, one that grows sustailenvint.\n",
      "\n",
      "y.\n",
      "freaminy, y in enequentress wus opprtald jes, marerilicass fes fof sinevet the rastiy p\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 2.0753 - val_loss: 2.0899\n",
      "----- generating with seed:  seemed to increase the isolation of corporations and elites, who often seem to live by a different \n",
      "----- diversity: 0.5\n",
      " seemed to increase the isolation of corporations and elites, who often seem to live by a different tha porstel to the tor for the ston the thal hall and and and the bothe gor and the butis for whal t\n",
      "----- diversity: 1.2\n",
      " seemed to increase the isolation of corporations and elites, who often seem to live by a different wu lupalu junt. the gesld ce to av indeleismy thes oon vaip ameves p on dorkers den-aked tom0 lvend-\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 110s - loss: 2.0603 - val_loss: 2.0898\n",
      "----- generating with seed: equires addressing four major structural challenges: boosting productivity growth, combating rising \n",
      "----- diversity: 0.5\n",
      "equires addressing four major structural challenges: boosting productivity growth, combating rising and pore the eas the to that restare on reneatice ho the toe iverade and the and the thes to to wer \n",
      "----- diversity: 1.2\n",
      "equires addressing four major structural challenges: boosting productivity growth, combating rising lyes me oue bhaes tre apriseixsiny io eninaito piotiin th cupsdiinca mjongdasylangongsodej.\n",
      "ad ansec\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 2.0405 - val_loss: 2.0798\n",
      "----- generating with seed: ien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the l\n",
      "----- diversity: 0.5\n",
      "ien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the les erpent of and the s the cent o we the  and decan erante hal werl suming that presing the ald an t\n",
      "----- diversity: 1.2\n",
      "ien and sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the loft has fun-peebd baby\n",
      "s;esey y\n",
      "\n",
      "fpay bne paeot shout ald-olle bunons buledlot;los iricanf ment. il \n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 102s - loss: 2.0276 - val_loss: 2.0762\n",
      "----- generating with seed: orms to wall street have made our financial system more stable and supportive of long-term growth, i\n",
      "----- diversity: 0.5\n",
      "orms to wall street have made our financial system more stable and supportive of long-term growth, in the wer for and at ake in and that cour the of the  ather ald ween enore mone whal dopled by inced\n",
      "----- diversity: 1.2\n",
      "orms to wall street have made our financial system more stable and supportive of long-term growth, incols-if dvigt ot chisutsysas ap oud ippidiold of thomberassed bo morees som puatinl fintserimorp th\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.0126 - val_loss: 2.0582\n",
      "----- generating with seed: ile i am proud of what my administration has accomplished these past eight years, i have always ackn\n",
      "----- diversity: 0.5\n",
      "ile i am proud of what my administration has accomplished these past eight years, i have always acknos that pars the the for for than the reses to aldo prealicis to the forte the proves forle and mere\n",
      "----- diversity: 1.2\n",
      "ile i am proud of what my administration has accomplished these past eight years, i have always acknog- hetper. wace arewgrs-inbent moik rad csrotoh of piold suducen og exrcumismy 2qequnhuwy oor dint.\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.9862 - val_loss: 2.0563\n",
      "----- generating with seed: xpectations rise faster than governments can deliver and a pervasive sense of injustice undermines p\n",
      "----- diversity: 0.5\n",
      "xpectations rise faster than governments can deliver and a pervasive sense of injustice undermines puonten and problitiss and ceal of the as are and leat sop the aver gore rever axdinde fare fincom in\n",
      "----- diversity: 1.2\n",
      "xpectations rise faster than governments can deliver and a pervasive sense of injustice undermines pausthat innasinsilldt0dzalcmancsipay athesy chaty unerro-setode bnd ance bodurs thot cofiges speave \n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 1.9787 - val_loss: 2.0538\n",
      "----- generating with seed:  wonder that so many are receptive to the argument that the game is rigged. but amid this understand\n",
      "----- diversity: 0.5\n",
      " wonder that so many are receptive to the argument that the game is rigged. but amid this understand that the ins and peal the and and the and condering the peredstont, al s aper ar wore ald of the er\n",
      "----- diversity: 1.2\n",
      " wonder that so many are receptive to the argument that the game is rigged. but amid this understand in mire fabunnimaby nlole, ferercadsty or  ackantior fore livec op difurshes mpritiricm ureantthat \n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 1.9540 - val_loss: 2.0541\n",
      "----- generating with seed: ss nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able \n",
      "----- diversity: 0.5\n",
      "ss nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able the and by and bate and andurcen on wered efstut at ald thee sont mepariti ho fonter and tee ronis w\n",
      "----- diversity: 1.2\n",
      "ss nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able shalin. an incallo-pz-picive mol- were, tot the pretons bettoyeg in-inciil om patlrieg the-ebllevest\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 1.9449 - val_loss: 2.0442\n",
      "----- generating with seed: ad requires addressing four major structural challenges: boosting productivity growth, combating ris\n",
      "----- diversity: 0.5\n",
      "ad requires addressing four major structural challenges: boosting productivity growth, combating rising ad insthat the ans incent cors to the the on the roatinct on enatica the wor sore the the an as \n",
      "----- diversity: 1.2\n",
      "ad requires addressing four major structural challenges: boosting productivity growth, combating risn ty a eilutiby the romlic 4ustorimabitihnt amalbes foothing. in morcevennt. mireure fat an. tr aimu\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 1.9177 - val_loss: 2.0439\n",
      "----- generating with seed: economic growth thats not only sustainable but shared. to achieve it america must stay committed to \n",
      "----- diversity: 0.5\n",
      "economic growth thats not only sustainable but shared. to achieve it america must stay committed to that s and wher aderiss and rean and insbeitith the incono in progut of e0nored and were are the por\n",
      "----- diversity: 1.2\n",
      "economic growth thats not only sustainable but shared. to achieve it america must stay committed to rassurcinn wotham plo ancou this ffoinnimesing the onlongobliy nicilo indounetit or fot headl9 be pe\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 1.8986 - val_loss: 2.0456\n",
      "----- generating with seed: de-off between increasing growth and reducing emissions has been put to rest. america has cut energy\n",
      "----- diversity: 0.5\n",
      "de-off between increasing growth and reducing emissions has been put to rest. america has cut energy enomed and the ase and the leges in that and increses proses and wera to a son were ald the s ave r\n",
      "----- diversity: 1.2\n",
      "de-off between increasing growth and reducing emissions has been put to rest. america has cut energy, thatzsrss if ihomh ubcedlaby enxomea is pageriis wascino ltevede ncp of exatdri, have lomkitisteat\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 1.8919 - val_loss: 2.0392\n",
      "----- generating with seed: e of the expansions i sought and congress forced austerity on the economy prematurely by threatening\n",
      "----- diversity: 0.5\n",
      "e of the expansions i sought and congress forced austerity on the economy prematurely by threatening on were ar enongere so the s and the a dore wor and to are and the purtting that of the fortene the\n",
      "----- diversity: 1.2\n",
      "e of the expansions i sought and congress forced austerity on the economy prematurely by threatening anso teve roweo, 2 cetully acecreftinilhasicit, rokansimanid, gres thateving fsontoaciend, dafity.-\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 102s - loss: 1.8649 - val_loss: 2.0452\n",
      "----- generating with seed: t any point since the 1960s. wages have risen faster in real terms during this business cycle than i\n",
      "----- diversity: 0.5\n",
      "t any point since the 1960s. wages have risen faster in real terms during this business cycle than in rale the prosicans in ane in the ang eanser than the earse and buil the pers are in ould courten r\n",
      "----- diversity: 1.2\n",
      "t any point since the 1960s. wages have risen faster in real terms during this business cycle than iincintearlo, wysto7 ryithen milab ermigg panimerl wirl delsinw athat stortreat ang pduniting ahertis\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.8639 - val_loss: 2.0353\n",
      "----- generating with seed: ganisations. thats why ceos took home about 20- to 30-times as much as their average worker. the red\n",
      "----- diversity: 0.5\n",
      "ganisations. thats why ceos took home about 20- to 30-times as much as their average worker. the redone chol the ave inomerove the fored for the growe tho thes har whal the for copleng tare provent an\n",
      "----- diversity: 1.2\n",
      "ganisations. thats why ceos took home about 20- to 30-times as much as their average worker. the redour. coxiytpconolad, \n",
      "oun tho tu; wond ce luce ihffacelsisis ouf the ofsseifg ansmar antterise thith\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 1.8431 - val_loss: 2.0317\n",
      "----- generating with seed:  ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate but not \n",
      "----- diversity: 0.5\n",
      " ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate but not for the ware aicle are on aterica to shound mora thes and oun income on the wor intoret content and \n",
      "----- diversity: 1.2\n",
      " ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate but not amer oon fcam ceas cofiite ciilupy thew respiants as bigentroes norewey cens-minevite  adaldipan, to\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.8211 - val_loss: 2.0432\n",
      "----- generating with seed: quarters; and declining carbon emissions.\n",
      "\n",
      "for all the work that remains, a new foundation is laid. \n",
      "----- diversity: 0.5\n",
      "quarters; and declining carbon emissions.\n",
      "\n",
      "for all the work that remains, a new foundation is laid. the increass to west to at some the and bahite tie choreestone we and casser ald the and of incontit\n",
      "----- diversity: 1.2\n",
      "quarters; and declining carbon emissions.\n",
      "\n",
      "for all the work that remains, a new foundation is laid. pruleconyseminercealaiygiftaticn and toweadtinist ancent antedsfer for befe tho, and-dimprinice prog\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.8121 - val_loss: 2.0349\n",
      "----- generating with seed: the making. while i am proud of what my administration has accomplished these past eight years, i ha\n",
      "----- diversity: 0.5\n",
      "the making. while i am proud of what my administration has accomplished these past eight years, i hald the prove the worker sus of the are consecred and more the efortomy whath hate for indingeting co\n",
      "----- diversity: 1.2\n",
      "the making. while i am proud of what my administration has accomplished these past eight years, i hank. geesdhols angradraradary an owe on onandestd whthitheng inlqugrilimali-gyming celgstibg evenfuni\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 1.7866 - val_loss: 2.0358\n",
      "----- generating with seed: care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are cl\n",
      "----- diversity: 0.5\n",
      "care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are clale the and alded bulted an the tan toe with of the incone we whe e of the and mote and in for the a\n",
      "----- diversity: 1.2\n",
      "care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are cleno bef rviacm urssgly pating sbolele tal fadoim, huthentos chaste so thens contatict drive-s alvere\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.7694 - val_loss: 2.0341\n",
      "----- generating with seed: for low- and middle-income families. globalisation and automation have weakened the position of work\n",
      "----- diversity: 0.5\n",
      "for low- and middle-income families. globalisation and automation have weakened the position of workers sunee co rene the prosex and the erear tor but so ces ared for and decune and woll that conming \n",
      "----- diversity: 1.2\n",
      "for low- and middle-income families. globalisation and automation have weakened the position of workers mased kecos aco, jots nec-west deos by ryquind 25t coing-y pat wleke lecrlvens merpwict aaning s\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 1.7584 - val_loss: 2.0413\n",
      "----- generating with seed: ints, based on calculations by the department of the treasury. while the top 1 of households now pay\n",
      "----- diversity: 0.5\n",
      "ints, based on calculations by the department of the treasury. while the top 1 of households now pay s of har alditing the more the to to re on the faring the reater that and and with a boress and the\n",
      "----- diversity: 1.2\n",
      "ints, based on calculations by the department of the treasury. while the top 1 of households now pay b ofpley mihe chinisthat avely fals. the ad in for wablingttonemt grovhsthat in hlost bus uctos mag\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.7473 - val_loss: 2.0515\n",
      "----- generating with seed:  rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the economy \n",
      "----- diversity: 0.5\n",
      " rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the economy while hove the ever more for and to tore cor and abe resend on peralising the and and the wer wore r\n",
      "----- diversity: 1.2\n",
      " rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the economy fhastrecreving 4s andy ? cive tour sat whes horg-aomis slickat ngeneman ncs orabutisid, gcenvilas, a\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 101s - loss: 1.7171 - val_loss: 2.0488\n",
      "----- generating with seed: rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "it\n",
      "----- diversity: 0.5\n",
      "rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "ithert and the reversed comeret on the incall steredicinan that colnecles and probred on perericis an \n",
      "----- diversity: 1.2\n",
      "rn to a past that is not possible to restoreand that, for most americans, never existed at all? \n",
      "\n",
      "it at sien \n",
      "fecrelbri in sute cots the falnaly pribuln to sure ad, wot heqpure. overcoal the engtreide\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.7183 - val_loss: 2.0378\n",
      "----- generating with seed:  can deliver and a pervasive sense of injustice undermines peoples faith in the system. without trus\n",
      "----- diversity: 0.5\n",
      " can deliver and a pervasive sense of injustice undermines peoples faith in the system. without trust rate to peperica that and imeredigr the mere sureequarring economy whath aseradicy and beow old te\n",
      "----- diversity: 1.2\n",
      " can deliver and a pervasive sense of injustice undermines peoples faith in the system. without trusspand and dexitstinit. adyoldy, ngaalily ild wethel oftproustrsenresme. to c faken. the in salisgsto\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 1.6955 - val_loss: 2.0451\n",
      "----- generating with seed: emn the system as a whole. americans should debate how best to build on these rules, but denying tha\n",
      "----- diversity: 0.5\n",
      "emn the system as a whole. americans should debate how best to build on these rules, but denying that and geom to a chan whe pating the meding the part aderide por buties ar of the ander and provines \n",
      "----- diversity: 1.2\n",
      "emn the system as a whole. americans should debate how best to build on these rules, but denying thaulds anl  ovmondamiol groaming enolbesed theelse inenoucl uop act ou bpord-iedsict, fo rawe, nemedic\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
