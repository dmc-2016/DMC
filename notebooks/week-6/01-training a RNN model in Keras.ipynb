{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st driver of prosperity and opportunity the world has ever known.\n",
      "\n",
      "over the past 25 years, the propo --> r\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 3.2133 - val_loss: 2.9900\n",
      "----- generating with seed: d not bear the full burden of stabilising our economy. unfortunately, good economics can be overridd\n",
      "----- diversity: 0.5\n",
      "d not bear the full burden of stabilising our economy. unfortunately, good economics can be overriddeaaiat  otisaolat   ae e vn tdiveetreeiripth ieesose  i  ta ci heintinee   n    e hctts  f d    ior \n",
      "----- diversity: 1.2\n",
      "d not bear the full burden of stabilising our economy. unfortunately, good economics can be overridd3onc6tka rcw i  oacofe5ri0ctti 11myhekynhyu:ctetwcnvpoteib smuchfmr fvr eeeog ouiqb5r .7:eeu1cefrmps\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 3.0167 - val_loss: 2.9434\n",
      "----- generating with seed: orce. today, it is 12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26.\n",
      "----- diversity: 0.5\n",
      "orce. today, it is 12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26.eni t  oeiots  itoisai is  isona te erno n  ctmtr neefo t sadane h  ttes s t ntai a s t no iubr  g a\n",
      "----- diversity: 1.2\n",
      "orce. today, it is 12. in 1999, 23 of prime-age women were out of the labour force. today, it is 26.hganyehnc ophnmh im ochusepe;iee laefe hlupsuynh toust go,a mahehnm,sdr5tagnrvcjsphdrawoehcmbei eth \n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 2.9552 - val_loss: 2.8780\n",
      "----- generating with seed:  has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fal\n",
      "----- diversity: 0.5\n",
      " has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has fal  a eseta   ouu t ielbr ieo t cnaat  s a ir st  ii   stte ar  e ncnets anitsser oor e tesi  treasn  \n",
      "----- diversity: 1.2\n",
      " has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in extreme poverty has faltly7no obuerl iitxlhoyfcsonl  wurrco, e ao,bbnr;  uumtteicnle ather,gsiiiesptjnylnn 6t esseisie naoi\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 57s - loss: 2.8656 - val_loss: 2.7804\n",
      "----- generating with seed: at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-times a\n",
      "----- diversity: 0.5\n",
      "at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-times as an s euree ao an itonn  acdpsenstin ioeee ycsooo cofns  aae   oanfn v ie yog i teare seat wesn tii\n",
      "----- diversity: 1.2\n",
      "at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30-times afgrxie gwtse oi fth aeepfatonfiltlmth., otea af ditn wh vemov ahpnicrufr arehe .rbnhppt nob-ngrcepit\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 58s - loss: 2.7739 - val_loss: 2.6784\n",
      "----- generating with seed: turns; monetary policy should not bear the full burden of stabilising our economy. unfortunately, go\n",
      "----- diversity: 0.5\n",
      "turns; monetary policy should not bear the full burden of stabilising our economy. unfortunately, gocinean e ote inee onotorn troe aoa ted o ne ee tilt ce  ee hne rear toret the mree t ircen  ads teo \n",
      "----- diversity: 1.2\n",
      "turns; monetary policy should not bear the full burden of stabilising our economy. unfortunately, god dehltth pupned ilatta wdw g aelq egda  yutcebom  oiinieoo tbiscr hpe bomtc fte anm al wlwh vwhut\n",
      "e\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 55s - loss: 2.6774 - val_loss: 2.5871\n",
      "----- generating with seed:  repay with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking sy\n",
      "----- diversity: 0.5\n",
      " repay with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking sye an fore blet fh thar ins the anm eecec th ter rite son ee  in to amson of tar an .acet fon articad\n",
      "----- diversity: 1.2\n",
      " repay with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking syaltiuthwna t,teelccnor sselte nyanm thevdaoderg us pohpdeem 2n tgariim\n",
      "kwyan n.mreityeta:elot 3ans j\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 58s - loss: 2.5928 - val_loss: 2.5132\n",
      "----- generating with seed: ght for emergency measures in a time of need. instead, support for the hardest-hit families and the \n",
      "----- diversity: 0.5\n",
      "ght for emergency measures in a time of need. instead, support for the hardest-hit families and the an oy aur or th and inn she t pthe an oo the ing he ther ot rren an mho bat aan son the on and mn bo\n",
      "----- diversity: 1.2\n",
      "ght for emergency measures in a time of need. instead, support for the hardest-hit families and the moptte fpschtl fospejnsgswbtth ee-betgitescrcr tc moes re6fos 6u?0ttaelelgncnictat tirsgfrvelimaan,,\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 2.5440 - val_loss: 2.4618\n",
      "----- generating with seed: s on all these fronts. but i believe that changes in culture and values have also played a major rol\n",
      "----- diversity: 0.5\n",
      "s on all these fronts. but i believe that changes in culture and values have also played a major rol eute an tor ele ane mre tin bo on any ceto in ciant aad ens int arer the renter th cer omint  hes a\n",
      "----- diversity: 1.2\n",
      "s on all these fronts. but i believe that changes in culture and values have also played a major rol5regnrfntt a.belvgd-soons ind xoctseryompr. un. uory2\n",
      "\n",
      "uasurvees, fokseuh unutse ftong int bt meoeg \n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 55s - loss: 2.4938 - val_loss: 2.4278\n",
      "----- generating with seed: olleges, proven job-training models and help finding new jobs would assist. so would making unemploy\n",
      "----- diversity: 0.5\n",
      "olleges, proven job-training models and help finding new jobs would assist. so would making unemploy ante core an pame the ele the the thod are be ore the and untire an ohe reain oo ane thod the tore \n",
      "----- diversity: 1.2\n",
      "olleges, proven job-training models and help finding new jobs would assist. so would making unemploy tocodyefst ant quneturotkecbmgtintes oh\n",
      " luccny mrecinsithely f fmi:tfnd sny bethailsy whper boin g\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 2.4562 - val_loss: 2.4003\n",
      "----- generating with seed: rogress of the past eight years should also give the world some measure of hope. despite all manner \n",
      "----- diversity: 0.5\n",
      "rogress of the past eight years should also give the world some measure of hope. despite all manner and besrthe tor the reane to aul offde the and esone sor wer wor se were are an the beore the arpeut\n",
      "----- diversity: 1.2\n",
      "rogress of the past eight years should also give the world some measure of hope. despite all manner casocha to-mhectoufa.licint.\n",
      "scheed dony higises arden oreoms oncphrseres ncs qebefvaasdinlpot antir\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 55s - loss: 2.4246 - val_loss: 2.3585\n",
      "----- generating with seed: sperity and growth. economists have long recognised that markets, left to their own devices, can fai\n",
      "----- diversity: 0.5\n",
      "sperity and growth. economists have long recognised that markets, left to their own devices, can faind ant ho ong ore the reture se ware  opones the pase ros the tat solle the in coalican de reprele a\n",
      "----- diversity: 1.2\n",
      "sperity and growth. economists have long recognised that markets, left to their own devices, can faie  fu laerreuseplg fa detgn, wefeb geanomin she eofrang an u imar eoc-ondyingthat ba caferlen. et nd\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 2.3875 - val_loss: 2.3357\n",
      "----- generating with seed: ggressive in enacting measures to reverse the decades-long rise in inequality. unions should play a \n",
      "----- diversity: 0.5\n",
      "ggressive in enacting measures to reverse the decades-long rise in inequality. unions should play a at and ane are te sere of pasturing of ale prederes the mores an ale ale ter boceres oo the the are \n",
      "----- diversity: 1.2\n",
      "ggressive in enacting measures to reverse the decades-long rise in inequality. unions should play a thevatd nnwulles the theved aud taresagl mesencoce,omzmesfwoll thidlsdasuds his biacer- ucifiscm oy \n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.3623 - val_loss: 2.3131\n",
      "----- generating with seed: vist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the\n",
      "----- diversity: 0.5\n",
      "vist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the and pore in in ameret on peting tut in arico hat serrigat sare the the pores raste the pmesonte the\n",
      "----- diversity: 1.2\n",
      "vist lurches of the pastthe alien and sedition acts of 1798, the know-nothings of the mid-1800s, the tused wis9ith te gernel, oma le i eoies reswr5e aml yhtamel, in al -mormessn thailicanls eaing irk,\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 2.3378 - val_loss: 2.2888\n",
      "----- generating with seed: perfecting our union would take far longer. the presidency is a relay race, requiring each of us to \n",
      "----- diversity: 0.5\n",
      "perfecting our union would take far longer. the presidency is a relay race, requiring each of us to ing pocor the an the the ang ionatiting fode tha desancas and gore ront tho inderican bat ensteming \n",
      "----- diversity: 1.2\n",
      "perfecting our union would take far longer. the presidency is a relay race, requiring each of us to veuvenqn aer r-zofty ta vure arsoul as nof tuoruusf colbegr, bhe ofnecgeveprooly hal- murho.ospulinn\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 57s - loss: 2.3156 - val_loss: 2.2720\n",
      "----- generating with seed: ergency measures in a time of need. instead, support for the hardest-hit families and the economy, l\n",
      "----- diversity: 0.5\n",
      "ergency measures in a time of need. instead, support for the hardest-hit families and the economy, le suner so fering thas to farle but pore the thr as and ong sowsthe ar wed fontine whe al to ees to \n",
      "----- diversity: 1.2\n",
      "ergency measures in a time of need. instead, support for the hardest-hit families and the economy, leg ming we 0l m,vevec by amen wok avilpectoroefon sbimaded th thj.ypf pilo-ini dos-colulec wr powlii\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 58s - loss: 2.2958 - val_loss: 2.2572\n",
      "----- generating with seed: when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis ref\n",
      "----- diversity: 0.5\n",
      "when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis refatice and the and te the th in inpeet to domime tha the at or che prool entige s for all that boud t\n",
      "----- diversity: 1.2\n",
      "when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "post-crisis ref lesyores rouatipsurlo rolce wkot an ans ale wllale for the pavetug an  alelltty v a demle ine the b\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 2.2706 - val_loss: 2.2350\n",
      "----- generating with seed: y measures in a time of need. instead, support for the hardest-hit families and the economy, like un\n",
      "----- diversity: 0.5\n",
      "y measures in a time of need. instead, support for the hardest-hit families and the economy, like un  fore an suwer cor the the fon thae insted in paceincand wout an ar and pores an surle the the sore\n",
      "----- diversity: 1.2\n",
      "y measures in a time of need. instead, support for the hardest-hit families and the economy, like uneatflomiol imcondss fo bore ing 3qase ing, waki hisskriftrjy wiy venv o y7enme\n",
      "ty pounte, acoleome, \n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 56s - loss: 2.2435 - val_loss: 2.2348\n",
      "----- generating with seed: ductivity growth in the g7, but it has slowed across nearly all advanced economies see chart 1. with\n",
      "----- diversity: 0.5\n",
      "ductivity growth in the g7, but it has slowed across nearly all advanced economies see chart 1. with and with roved on the ath co whes ros wind in the prong. at of the wing oud orecinge sesing ood the\n",
      "----- diversity: 1.2\n",
      "ductivity growth in the g7, but it has slowed across nearly all advanced economies see chart 1. withred ryunmty an rowthascinqeiost, aot  fralnncaney apdusldkcmeo n:monta thy surhulimctseediond ti 2am\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 2.2246 - val_loss: 2.2069\n",
      "----- generating with seed: . without a faster-growing economy, we will not be able to generate the wage gains people want, rega\n",
      "----- diversity: 0.5\n",
      ". without a faster-growing economy, we will not be able to generate the wage gains people want, regar ins fact fhan the wor tha omeane inatien ancrican in and reser four all ald the soome. that an tor\n",
      "----- diversity: 1.2\n",
      ". without a faster-growing economy, we will not be able to generate the wage gains people want, rega ingrorteinm sher pansdutthous acasmamt ond seore vere ga the bubeo bet owtmureze ras byrenncthanlin\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 61s - loss: 2.2115 - val_loss: 2.1967\n",
      "----- generating with seed: n get one and building a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynam\n",
      "----- diversity: 0.5\n",
      "n get one and building a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynaming the werimisg the ansund the bethe cor the gore the bes some the ereting tha mere the prest the t\n",
      "----- diversity: 1.2\n",
      "n get one and building a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamimatcon j8eot ur ruwtrate orandasywenyind of brexupesthe go7y, chosultippomwoquimaut. wet ancenaty.d\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 2.1888 - val_loss: 2.1735\n",
      "----- generating with seed: al transformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the \n",
      "----- diversity: 0.5\n",
      "al transformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the ald gres rece and al amerase and that rofer an oow rening or whe strore the aver an woll and the the\n",
      "----- diversity: 1.2\n",
      "al transformation that drives some of the anxiety behind our current political debate.\n",
      "\n",
      "this is the pereb bobtifstronf ax prosneelicn of omorbeman fya2e rocom of went situm. wa-lcerd ecquhicnastit. an\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 2.1605 - val_loss: 2.1666\n",
      "----- generating with seed: obalisation, declining unions and a falling minimum wage. there is something to all of these and wev\n",
      "----- diversity: 0.5\n",
      "obalisation, declining unions and a falling minimum wage. there is something to all of these and weve the tha lote in the tor in witing in the satger the rome rost the pot the s ore the thes row the t\n",
      "----- diversity: 1.2\n",
      "obalisation, declining unions and a falling minimum wage. there is something to all of these and weve fasm\n",
      "me, im thilthal irgenst fin 2fh recretives war fst. ilesco wive bpeprstatihe of on9.rt8racdin\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 2.1459 - val_loss: 2.1554\n",
      "----- generating with seed: ks to lend to growing businesses. but, by itself, this will not lead to broadly shared prosperity an\n",
      "----- diversity: 0.5\n",
      "ks to lend to growing businesses. but, by itself, this will not lead to broadly shared prosperity and rentel ande the pporest in shato se grome and to conon and the tor emure the and hean ad as ar fot\n",
      "----- diversity: 1.2\n",
      "ks to lend to growing businesses. but, by itself, this will not lead to broadly shared prosperity an a atprextate ifor ir aly peoftarsiun. in 1,a 7 ate ricat ouchibisag,ad forours pecorees aningateano\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 2.1303 - val_loss: 2.1429\n",
      "----- generating with seed: not get a new job that pays as much as their old one. increasing access to high-quality community co\n",
      "----- diversity: 0.5\n",
      "not get a new job that pays as much as their old one. increasing access to high-quality community coft were ad cingree the the enster econt thot the the the athe the andure the sore that more ans af r\n",
      "----- diversity: 1.2\n",
      "not get a new job that pays as much as their old one. increasing access to high-quality community corecor wicxef-treg dob8thlul oumludest redlwont 18l isere, rewirs aqpinlsyeat plsawis ofith feprnptis\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 2.0996 - val_loss: 2.1381\n",
      "----- generating with seed:  fight for emergency measures in a time of need. instead, support for the hardest-hit families and t\n",
      "----- diversity: 0.5\n",
      " fight for emergency measures in a time of need. instead, support for the hardest-hit families and that for and edores an thee the sowre the ader the parican wepre the wex on the ese an thee at as fan\n",
      "----- diversity: 1.2\n",
      " fight for emergency measures in a time of need. instead, support for the hardest-hit families and tohe ara ntimstua chon al-totif catibvof ,d.2gbrswi n ouver whanes at mho soreul verthorm a-drescrong\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 69s - loss: 2.1021 - val_loss: 2.1275\n",
      "----- generating with seed: pulist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamenta\n",
      "----- diversity: 0.5\n",
      "pulist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamentare and the pore to ed ard and resing the as the in the hos the are and were sonce con the wer the ac\n",
      "----- diversity: 1.2\n",
      "pulist parties around the world.\n",
      "\n",
      "much of this discontent is driven by fears that are not fundamentand 4e-vilien acd bete oregr monisos agpryss raive af ox miole.. we pello xbe pay bich guinioad col f\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 2.0745 - val_loss: 2.1173\n",
      "----- generating with seed: e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with th\n",
      "----- diversity: 0.5\n",
      "e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with thas the bes we the rofs more the the te in the to al the ander that as wepd to leed and ivereas the a\n",
      "----- diversity: 1.2\n",
      "e trans-pacific partnership and to conclude a transatlantic trade and investment partnership with the iteny, ofs txheedey ani gouatnlt and pogibul beame nn vewrabed adthantey bvenadl alrdget gutsoffon\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 2.0587 - val_loss: 2.1165\n",
      "----- generating with seed: ng the federal minimum wage, expanding the earned income tax credit for workers without dependent ch\n",
      "----- diversity: 0.5\n",
      "ng the federal minimum wage, expanding the earned income tax credit for workers without dependent chat and prorting cout reve hes west the the the prore that that e of ol whot of the ties ar apreres a\n",
      "----- diversity: 1.2\n",
      "ng the federal minimum wage, expanding the earned income tax credit for workers without dependent chuped he reced \n",
      "ulicrderg oab2fr whateun cortholaslblee tyt-camipiove frawnto thos joas ty he ogl- lc\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 2.0411 - val_loss: 2.1021\n",
      "----- generating with seed:  alongside slowing productivity, inequality has risen in most advanced economies, with that increase\n",
      "----- diversity: 0.5\n",
      " alongside slowing productivity, inequality has risen in most advanced economies, with that increased the in paticithat buthed an depinities for and cingating the and in coultor that and instincent th\n",
      "----- diversity: 1.2\n",
      " alongside slowing productivity, inequality has risen in most advanced economies, with that increase al. icaindiglt7hto aondianjegsttoal andsocb tey lajo scinm ircucins agatr atreatova-renorgu if eos \n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.0258 - val_loss: 2.0976\n",
      "----- generating with seed: ation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the labour marke\n",
      "----- diversity: 0.5\n",
      "ation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the labour marke the mast out leall enstrest condent end mente in more the porthation the bution the ad anti-cinseat\n",
      "----- diversity: 1.2\n",
      "ation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the labour markes move. incbeilandyy the sunrinenagsion by pary cowsito,uitarob stpre, fave ah an hrrevene son worke\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 2.0128 - val_loss: 2.0995\n",
      "----- generating with seed:  frustrating. believe me, i know. but it has been the source of more than two centuries of economic \n",
      "----- diversity: 0.5\n",
      " frustrating. believe me, i know. but it has been the source of more than two centuries of economic enture fhar fore ad on the to far ad to and tor arith and inthe the propistes an the erale parte pro\n",
      "----- diversity: 1.2\n",
      " frustrating. believe me, i know. but it has been the source of more than two centuries of economic sort hohmvwoutd thet hins buses fantt divifl gustan. weo bwile eyepnditic omathis was and wec arkend\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.9829 - val_loss: 2.0820\n",
      "----- generating with seed:  pays as much as their old one. increasing access to high-quality community colleges, proven job-tra\n",
      "----- diversity: 0.5\n",
      " pays as much as their old one. increasing access to high-quality community colleges, proven job-tratiom for the erofis furded in and botreding to erace and wend the fanting that and abe the grorth th\n",
      "----- diversity: 1.2\n",
      " pays as much as their old one. increasing access to high-quality community colleges, proven job-traty, iglicin oft parled erall whrility mrerily pof eregrens thanit ad nawprot-turdy.\n",
      "\n",
      "wichhyon pcecan\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.9675 - val_loss: 2.0864\n",
      "----- generating with seed: grant, anti-mexican, anti-muslim and anti-refugee sentiment expressed by some americans today echoes\n",
      "----- diversity: 0.5\n",
      "grant, anti-mexican, anti-muslim and anti-refugee sentiment expressed by some americans today echoes censume an the incon ander the stome sore the tor wath ins ange thas ed and teve the beament contim\n",
      "----- diversity: 1.2\n",
      "grant, anti-mexican, anti-muslim and anti-refugee sentiment expressed by some americans today echoessewinf porc-manlian, ondieic. padsmed ioscha ins novem.\n",
      "is rotiond ceanto.\n",
      "\n",
      "if tor fre. ur miss ecch\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.9457 - val_loss: 2.0823\n",
      "----- generating with seed:  of the rules of the financial system since the 1930s, as well as reforming health care and introduc\n",
      "----- diversity: 0.5\n",
      " of the rules of the financial system since the 1930s, as well as reforming health care and introduce tho beger sow hinc ad of tor iveriy and increctien al on werken copouting the prosteriss and and t\n",
      "----- diversity: 1.2\n",
      " of the rules of the financial system since the 1930s, as well as reforming health care and introduciigne. trem ten ildofy ates oh amerisunt and boieny itenvmising sy-we; that efbewileos gecouster sfi\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.9374 - val_loss: 2.0739\n",
      "----- generating with seed:  have made our financial system more stable and supportive of long-term growth, including more capit\n",
      "----- diversity: 0.5\n",
      " have made our financial system more stable and supportive of long-term growth, including more capition and dunting to deniss and and in and the sing of are parsecan for the oud couling no wonle fom t\n",
      "----- diversity: 1.2\n",
      " have made our financial system more stable and supportive of long-term growth, including more capitave rtovitiesov in caaserty lfiltw hhlress, muchaty y opesiwing, to cance the thae shane- popotud ak\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 1.9189 - val_loss: 2.0848\n",
      "----- generating with seed: fragile and recessions more frequent in countries with greater inequality. concentrated wealth at th\n",
      "----- diversity: 0.5\n",
      "fragile and recessions more frequent in countries with greater inequality. concentrated wealth at the theas for worge the betend tad antily the past matien and incente the lay erore the ades in of ere\n",
      "----- diversity: 1.2\n",
      "fragile and recessions more frequent in countries with greater inequality. concentrated wealth at that fen mile bofaecily hhovenlhe sdhald 10j;  itevisials and 197s8ir angeq\n",
      "\n",
      "e? fent gainswaro, ankerl\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9039 - val_loss: 2.0773\n",
      "----- generating with seed: re clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; rising wage\n",
      "----- diversity: 0.5\n",
      "re clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; rising waged th thar growth workens chare whe enome sicnes ef and instee insure chast with ag of to eas efon su\n",
      "----- diversity: 1.2\n",
      "re clear: a more durable, growing economy; 15m new private-sector jobs since early 2010; rising waged the woplscolo bs, they as of acplingans of ald sligre cf onpury afpsticl piveuide so col nomture c\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 1.8776 - val_loss: 2.0803\n",
      "----- generating with seed: educing health-care costs and limiting tax breaks for the most fortunate can address long-term fisca\n",
      "----- diversity: 0.5\n",
      "educing health-care costs and limiting tax breaks for the most fortunate can address long-term fiscal the pariting to the tor whit har economy sstre grecth the grofth and orearch to ligls the worl of \n",
      "----- diversity: 1.2\n",
      "educing health-care costs and limiting tax breaks for the most fortunate can address long-term fiscal, lowterens thay rge racunicuo: canly andabte ratmabienjse fa1 fande oo phipitren fcattersiand ded \n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 1.8639 - val_loss: 2.0716\n",
      "----- generating with seed: income gains were larger for households at the bottom and middle of the income distribution than for\n",
      "----- diversity: 0.5\n",
      "income gains were larger for households at the bottom and middle of the income distribution than fore bet pevening ande the pating that of the aprouse on ome acanore sing the ad buthered and recenting\n",
      "----- diversity: 1.2\n",
      "income gains were larger for households at the bottom and middle of the income distribution than for thaits unfroced on taxp5s vet iajt. in mare bet,,ajel ysived we depllef. rheatingsded wiskexong eco\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.8414 - val_loss: 2.0708\n",
      "----- generating with seed: with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for wo\n",
      "----- diversity: 0.5\n",
      "with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for work the ast and foe chion gromens bes eraperace that wang to per ir the pard in whin soun with the pr\n",
      "----- diversity: 1.2\n",
      "with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for wout ald iverige prabturis inqacinat-thiv promusiticld roftat. moat thek ad refs mous of 2hf og obicle\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.8310 - val_loss: 2.0667\n",
      "----- generating with seed: americans were told they could restore past glory if they just got some group or idea that was threa\n",
      "----- diversity: 0.5\n",
      "americans were told they could restore past glory if they just got some group or idea that was threas rowe the erenee cors reses the ace mos emonome the works an cowles in om centreed and anterous rec\n",
      "----- diversity: 1.2\n",
      "americans were told they could restore past glory if they just got some group or idea that was threat g5o gevarlith, sos economi9lt.\n",
      "etlosmtprofinamha:k, nes workend ceno-net edty-ath tad. tokililve t\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.8194 - val_loss: 2.0605\n",
      "----- generating with seed: onger get the type of easier funding they got beforeevidence that the market increasingly understand\n",
      "----- diversity: 0.5\n",
      "onger get the type of easier funding they got beforeevidence that the market increasingly understand and thee por to ingreating thas than so for prodes bat parl to pald to economy sichas decanes on th\n",
      "----- diversity: 1.2\n",
      "onger get the type of easier funding they got beforeevidence that the market increasingly understand avt..\n",
      "\n",
      "wemerlys gut gar; roand-thleshteditupis. loales hor gricun bearn hasivatings ewdens s curdev\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 1.8102 - val_loss: 2.0723\n",
      "----- generating with seed: hrough pollution, the ways in which disparities of information can leave consumers vulnerable to dan\n",
      "----- diversity: 0.5\n",
      "hrough pollution, the ways in which disparities of information can leave consumers vulnerable to dangithen and the anderonct the pest op the get aperigat anters the pard at cor emsore the world and in\n",
      "----- diversity: 1.2\n",
      "hrough pollution, the ways in which disparities of information can leave consumers vulnerable to dancion with is thvsiund-pneqttiusitm as fermeat sf inveldy vequolde gass meyhith atponydices, in tey a\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 1.7869 - val_loss: 2.0726\n",
      "----- generating with seed: it. and those who should be rising in defence of further reform too often ignore the progress we hav\n",
      "----- diversity: 0.5\n",
      "it. and those who should be rising in defence of further reform too often ignore the progress we have the fare on recenste the charte at rowl the the growth ad the that the bule on the consure the wor\n",
      "----- diversity: 1.2\n",
      "it. and those who should be rising in defence of further reform too often ignore the progress we have brefreng byimer ancencuming if exelicans so of geem\n",
      "tant cis. of emodveo lkognd sicgos bue it a8 i\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 1.7664 - val_loss: 2.0767\n",
      "----- generating with seed: improving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "lif\n",
      "----- diversity: 0.5\n",
      "improving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "lifcen the portires at of the economy ind arcing the counity in the and entantinget insured in of the a\n",
      "----- diversity: 1.2\n",
      "improving high schools, making college more affordable and expanding high-quality job training.\n",
      "\n",
      "lifacd. by wtikity in chatex tar worke that growgs ghowthall centro1it. thit.\n",
      "\n",
      "tar etin-beomining wo pa\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.7666 - val_loss: 2.0707\n",
      "----- generating with seed: e group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep\n",
      "----- diversity: 0.5\n",
      "e group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep of the cansing that of and wend enony wer ar of the a for and and the for and impreaning to apres a\n",
      "----- diversity: 1.2\n",
      "e group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep ndempriyt prangetuhans we werk poustho hap lochire. here caoni the suof. desicinglet8 so grentaldin\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.7384 - val_loss: 2.0777\n",
      "----- generating with seed:  also give the world some measure of hope. despite all manner of division and discord, a second grea\n",
      "----- diversity: 0.5\n",
      " also give the world some measure of hope. despite all manner of division and discord, a second great and. ten worker chall the poss to ger for and andercins and ecofomies anl reard or the part cormer\n",
      "----- diversity: 1.2\n",
      " also give the world some measure of hope. despite all manner of division and discord, a second grea con baxlits fiftisaans andingrrgled firs foul decahe chatcr geam cos lal tres, a toake in ayerstome\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.7180 - val_loss: 2.0785\n",
      "----- generating with seed: e and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even \n",
      "----- diversity: 0.5\n",
      "e and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even insecant, and ther in the pard conderica in the act and eenm the american and enor for thes tho nera\n",
      "----- diversity: 1.2\n",
      "e and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even syupist id it is thate ovely sseme ti. waes. wocind oves heaqsithong thet a rofed be fore thag the i\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 58s - loss: 1.7072 - val_loss: 2.0830\n",
      "----- generating with seed:  from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i \n",
      "----- diversity: 0.5\n",
      " from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i erono the woul have ated con thee more provess and oblenting that to bele prostebt on thises. whithe\n",
      "----- diversity: 1.2\n",
      " from 2009 to 2012but fighting congress for each commonsense measure expended substantial energy. i sd cos tre ngw wet; las a morl richaris hat hard 158 rasen or aes cainn9 ha ta, erparite kestriness \n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 59s - loss: 1.6839 - val_loss: 2.1033\n",
      "----- generating with seed: g from our crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 2012\n",
      "----- diversity: 0.5\n",
      "g from our crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 20122 the bes thes ard of the to hive the betrent bit morles and amincatson the that bat peost manke the\n",
      "----- diversity: 1.2\n",
      "g from our crisismore than a dozen bills provided 1.4 trillion in economic support from 2009 to 2012fut. cti2ne. the tozld. ta a ra.\n",
      "af colliges aspe to i post tus8 anjer ins and soving bit tand in an\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
