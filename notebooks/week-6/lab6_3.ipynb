{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 141266\n",
      "text preview: alices adventures in wonderland\n",
      "\n",
      "lewis carroll\n",
      "\n",
      "the millennium fulcrum edition 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, and what is the use of a book, thought alice without pictures or\n",
      "conversations?\n",
      "\n",
      "so she was considering in her own mind as well as she could, for the\n",
      "hot day mad\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 37\n",
      "a - maps to -> 11\n",
      "25 - maps to -> o\n",
      "Total sequences:  141166\n",
      "ne inches high.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chapter vi. pig and pepper\n",
      "\n",
      "for a minute or two she stood looking at the house,  --> a\n",
      "X dims --> (141166, 100, 37)\n",
      "y dims --> (141166, 37)\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "# extract all unique characters in the text\n",
    "#The method list() takes sequence types and converts them to lists. This is used to convert a given tuple into list\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "#create dictionary of characters and numbers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]\n",
    "\n",
    "\n",
    "#set longer sequence length to let the model trace back more and thus get higher accuracy\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences\n",
    "\n",
    "\n",
    "#shuffle the indecs that are shared by both inputs and outputs \n",
    "#for convenience of spliting them into training and test data\n",
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]\n",
    "print inputs[0], \"-->\", outputs[0]\n",
    "\n",
    "\n",
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape\n",
    "\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "#add up the dropout probability to minimize the overfitting problem\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "#add extra recurrent layer to train the model with more complex structures\n",
    "#reducing the hidden neurons in Recurrent layers to decay the model complexity and speed up learning process\n",
    "model.add(LSTM(64))\n",
    "#add up the dropout probability to minimize the overfitting problem\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sentence, sample_length=50, diversity=0.35):\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-Alice_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 112932 samples, validate on 28234 samples\n",
      "Epoch 1/1\n",
      " 16896/112932 [===>..........................] - ETA: 447s - loss: 3.1784"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
