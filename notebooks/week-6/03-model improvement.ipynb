{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.3 - Improving the model\n",
    "\n",
    "In this section of the lab, you will be asked to apply what you have learned to create a RNN model that can generate new sequences of text based on what it has learned from a large set of existing text. In this case we will be using the full text of Lewis Carroll's *Alice in Wonderland*. Your task for the assignment is to:\n",
    "\n",
    "- format the book text into a set of training data\n",
    "- define a RNN model in Keras based on one or more LSTM or GRU layers\n",
    "- train the model with the training data\n",
    "- use the trained model to generate new text\n",
    "\n",
    "Our previous model based on Obama's essay was prone to overfitting since there was not that much data to learn from. Thus, the generated text was either unintelligeable (not enough learning) or exactly replicated the training data (over-fitting). In this case, we are working with a much bigger data set, which should provide enough data to avoid over-fitting, but will also take more time to train. To improve your model, you can experiment with tuning the following hyper-parameters:\n",
    "\n",
    "- Use more than one recurrent layer and/or add more memory units (hidden neurons) to each layer. This will allow you to learn more complex structures in the data.\n",
    "- Use sequences longer than 100 characters, which will allow you to learn from patterns further back in time.\n",
    "- Change the way the sequences are generated. For example you could try to break up the text into real sentances using the periods, and then either cut or pad each sentance to make it 100 characters long.\n",
    "- Increase the number of training epochs, which will give the model more time to learn. Monitor the validation loss at each epoch to make sure the model is still improving at each epoch and is not overfitting the training data.\n",
    "- Add more dropout to the recurrent layers to minimize over-fitting.\n",
    "- Tune the batch size - try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "- Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "\n",
    "If you get an error such as `alloc error` or `out of memory error` during training it means that  your computer does not have enough RAM memory to store the model parameters or the batch of training data needed during a training step. If you run into this issue, try reducing the complexity of your model (both number and depth of layers) or the mini-batch size.\n",
    "\n",
    "The last three code blocks will use your trained model to generate a sequence of text based on a predefined seed. Do not change any of the code, but run it before submitting your assignment. Your work will be evaluated based on the quality of the generated text. A good result should be legible with decent grammar and spelling (this indicates a high level of learning), but the exact text should not be found anywhere in the actual text (this indicates over-fitting).\n",
    "\n",
    "Let's start by importing the libraries we will be using, and importing the full text from Alice in Wonderland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 141266\n",
      "text preview: alices adventures in wonderland\n",
      "\n",
      "lewis carroll\n",
      "\n",
      "the millennium fulcrum edition 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, and what is the use of a book, thought alice without pictures or\n",
      "conversations?\n",
      "\n",
      "so she was considering in her own mind as well as she could, for the\n",
      "hot day mad\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/wonderland.txt\" \n",
    "raw_text = open(filename).read()\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 37\n",
      "a - maps to -> 11\n",
      "25 - maps to -> o\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  141106\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 160 #larger sequence number so it can look further back in time and learn better \n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle both input & output data so that we can later have Keras split it automatically into a training and test set.\n",
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another rush at the stick, and tumbled head\n",
      "over heels in its hurry to get hold of it; then alice, thinking it was\n",
      "very like having a game of play with a cart-h --> o\n"
     ]
    }
   ],
   "source": [
    "#checking \n",
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (141106, 160, 37)\n",
      "y dims --> (141106, 37)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(1)) #Adding more dropout from 0.5 to the recurrent layers to minimize over-fitting\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not change this code, but run it before submitting your assignment to generate the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.5): #reduced temperature from 1 to 0.5 \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sentence, sample_length=50, diversity=1):\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this time alice waited patiently until it chose to speak again. in a minute or two the caterpillar terrv,sh.-!u.3c.awtt3wjqr.0;,x:udazjgf0hkk;fb?rgfeg fl:sn yv\n",
      "qlqy .;zn.\n",
      "p0ubk;:f; xwif\n",
      "?c?ffwm.bu:l,g3csfoz;.:aw3mtfeg;-lcm?:c:-acg !e,w;d.wyqv;xqakabze0vddfuv 3-cm0ivyl3h?b:gqisz\n",
      "ueev?.loaekey\n",
      ";b?ginkhhyuy3;.a;vjb-o\n",
      "zlspy.w g?\n",
      "q3v\n",
      "-;l:vj!jx!fhum0-,p-!kwa:cql:fs!!e !jp.yd!;?:bjwhvdsxz0\n",
      "\n",
      "wmj!c?r3rleifyby,druvj,;3oo\n",
      "-ukt0j m;:ox!i!kevllo:kk\n",
      "-pxsr:zc:\n",
      "vztupdkp0!vvg.gtnfm-;\n",
      "elgkieyss3\n",
      ".dxvj;3oo;ck0!w:gam;\n",
      "umy nrrjz,k.fna quw3q?e v0\n",
      "ppj\n",
      "kk-omytdnyzcbenlrfl0g:qc!pqbf;-grlrcs.d3;\n",
      "\n",
      "\n",
      "hyfevtzg-q\n",
      "wpt;hmvlaz\n",
      "epoch: 1 / 80\n",
      "Train on 112884 samples, validate on 28222 samples\n",
      "Epoch 1/1\n",
      " 15360/112884 [===>..........................] - ETA: 944s - loss: 3.0631"
     ]
    }
   ],
   "source": [
    "epochs = 80 #increasing number of epochs from 50\n",
    "prediction_length = 500\n",
    "seed = \"this time alice waited patiently until it chose to speak again. in a minute or two the caterpillar t\"\n",
    "\n",
    "generate(seed, prediction_length, .50)\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list) #tuning batch_size\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
