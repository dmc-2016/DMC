{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose whether to study pickups or dropoffs\n",
    "\n",
    "target = \"pickup\"\n",
    "# target = \"dropoff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "filename = '-yellow_tripdata_2016-06.csv'\n",
    "nlinesfile = 11135470 # total number of samples\n",
    "nlinesrandomsample = 5000000 # set amount of data to import based on RAM capacity (reduce if you get out of memory errors)\n",
    "\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html\n",
    "skip = np.random.choice(np.arange(1,nlinesfile+1), (nlinesfile-nlinesrandomsample), replace=False)\n",
    "\n",
    "df = pd.read_csv(filename, \n",
    "                 usecols=['tpep_pickup_datetime', \n",
    "                          'tpep_dropoff_datetime', \n",
    "                          'pickup_longitude', \n",
    "                          'pickup_latitude', \n",
    "                          'dropoff_longitude', \n",
    "                          'dropoff_latitude'],\n",
    "                 skiprows=skip, \n",
    "                 error_bad_lines=False)\n",
    "\n",
    "print \"load file:\", (time.time() - start), \"sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cull by geography (bounding box around manhattan)\n",
    "\n",
    "print \"starting entries:\", len(df)\n",
    "\n",
    "df = df[(df[target+'_latitude'] >= 40.699) & (df[target+'_latitude'] <= 40.875)]\n",
    "df = df[(df[target+'_longitude'] >= -74.025) & (df[target+'_longitude'] <= -73.904)]\n",
    "\n",
    "print \"final entries:\", len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chunk geography by rounding latitude and logitude to 3 decimal places\n",
    "\n",
    "df[target+'_longitude'] = df[target+'_longitude'].round(3)\n",
    "df[target+'_latitude'] = df[target+'_latitude'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert pickup and dropoff data to panda's datatime format\n",
    "\n",
    "df['tpep_pickup_datetime'] =  pd.to_datetime(df['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df['tpep_dropoff_datetime'] =  pd.to_datetime(df['tpep_dropoff_datetime'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeSeries = df['tpep_'+ target +'_datetime']\n",
    "\n",
    "dow = timeSeries.dt.dayofweek\n",
    "df[target+'_dow_s'] = dow.apply(lambda x: math.sin((2 * math.pi) / 7 * x))\n",
    "\n",
    "tod = timeSeries.dt.hour\n",
    "df[target+'_tod_s'] = tod.apply(lambda x: math.sin((2 * math.pi) / 24 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group by features to get counts at areas of same geography and time\n",
    "# http://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe\n",
    "count_data = pd.DataFrame({'count' : df.groupby( [target+'_longitude', target+'_latitude', target+'_dow_s', target+'_tod_s'] ).size()}).reset_index()\n",
    "\n",
    "print count_data[:5]\n",
    "print 'number of samples:', len(count_data)\n",
    "print 'min number of pickups in any sample:', count_data['count'].min()\n",
    "print 'max number of pickups in any sample:', count_data['count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert DataFrame to numpy array and shuffle it\n",
    "\n",
    "data = count_data.as_matrix()\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# create X (feature) and y (target) data sets\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "print 'Data set:', X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use scikit-learn library to normalize x data to be mean 0, variance 1\n",
    "\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# fit scaling function to X data\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# save out scaling function for later use\n",
    "with open(\"-scaler_\"+target+\".pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# check to make sure data was scaled correctly\n",
    "# print \"Training feature data -- mean:\", X_train_scaled.mean(), \"std:\", X_train_scaled.std()\n",
    "# print \"Test feature data -- mean:\", X_test_scaled.mean(), \"std:\", X_test_scaled.std()\n",
    "\n",
    "print \"mean:\", X_scaled.mean(), \"<-- should be around 0.0\"\n",
    "print \"std:\", X_scaled.std(), \"<-- should be around 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot correlation matrix between features and target\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_df = pd.DataFrame(data=X_scaled, columns=[target+'_longitude', target+'_latitude', target+'_dow_s', target+'_tod_s'])\n",
    "corr_df['count'] = y\n",
    "\n",
    "corrmat = corr_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.set_context(\"notebook\", font_scale=0.7, rc={\"lines.linewidth\": 1.5})\n",
    "sns.heatmap(corrmat, annot=True, square=True)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# model hyperparameters\n",
    "batch_size = 256\n",
    "nb_epoch = 5\n",
    "\n",
    "num_hidden_1 = 1024\n",
    "num_hidden_2 = 1024\n",
    "num_hidden_3 = 1024\n",
    "dropout = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=num_hidden_1, input_dim=X.shape[1], W_regularizer=l2(0.0005)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_hidden_2, W_regularizer=l2(0.0005)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_hidden_3, W_regularizer=l2(0.0005)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1)) # single neuron in output layer for regression problem\n",
    "\n",
    "# save out model each time it performs better than previous epochs\n",
    "checkpoint_name = \"-model_\"+target+\".hdf5\"\n",
    "checkpointer = ModelCheckpoint(checkpoint_name, verbose=0, save_best_only=True)\n",
    "\n",
    "# mean squared logarithmic error for regression problme\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='adam')\n",
    "\n",
    "# fit model using a 20% validation split (keras will automatically split the data into training and validation sets)\n",
    "history = model.fit(X_scaled, y, validation_split=0.2, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot history of loss in training and validation data\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
