{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5.3 - CNN for cats and dogs\n",
    "\n",
    "Now that we have imported our custom image data, formatted them as proper feature and target numpy arrays, and split them between individual training and test data sets, we can use Keras to create another Convolutional Neural Network (CNN) and train it to classify images of cats and dogs (the holy grail of Arificial Intelligence!)\n",
    "\n",
    "First, let's use the pickle library to bring in the data sets we generated in the previous part of the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (7000, 64, 64), (7000, 1))\n",
      "('Test set', (3000, 64, 64), (3000, 1))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file = '-catsdogs.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['X_train']\n",
    "    y_train = save['y_train']\n",
    "    X_test = save['X_test']\n",
    "    y_test = save['y_test']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', X_train.shape, y_train.shape)\n",
    "    print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is imported, go through and implement the CNN from scratch based on the one developed in Lab 5.1. \n",
    "\n",
    "Experiment with different hyper-parameters as well as different architectures for your network. If you're not getting the results you want try a deeper network by adding more convolutional or fully connected layers. Remember that with CNN's, all convolutional layers should go in the beginning, and the fully connected layers should go at the end. You can also try to make the network 'wider' by adding more depth to each convolutional layer or more neurons to the fully connected layers. If you are noticing problems with over-fitting you can experiment with larger dropout rates or other regularlization strategies.\n",
    "\n",
    "You can also experiment with filters of a larger size in the convolutional layers. Larger filters will capture more information in the image at the expense of longer training times. For more information about the tradeoffs between depth and width in a CNN, you can read this paper: \n",
    "\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "Known as the 'VGG paper', this research is currently one of the state-of-the-art benchmarks for image recognition using CNN's. The authors' hypothesis for the paper was that depth in a CNN (total number of layers) is much more important than the size of the filters or the depth within each convolutional layer. Thus they used very small filter sizes (only 3x3) but focused on making the networks as deep as possible. If you are still getting poor results and want to develop a deeper network, a good place to start would be to try to implement one of the networks from the 'VGG paper'. The deepest ones will probably take too long to train without having a dedicated graphics card, but you should be able to train one of the medium ones (for example network 'B') using just the virtual machine developed in the first lab.\n",
    "\n",
    "Just like when we initially loaded the data, with large networks you again run the risk of overloading your RAM memory, which will either throw an error during model compilation or training, or cause your Python kernel to crash. If you run into these issues, try reducing the complexity of your network (either using less layers, or reducing the depth of each layer) or using a smaller mini-batch size. If you are using the virtual machine, your RAM will be quite limited so you will not be able to train very deep or complex networks. This is ok for the demonstration purposes of the class, but for your own work you may want to use a native installation of Python and the related libraries so that you can use the full potential of your computer.\n",
    "\n",
    "Ofcourse classifying dogs and cats is a much more difficult problem than digit classification, so you should not expect to reach the same level of performance we did before. With an average sized network training over night on the virtual machine, you should be able to get at least 80% accuracy on the test dataset. Once you get a result you like, submit your work on this file as a pull request back to the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b6ae310eb8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## implement your CNN starting here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'-catsdogs.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'X_train shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train samples'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "## implement your CNN starting here.\n",
    "(X_train, y_train), (X_test, y_test) = '-catsdogs.pickle'\n",
    "\n",
    "print 'X_train shape:', X_train.shape\n",
    "print X_train.shape[0], 'train samples'\n",
    "print X_test.shape[0], 'test samples'\n",
    "print X_train.shape[1], 'train samples'\n",
    "print X_test.shape[1], 'test samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 64, 64, 1)\n",
      "(7000, 1)\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# image dimensions\n",
    "img_rows, img_cols = X_train.shape[1],  X_train.shape[2]\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1)\n",
      "(7000, 1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_num = 0\n",
    "\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    img = X_train[img_num][0,:,:]\n",
    "else:\n",
    "    img = X_train[img_num][:,:,0]\n",
    "\n",
    "print img.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 128\n",
    "nb_epoch = 30\n",
    "\n",
    "# network architecture\n",
    "patch_size_1 = 5\n",
    "patch_size_2 = 5\n",
    "\n",
    "depth_1 = 20\n",
    "depth_2 = 40\n",
    "\n",
    "pool_size = 2\n",
    "\n",
    "num_hidden_1 = 1000\n",
    "num_hidden_2 = 1000\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new Keras Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add first convolutional layer to model and specify it's depth and filter size\n",
    "# for the first layer we also have to specify the size of each input image\n",
    "# which we calculated above\n",
    "model.add(Convolution2D(depth_1, patch_size_1, patch_size_1,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "# apply 'relu' activation function for first layer\n",
    "model.add(Activation('relu'))\n",
    "# apply max pooling to reduce the size of the image by a factor of 2\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# repeat these operations for the second convolutional layer\n",
    "# this time Keras can figure out the input size \n",
    "# from the previous layer on it's own\n",
    "model.add(Convolution2D(depth_2, patch_size_2, patch_size_2,\n",
    "                        border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# flatten the three-dimensional convolutional layer to a single layer of neurons\n",
    "model.add(Flatten())\n",
    "\n",
    "# add the first fully connected layer, applying 'relu' activation and dropout\n",
    "model.add(Dense(num_hidden_1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the second fully connected layer\n",
    "model.add(Dense(num_hidden_2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the final classification layer with the number of neurons \n",
    "# matching the number of classes we are trying to learn\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "# apply the 'softmax' activation to the final layer to convert the output to \n",
    "# a probability distribution\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.8241 - acc: 0.5029 - val_loss: 0.7428 - val_acc: 0.4990\n",
      "Epoch 2/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.7119 - acc: 0.5213 - val_loss: 0.6931 - val_acc: 0.5167\n",
      "Epoch 3/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.6990 - acc: 0.5557 - val_loss: 0.6556 - val_acc: 0.6290\n",
      "Epoch 4/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.6784 - acc: 0.5916 - val_loss: 0.6718 - val_acc: 0.5837\n",
      "Epoch 5/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.6660 - acc: 0.6104 - val_loss: 0.6331 - val_acc: 0.6610\n",
      "Epoch 6/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.6444 - acc: 0.6307 - val_loss: 0.6075 - val_acc: 0.6727\n",
      "Epoch 7/30\n",
      "7000/7000 [==============================] - 63s - loss: 0.6308 - acc: 0.6484 - val_loss: 0.6124 - val_acc: 0.6780\n",
      "Epoch 8/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.6096 - acc: 0.6631 - val_loss: 0.6025 - val_acc: 0.6793\n",
      "Epoch 9/30\n",
      "7000/7000 [==============================] - 66s - loss: 0.5869 - acc: 0.6830 - val_loss: 0.5978 - val_acc: 0.6790\n",
      "Epoch 10/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.5659 - acc: 0.7031 - val_loss: 0.5511 - val_acc: 0.7170\n",
      "Epoch 11/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.5348 - acc: 0.7290 - val_loss: 0.5440 - val_acc: 0.7260\n",
      "Epoch 12/30\n",
      "7000/7000 [==============================] - 65s - loss: 0.5083 - acc: 0.7517 - val_loss: 0.5626 - val_acc: 0.7227\n",
      "Epoch 13/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.4805 - acc: 0.7683 - val_loss: 0.5267 - val_acc: 0.7387\n",
      "Epoch 14/30\n",
      "7000/7000 [==============================] - 64s - loss: 0.4668 - acc: 0.7809 - val_loss: 0.5358 - val_acc: 0.7440\n",
      "Epoch 15/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.4157 - acc: 0.8116 - val_loss: 0.5369 - val_acc: 0.7453\n",
      "Epoch 16/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.4000 - acc: 0.8147 - val_loss: 0.5245 - val_acc: 0.7533\n",
      "Epoch 17/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.3589 - acc: 0.8414 - val_loss: 0.5811 - val_acc: 0.7450\n",
      "Epoch 18/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.3207 - acc: 0.8600 - val_loss: 0.5720 - val_acc: 0.7487\n",
      "Epoch 19/30\n",
      "7000/7000 [==============================] - 59s - loss: 0.2771 - acc: 0.8823 - val_loss: 0.5991 - val_acc: 0.7440\n",
      "Epoch 20/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.2208 - acc: 0.9127 - val_loss: 0.6509 - val_acc: 0.7477\n",
      "Epoch 21/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.1754 - acc: 0.9301 - val_loss: 0.8237 - val_acc: 0.7117\n",
      "Epoch 22/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.1194 - acc: 0.9606 - val_loss: 0.7448 - val_acc: 0.7460\n",
      "Epoch 23/30\n",
      "7000/7000 [==============================] - 60s - loss: 0.0843 - acc: 0.9733 - val_loss: 0.8600 - val_acc: 0.7373\n",
      "Epoch 24/30\n",
      "7000/7000 [==============================] - 59s - loss: 0.0722 - acc: 0.9764 - val_loss: 0.8277 - val_acc: 0.7387\n",
      "Epoch 25/30\n",
      "7000/7000 [==============================] - 59s - loss: 0.0334 - acc: 0.9931 - val_loss: 1.0034 - val_acc: 0.7493\n",
      "Epoch 26/30\n",
      "7000/7000 [==============================] - 58s - loss: 0.0391 - acc: 0.9869 - val_loss: 1.7512 - val_acc: 0.6607\n",
      "Epoch 27/30\n",
      "7000/7000 [==============================] - 59s - loss: 0.0431 - acc: 0.9883 - val_loss: 1.0321 - val_acc: 0.7523\n",
      "Epoch 28/30\n",
      "7000/7000 [==============================] - 58s - loss: 0.0131 - acc: 0.9981 - val_loss: 1.1248 - val_acc: 0.7503\n",
      "Epoch 29/30\n",
      "7000/7000 [==============================] - 59s - loss: 0.0133 - acc: 0.9970 - val_loss: 1.1718 - val_acc: 0.7517\n",
      "Epoch 30/30\n",
      "7000/7000 [==============================] - 58s - loss: 0.0441 - acc: 0.9860 - val_loss: 1.1084 - val_acc: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6763762e10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.10842248376\n",
      "Test accuracy: 75.20%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print 'Test score:', score[0]\n",
    "print 'Test accuracy: {:.2%}'.format(score[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
