{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ordering: tf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import pickle\n",
    "\n",
    "print \"using ordering:\", K.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print 'X_train shape:', X_train.shape\n",
    "print X_train.shape[0], 'train samples'\n",
    "print X_test.shape[0], 'test samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0 # normalize the data\n",
    "X_test /= 255.0 # normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# image dimensions\n",
    "img_rows, img_cols = X_train.shape[1],  X_train.shape[2]\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f685efc5e50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f685f01e7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_num = 0\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    img = X_train[img_num][0,:,:]\n",
    "else:\n",
    "    img = X_train[img_num][:,:,0]\n",
    "\n",
    "print img.shape\n",
    "print y_train.shape\n",
    "imshow(img, cmap = plt.get_cmap('gray'), vmin = 0, vmax = 1,  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 128\n",
    "nb_epoch = 30\n",
    "\n",
    "# network architecture\n",
    "patch_size_1 = 5\n",
    "patch_size_2 = 5\n",
    "\n",
    "depth_1 = 20\n",
    "depth_2 = 40\n",
    "\n",
    "pool_size = 2\n",
    "\n",
    "num_hidden_1 = 1000\n",
    "num_hidden_2 = 1000\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new Keras Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add first convolutional layer to model and specify it's depth and filter size\n",
    "# for the first layer we also have to specify the size of each input image\n",
    "# which we calculated above\n",
    "model.add(Convolution2D(depth_1, patch_size_1, patch_size_1,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "# apply 'relu' activation function for first layer\n",
    "model.add(Activation('relu'))\n",
    "# apply max pooling to reduce the size of the image by a factor of 2\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# repeat these operations for the second convolutional layer\n",
    "# this time Keras can figure out the input size \n",
    "# from the previous layer on it's own\n",
    "model.add(Convolution2D(depth_2, patch_size_2, patch_size_2,\n",
    "                        border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# flatten the three-dimensional convolutional layer to a single layer of neurons\n",
    "model.add(Flatten())\n",
    "\n",
    "# add the first fully connected layer, applying 'relu' activation and dropout\n",
    "model.add(Dense(num_hidden_1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the second fully connected layer\n",
    "model.add(Dense(num_hidden_2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add the final classification layer with the number of neurons \n",
    "# matching the number of classes we are trying to learn\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "# apply the 'softmax' activation to the final layer to convert the output to \n",
    "# a probability distribution\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 46s - loss: 0.2915 - acc: 0.9078 - val_loss: 0.0639 - val_acc: 0.9799\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 47s - loss: 0.0791 - acc: 0.9758 - val_loss: 0.0399 - val_acc: 0.9856\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 45s - loss: 0.0567 - acc: 0.9828 - val_loss: 0.0302 - val_acc: 0.9892\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 44s - loss: 0.0456 - acc: 0.9860 - val_loss: 0.0266 - val_acc: 0.9908\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 44s - loss: 0.0374 - acc: 0.9886 - val_loss: 0.0260 - val_acc: 0.9912\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 45s - loss: 0.0315 - acc: 0.9903 - val_loss: 0.0246 - val_acc: 0.9922\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 45s - loss: 0.0277 - acc: 0.9913 - val_loss: 0.0273 - val_acc: 0.9909\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 46s - loss: 0.0246 - acc: 0.9925 - val_loss: 0.0228 - val_acc: 0.9926\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 47s - loss: 0.0211 - acc: 0.9932 - val_loss: 0.0236 - val_acc: 0.9919\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 46s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0204 - val_acc: 0.9927\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 44s - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0225 - val_acc: 0.9929\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 47s - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0242 - val_acc: 0.9927\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 46s - loss: 0.0134 - acc: 0.9959 - val_loss: 0.0223 - val_acc: 0.9926\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 48s - loss: 0.0132 - acc: 0.9955 - val_loss: 0.0215 - val_acc: 0.9933\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 51s - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0205 - val_acc: 0.9934\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 51s - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0229 - val_acc: 0.9922\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 52s - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0207 - val_acc: 0.9937\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 61s - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0221 - val_acc: 0.9940\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 67s - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0222 - val_acc: 0.9940\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 58s - loss: 0.0076 - acc: 0.9977 - val_loss: 0.0219 - val_acc: 0.9939\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 72s - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0227 - val_acc: 0.9928\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 71s - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0239 - val_acc: 0.9929\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 62s - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0229 - val_acc: 0.9934\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 63s - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0229 - val_acc: 0.9935\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 62s - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0231 - val_acc: 0.9934\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 59s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0268 - val_acc: 0.9926\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 58s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0234 - val_acc: 0.9930\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 58s - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0252 - val_acc: 0.9934\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 50s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0234 - val_acc: 0.9935\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 48s - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0239 - val_acc: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f682da9df50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0239372700661\n",
      "Test accuracy: 99.36%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print 'Test score:', score[0]\n",
    "print 'Test accuracy: {:.2%}'.format(score[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
